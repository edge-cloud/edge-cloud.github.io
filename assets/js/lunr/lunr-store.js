var store = [{
        "title": "Freeradius with Let's Encrypt certificate for WPA Enterprise (802.1x) WiFi setup",
        "excerpt":"Intro of what to accomplish   Setup   Kerberos   /etc/krb5.conf  # # /etc/krb5.conf #  [libdefaults]  default_realm = DOMAIN.LOCAL  # EoF   Samba   /etc/samba/smb.conf  # # /etc/samba/smb.conf #  # start of global variables [global]  # server information, this is the domain/workgroup workgroup = DOMAIN  # Kerberos / authentication information realm = DOMAIN.LOCAL  # system hostname netbios name = RADIUS1  # security used (Active Directory) security = ADS  # EoF   /etc/hosts  127.0.0.1    radius1.domain.local radius1 localhost.localdomain localhost   Join the domain   Let’s Encrypt   Freeradius   Testing  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/04/30/freeradius-with-letsencrypt-for-wpa-enterprise/",
        "teaser":null},{
        "title": "AWS Direct Connect deep dive",
        "excerpt":"Intro of what to accomplish   Heading 1   Heading 1.1   Bold   Note: This is a notice box   # # Code #                           Dedicated Connections       Hosted Connections       Hosted Virtual Interfaces                       AWS asigned capacity       1 Gbps or 10 Gbps       50 Mbps to 10 Gbps       None                 Private or Public Virtual Interfaces (VIFs)       50       1       1                 Transit Virtual Interface (VIF)       1       1 (if assigned capacity &gt;= 1 Gbps)       None                                      Figure 1: Direct Connect Overview                                Figure 2: Direct Connect Cross Connect                                Figure 3: Direct Connect Connectivity Options     ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/12/13/dx-deep-dive/",
        "teaser":null},{
        "title": "Securing your AWS networks",
        "excerpt":"Intro of what to accomplish   Scope   AWS Well Architected - Security Pillar                              Figure 1: Protecting network and host-level boundaries     Toolbox for securing AWS networks   Amazon VPC Security Groups                              Figure 2: Security Groups     Amazon VPC Network ACLs (Access Control Lists)                              Figure 3: Network Access Control Lists (ACLs)     AWS WAF (Web Application Firewall)                              Figure 4: Web Application Firewalls (WAF)     AWS Firewall Manager                              Figure 5: Firewall Manager     AWS Shield                              Figure 6: AWS Shield     AWS Transit Gateway with 3rd party firewall solutions                              Figure 7: Transit Gateway                                Figure 7: Transit Gateway with Palo Alto Networks Integration     AWS Ingress Routing with 3rd party firewall solutions   Bold   Note: This is a notice box   # # Code #   ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2020/01/03/securing-your-aws-network/",
        "teaser":null},{
        "title": "Title goes here",
        "excerpt":"Intro of what to accomplish   Heading 1   Heading 1.1   Bold   Note: This is a notice box   # # Code #                               Figure 1: Setup Overview of EC2-based VPN endpoint for Site-to-Site VPN with AWS      ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2020/01/01/template/",
        "teaser":null},{
        "title": "Title goes here",
        "excerpt":"Intro of what to accomplish   Heading 1   Heading 1.1   Bold   Note: This is a notice box   # # Code #                               Figure 1: Setup Overview of EC2-based VPN endpoint for Site-to-Site VPN with AWS                  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2020/01/01/template/",
        "teaser":null},{
        "title": "Better understanding BGP",
        "excerpt":"In a previous post we took a look at some of the fundamental principles of IP routing. Today we want to look at some more details of related BGP Routing protocol concepts. While these principles and concepts are generic, we will again use examples based on AWS networking.   This blog post is not intended to be an all encompassing primer on BGP. Instead I’ve seen numerous people confused by some of these principles and concepts while either designing networks or troubleshooting them. Therefore it appears to be a good idea to select and explain them explicitly.   Routing Protocols   A Routing Protocol specifies how routers communicate with each other to exchange information about the network and as a result populate and update the local route tables. While there are many different routing protocols, they all fall into the categories of either Interior gateway protocols or Exterior gateway protocols. Interior gateway protocols (IGPs), such as e.g. OSPF, RIP, or IS-IS, exchange routing information within a single routing domain, thereby under the control of a single administration. Exterior gateway protocols (EGP), such as e.g. BGP exchange routing information between autonomous systems.   BGP   Border Gateway Protocol (BGP) is an Exterior gateway protocols (EGP) designed to exchange routing and reachability information across Autonomous Systems (AS) on an Internet scale. It can be used for routing within an autonomous system, which is called Interior Border Gateway Protocol /  Internal BGP (iBGP). Or it can be used - as on the Internet - to route between AS, which is called Exterior Border Gateway Protocol / External BGP (eBGP). Here we will focus on the eBGP use case.   BGP Best Path Selection Algorithm   Within BGP the Best Path Selection Algorithm is used to select the best route, which is then installed into the local route table. As the Internet route table - used in the Default Free Zone - holds approximately 850,000 IPv4 and 95,000 IPv6 routes as of today and because some of these routes might be received from multiple peer - e.g. Transit Provider and direct peering, this selection is no easy task. Unless the default settings within a BGP enabled router are changed, the Best Path Selection Algorithm selects the shortest path as the best path, where shortest path means the least amount of AS in the path.   In the following we want to look in more detail at the three most important selection criteria within the BGP Best Path Selection Algorithm:     Local Preference or often “Local_Pref” for short is the second criteria that is considered. The default Local_Pref is 100 and routes with a higher local preference are preferred.   AS_PATH is the fourth criteria considered. Routes with the shortest AS_PATH attribute are preferred.   Multi-exit discriminator (MED) is the sixth selection criteria considered. Here routes with a lower MED value are preferred with 0 being the default value.   Local_Pref   As previously seen, Local_Pref is one of the first Best Path Selection Algorithm criteria that a router looks at. It is evaluated before the AS path length. While the default value of Local_Pref is 100, routes that have a higher Local_Pref value are preferred. An important characteristic to consider is that Local_Pref is local in the sense that the attribute is only propagated over iBGP sessions (within your AS) and not over eBGP sessions (to external ASes). Therefore you might see BGP route tables with empty entries for Local_Pref for a given route, sometimes along with other routes that do have an explicit entry. In this case the empty entries just mean that the deafult value of 100 applies.   In practice Local_Pref can be used to specify how traffic should leave our AS, therefore it can guide the exit path (See Figure 1).                              Figure 1: Local_Pref dictates how traffic leaves a local ASN.     This is typically used for traffic engineering purposes, where an ASN wants to prefer a certain kind of peer over another. Usually this is done for financial reasons, as traffic exchanged over e.g. a Transit peering might incur cost, while traffic exchanged over a direct peering might be settlement free.   As a result a typical mapping of BGP session types to Local_Pref values could look like this:                  BGP Session       Local_Pref                       Private Peering       500                 Direct Peering via IXP       400                 Peering via IXP Route Server       300                 Transit       200           Here we generally prefer settlement-free peering with higher Local_Pref over paid transit with lower Local_Pref.   AS_PATH length   A common mechanism to manage traffic across AS with BGP is to make a BGP AS_PATH longer via AS path prepending. Prepending means adding one or more AS numbers to the left side of the AS path. Normally this is done using one’s own AS number, while announcing routes to another AS.   With that we can influence how traffic will reach our ASN. Similar to what I described before we might not only have a commercial interest in reducing the cost that we pay for Transit for traffic leaving our ASN, but also for traffic entering our ASN. We have seen that we can perform traffic engineering for egress traffic via Local_Pref, using AS path prepending can be used for traffic engineering on ingress traffic.   With this we could use AS path prepending for IP prefixes originated or announced by our ASN over various types of BGP session types like this to optimize for cost:                  BGP Session       AS path prepending                       Private Peering       None                 Direct Peering via IXP       1x                 Peering via IXP Route Server       2x                 Transit       3x           In this case we tell other ASNs to prefer path via our settlement-free peering through lower AS_PATH length over paid transit through longer AS_PATH length.   Multi-Exit Discriminator (MED)   Multi-Exit Discriminator (MED)                              Figure 2: Multi-Exit Discriminator (MED) suggests how traffic should enter an ASN.     CSR1000V#sh ip bgp BGP table version is 292, local router ID is 1.1.1.1 Status codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal,               r RIB-failure, S Stale, m multipath, b backup-path, f RT-Filter,               x best-external, a additional-path, c RIB-compressed,               t secondary path, L long-lived-stale, Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found       Network          Next Hop            Metric LocPrf Weight Path       0.0.0.0          0.0.0.0                                0 i  *&gt;   10.0.1.0/24      0.0.0.0                  0         32768 i  *&gt;   10.0.16.0/24     0.0.0.0                  0         32768 i  *&gt;   172.16.0.0/24    169.254.63.25          100             0 64512 i  *                     169.254.39.225         200             0 64512 i   Summary   Fill me out  ","categories": ["EdgeCloud"],
        "tags": ["BGP","Network"],
        "url": "https://www.edge-cloud.net/2020/09/25/understanding-bgp/",
        "teaser":null},{
        "title": "Better understanding IP Routing",
        "excerpt":"Today we will look at some of the fundamental principles of IP routing. While these principles and concepts are generic, we will use examples based on AWS networking.   This blog post is not intended to be an all encompassing primer on IP routing. Instead I’ve seen numerous people confused by some of these principles and concepts while either designing networks or troubleshooting them. Therefore it appears to be a good idea to highlight and explain them explicitly.   In a future related blog post we will look at BGP routing in more detail.   Please keep in mind that we will be using AWS VPCs and TGWs to illustrate routing principles. The resulting AWS networking designs are therefore used for illustration purposes and are not suited or recommended for production deployments.   Routing   Routing and more specifically here, IP routing, deals with selecting a path for traffic in an IP network. Routing directs a “hop” within a network on how to  forward IP packets based on a routing tables and the destination IP address within the packet.   As we will see later, routing tables maintain information on how to reach various network destinations. Typically they are either configured manually (also known as “Static Routing”) or with the help of a routing protocol (e.g. BGP).   Hop-by-Hop Routing   One of the most fundamental concepts to understand in IP routing is that the actual forwarding decision is made on a hop-by-hop basis. This means that within each hop of the network path, a router makes a forwarding decision based on the local route table. Image this to be like a board game, where at each step in the game it is decided where to go next. Neither the previous nor the next step have any influence on the local decision.   Taking AWS VPCs and Transit Gateways (TGWs) as an example, we can quickly understand how this hop-by-hop decision making plays out, while looking at the routing tables of the VPCs and TGWs (See Figure 1).                              Figure 1: IP Hop-by-Hop routing with VPCs and multiple Transit Gateways (TGW)     Traffic from an EC2 instance in VPC 1 wanting to reach another EC2 instance in VPC 2 will have to follow this hop-by-hop process through the five routing tables involved here. What do you think? Will traffic from VPC 1 reach VPC 2? Or is there a mistake in the route tables?   Let’s look at each hop, step-by-step:     VPC 1: Traffic for destinations within VPC 2’s CIDR of 10.2.0.0/16 are send to the TGW 1 over the VPC attachment.   TGW 1: Inspecting the route table of TGW 1, we can see that traffic for 10.2.0.0/16 is send via a TGW peering to TGW 2.   TGW 2: Looking at the route table of TGW 2, we can also find an entry for the destination of 10.2.0.0/16. It specifies that traffic should be send via another TGW peering to TGW 3.   TGW 3: At this point our traffic has already made it into the correct AWS regions. Let’s see what happens next: The route table of TGW 3 indicates that traffic for 10.2.0.0/16 will be forwarded to VPC 2.   VPC 2: Last but not least, the route table of VPC 2 shows that traffic for the locally used CIDR 10.2.0.0/16 remains within the VPC and is delivered to the corresponding EC2 instance.   But what about the return traffic from VPC 2 to VPC 1? Read on to see how another important principle of IP routing plays a role here.   Directional   Another important principle of IP routing, effectively caused by the hop-by-hop decision making behavior is that path determination is directional. Looking back at the provided example in the previous section (See Figure 1), we only validated that traffic from VPC 1 can reach VPC 2. But we did not validate any information on whether traffic from VPC 2 can reach VPC 1.   I leave it up to you as an exercise to determine if the route tables across the VPCs and TGWs are setup correctly to allow return traffic and thereby enable bidirectional communication. Comment below in case you find a mistake.   When designing route tables or troubleshooting network connectivity it’s always important that you look at traffic flows in both directions and plan or check route table independently for both directions. Also when talking with co-workers, customer, support staff, or anyone alike it is also important that you indicate the direction of the traffic flow that you are referring to.   Asymmetric Routing   What’s even more interesting is that the directional nature of IP forwarding can lead to asymmetric traffic flows. But there is nothing wrong about asymmetric traffic flows and the majority of the Internet relies on it while exchanging traffic between ASNs via Peering or Transit. Think of asymmetric IP traffic flow as a hiking trail loop (See Figure 2). Such hiking trails are often more fascinating than an out-and-return path as you get to see a different set of landscape, plants and animals on the way back as compared to the way out.                              Figure 2: Asymmetric routing is like a hiking-trail loop.     And as long as you make the correct decision at your “routing hops” - aka. a trail fork - you will return to your trail head as well.   Let’s extend the above example using AWS VPCs and TGWs to showcase asymmetric routing. For this we add another TGW and two more TGW peering connections along with changes to the route table (See Figure 3).                              Figure 3: Asymmetric routing with VPCs and multiple Transit Gateways (TGW)     Now, if you follow the path of traffic from VPC 1 to VPC 2, you’ll notice that nothing has changed. Traffic still traverses TGW 1, TGW 2, and TGW 3 on the way to VPC 2. But at the same time look at traffic from VPC 2 to VPC 1. What do you notice? Looking at the route tables of the TGWs you should notice that traffic  on the return path from VPC 2 to VPC 1 will traverse TGW 3, TGW 4, and TGW 1, thereby creating and asymmetric path.   This asymmetric traffic flow is depicted with the green arrows.   Route Tables   Next we will look at route tables in a bit more detail. Being able to read and understand route tables, will help you understand the routing decision of the hops within each path.   The most simple route tables have already been depicted in Figure 1 and Figure 3. These routes show a simple mapping between the destination CIDR - also called prefix or network - and the next hop.   Translated into a route table on a Cisco device this might look like this:   CSR1000V-01#sh ip bgp BGP table version is 297, local router ID is 1.1.1.1 Status codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal,               r RIB-failure, S Stale, m multipath, b backup-path, f RT-Filter,               x best-external, a additional-path, c RIB-compressed,               t secondary path, L long-lived-stale, Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found       Network          Next Hop            Metric LocPrf Weight Path       0.0.0.0          0.0.0.0                                0 i  *&gt;   10.0.1.0/24      0.0.0.0                  0         32768 i  *&gt;   10.0.16.0/24     0.0.0.0                  0         32768 i  *&gt;   10.1.0.0/16      169.254.15.221         100             0 64512 i   Focus on the last line, which effectively translates into: Packets for the prefix “10.1.0.0/16” should be send to the next hop with the IP address of “169.254.15.221”.   Longest prefix match   After this let’s look at longest prefix match, sometimes also referred to as “more specific routing”. This algorithm specifies which entry to be chosen from the IP routing table in case of destination addresses matching more than one entry. For IP routing the most specific of the matching table entries — the one with the longest subnet mask — is called the longest prefix match and is the one chosen.   Consider the below routing table on a Cisco device as an example and especially focus on the last five lines:   CSR1000V-01#sh ip bgp BGP table version is 297, local router ID is 1.1.1.1 Status codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal,               r RIB-failure, S Stale, m multipath, b backup-path, f RT-Filter,               x best-external, a additional-path, c RIB-compressed,               t secondary path, L long-lived-stale, Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found       Network          Next Hop            Metric LocPrf Weight Path       0.0.0.0          0.0.0.0                                0 i  *&gt;   10.0.1.0/24      0.0.0.0                  0         32768 i  *&gt;   10.0.16.0/24     0.0.0.0                  0         32768 i  *&gt;   10.1.0.0/16      169.254.15.221         100             0 64512 i  *&gt;   10.1.1.0/24      169.254.16.222         100             0 64513 i  *&gt;   10.1.2.0/24      169.254.17.223         100             0 64514 i  *&gt;   10.1.3.0/24      169.254.18.224         100             0 64515 i  *&gt;   10.1.4.0/24      169.254.19.225         100             0 64516 i   Here we can see that the destination IP address of “10.1.1.1” would match both the entry for “10.1.0.0/16”, as well as the entry for “10.1.1.0/24”. As the entry for “10.1.1.0/24” has a longer subnet mask - it is more specific - and therefore the chosen entry. With that this entry would be chosen and matching traffic send to 169.254.16.222 as the next hop.   Equal Cost Multipath (ECMP)   Usually with IP forwarding there is one egress or outbound path per hop for a given destination IP. This rule can be softened via a routing strategy called Equal-cost multi-path routing (ECMP). With ECMP, packet forwarding to a single destination IP can occur over multiple “best path”.   Again, let’s have a look at an example and consider the below routing table on a Cisco router, especially the last two lines:   CSR1000V#sh ip bgp BGP table version is 297, local router ID is 1.1.1.1 Status codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal,               r RIB-failure, S Stale, m multipath, b backup-path, f RT-Filter,               x best-external, a additional-path, c RIB-compressed,               t secondary path, L long-lived-stale, Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found       Network          Next Hop            Metric LocPrf Weight Path       0.0.0.0          0.0.0.0                                0 i  *&gt;   10.0.1.0/24      0.0.0.0                  0         32768 i  *&gt;   10.0.16.0/24     0.0.0.0                  0         32768 i  *m   10.0.255.0/24    169.254.13.253         100             0 64512 i  *&gt;                    169.254.15.221         100             0 64512 i   In this case we can see that we have a multipath route for the destination prefix of “10.0.255.0/24”, where both “169.254.13.253” and “169.254.15.221” are considered as the next best hop. In this case the router device will randomly send out traffic for this destination network over either next hop, while using a 5-tuple hash. A 5-tuple hash refers to a set of five different values that comprise a Transmission Control Protocol/Internet Protocol (TCP/IP) connection. It includes a source IP address/port number, destination IP address/port number and the protocol in use. Here with ECMP and 5-tuple hashing, packets belonging to the same 5-tuple travel to the same next hop, while packets from different 5-tuple may be send to another next hop.   Summary   In today’s post took a look at some of the fundamental principles of IP routing. A future post will look in more detail at  BGP Routing protocol concepts. Neither of these blog posts is intended to be an all encompassing primer on IP routing or BGP. Instead I’ve seen numerous people confused by some of these principles and concepts while either designing networks or troubleshooting them. Hopefully after reading through this post you feel a bit more confident to design, troubleshoot or just talk about IP networks.  ","categories": ["EdgeCloud"],
        "tags": ["BGP","Network"],
        "url": "https://www.edge-cloud.net/2020/09/18/understanding-routing/",
        "teaser":null},{
        "title": "Configuring F5 Big-IP LTM with VMware vCloud Director [Updated]",
        "excerpt":"While there are numerous instructions and blog posts out there which try to show the load balancing configuration of an F5 Big-IP LTM with VMware vCloud Director (vCD), none of them proved to be complete and comprehensive. This post attempts to give this complete and comprehensive walk through guide.   Update June 06, 2013: In the original version of this article, I described the usage of a standard F5 TCP monitor for the vCD Console Proxy. This will lead to a log pollution, as each connection by the F5 monitor is logged as a failed attempt on vCD. Thus this revised guide makes usage of a console proxy feature to access the SDK to monitor the health of a cell for the Console Proxy.   1. Logical setup of a load-balanced VMware vCloud Director configuration   The goal will be to frontend two or more VMware vCloud Director cells with a F5 Big-IP LTM as shown in Figure 1. HTTPS and Console Proxy traffic shall be forwarded to one of the cells and HTTP traffic shall be terminated on the load balancer and answered with a HTTP 301 (Redirect) answer to the https://… URL.                              Figure 1: VMware vCloud Director with an F5 Big-IP LTM load balancer     Each VMware vCloud Director cell is configured with at least two IP addresses. One to terminate HTTP and HTTPS communication to access the web-based GUI as well as the REST API, the other one for accessing the Console Proxy. As a result also the F5 Big-IP LTM needs to host two Virtual IP (VIP) to offer the corresponding services.   Let’s look at the desired traffic flow for HTTPS, HTTP and Console Proxy in detail:   1.1 HTTPS   As shown in Figure 2, HTTPS traffic will reside on IP 1 with the port TCP/443 of the LTM and be load balanced across the cells to port TCP/443 on their IP 1. This traffic is standard HTTPS traffic and used for accessing the web-based GUI by either Administrators or End-Users as well as API access. It is therefore possible to use L7 load balancing and apply WAN acceleration as well as SSL termination to this traffic. This virtual server will later be accessible e.g. under the URL https://vcd.edge-cloud.net                              Figure 2: Desired traffic flow for HTTPS     1.2 HTTP   HTTP traffic is automatically redirected by VMware vCloud Director cells to HTTPS. Instead of forwarding HTTP traffic to the vCD cells, this redirect should be performed by the LTM as shown in Figure 3. Using an F5 iRule script this also gives the added benefit of specifying the destination of this redirect. As an example http://vcd.edge-cloud.net/ could be redirected to https://vcd.edge-cloud.net/cloud/org/MyOrg/.                              Figure 3: Desired traffic flow for HTTPS     1.3 Console Proxy   Although the Console Proxy uses the port TCP/443, the VMware Remote Console (VMRC) traffic across it, is not HTTPS traffic. Instead it is a VMware proprietary protocol that is tunneled through this well-known port. It is therefore only possible to use L4 load balancing and cannot use additional WAN acceleration or SSL termination. As port TCP/443 is also already used for the web-based GUI on IP 1, the Console Proxy has to be bound to IP 2.   To make things a bit more confusing, the Console Proxy can actually reply to standard HTTPS requests and thus acts like a standard web server in some way. We will use this capability to monitor the Console Proxy for health. More later in section 2.1.2.                              Figure 4: Desired traffic flow for Console Proxy     2. Configuring F5 Big-IP LTM   Configuring the F5 Big-IP LTM to reside in front of VMware vCloud Director cells requires a common set of configuration steps:      Creating custom health monitors for vCD HTTPS and Console Proxy   Creating member pools with the vCD cells to distribute requests among   Creating an iRule for the HTTP to HTTPS redirect   Creating the virtual servers accessible by end-users for HTTPS and Console Proxy   2.1 Creating a custom health monitor for vCD HTTPS and Console Proxy   2.1.1 HTTPS Health Monitor   A VMware vCloud Director cell offers the special URL https://hostname-of-cell/cloud/server_status, which shows whether the web-based GUI and API on that cell is fully functional or not. This provides advanced health information beyond e.g. just monitoring port TCP/443 for accessibility. Therefore we will create a custom health monitor (Figure 5) within F5 Big-IP to make use of this capability.                              Figure 5: Custom HTTP monitor in F5 Big-IP for VMware vCloud Director        Step 1: Click on the “plus” icon right next to Monitors to create a new monitor.   Step 2: Make sure to select HTTPS as the parent monitor type.   Step 3: Reduce the Interval and Timeout value for this monitor to be a bit more aggressive in detecting failures in a timely fashion.   Step 4: Enter the Send String GET /cloud/server_statusrn. This string includes the above mentioned URL.   Step 5: Enter the Receive String “Service is up.”. This string is the expected string for the case that the service is healthy.   2.1.2 Console Proxy Health Monitor   The actual VMware Remote Console (VMRC) traffic is only using the well-known port of 443, but is itself not HTTPS traffic. Nevertheless, the Console Proxy on the vCD cells does react to certain types of HTTPS requests and delivers information back via HTTPS.   Although it would be sufficient to configure the F5 load balancer with a standard TCP monitor – that monitors if port TCP/443 is alive – this leads to log file pollution on the corresponding vCD cells. Each such monitor request would be considered a failed connection and logged accordingly.   Thus a better approach is to use the possibility to connect with a standard webbrowser to the special URL https://hostname-of-console-proxy-cell/sdk/vimServiceVersions.xml, which delivers back an XML file with information about the Console Proxy version.   While the Console Proxy is healthy and able to proxy VMRC traffic, it will serve this XML file. Therefore we will create a custom health monitor (Figure 6) within F5 Big-IP to make use of this capability.                              Figure 6: Custom Console Proxy monitor in F5 Big-IP for VMware vCloud Director        Step 1: Click on the “plus” icon right next to Monitors to create a new monitor and make sure to select HTTPS as the parent monitor type.   Step 2: Reduce the Interval and Timeout value for this monitor to be a bit more aggressive in detecting failures in a timely fashion.   Step 3: Enter the Send String GET /sdk/vimServiceVersions.xml HTTP/1.1rnrnConnection: Closernrn. This string includes the above mentioned URL.   Step 4: Enter the Receive String urn:vim25. This string is the expected string for the case that the service is healthy.   2.2 Creating member pools with the vCD cells to distribute requests among   As shown in the Logical Setup section each vCD cell consist of two IP addresses. One used for the web-based GUI and API, the other one for the Console Proxy. As such we need to create two member pools (Figure 7):      HTTPS Pool: This pool includes IP 1 of all vCD cells and is used to distribute HTTPS request for the web-based GUI and API.   Console Proxy Pool: This pool includes IP 2 of all vCD cells and is used to distribute the Console Proxy requests.                              Figure 7: Overview of the pools to be created     Let’s start with the HTTPS pool:                              Figure 8: HTTPS pool in F5 Big-IP for VMware vCloud Director     Select the previously created custom health monitor for vCD as Health Monitor.   For the members specify the IP 1 of all your vCD cells along with HTTPS as the service port.   Next, create the Console Proxy pool:                              Figure 9: Console Proxy pool in F5 Big-IP for VMware vCloud Director     Ensure that tcp is selected as health monitor and not e.g. https. Keep in mind that even though the Console Proxy used the port TCP/443, it is not HTTPS traffic.   For the members specify the IP 2 of all your vCD cells along with HTTPS (port 443) as the service port. The selection of HTTPS will only enter 443 as the port but does not have any influence on the F5 Big-IP treating this traffic as HTTPS traffic.   2.3 Creating an iRule for the HTTP to HTTPS redirect   A custom iRule will allow you to redirect users accessing the IP 1 of the Big-IP load balancer via HTTP to the desired URL of the vCloud Service including the HTTPS part. This destination could e.g. be https://vcd.edge-cloud.net/cloud/ to access the web-based admin UI or https://vcd.edge-cloud.net/cloud/org/MyOrg/ to reach the web-based UI for a certain Org VDC.                              Figure 10: Custom iRule for redirecting HTTP traffic to a custom vCD URL     2.4 Creating the virtual servers accessible by end-users for HTTPS and Console Proxy   To implement the Logical Setup shown above we need to create a total of three virtual servers on the F5 Big-IP LTM (Figure 11):      HTTPS Virtual Server: This server is attached to IP 1 of the Big-IP LTM and serves web-based GUI and API.   HTTP Virtual Server: This server is attached to IP 1 of the Big-IP LTM and redirects clients to the HTTPS virtual server.   Console Proxy Server: This server is attached to IP 2 of the Big-IP LTM and serves the Console Proxy.                              Figure 11: Overview of Virtual Servers to be created     Let’s start with the HTTPS Virtual Server:                              Figure 12: Virtual Server for HTTPS - General Properties and Configuration        Step 1: Specify the IP 1 of the Big-IP LTM under which you will serve the VMware vCloud Director.   Step 2: Specify HTTPS as the “Service Port”.   Step 3: Choose http as the “HTTP Profile”. This Virtual Server serves standard HTTPS content and can therefore make use of L7 load balancing.   Step 4: Pick wan-optimized-compression as “HTTP Compression Profile”. This will compress this standard HTTPS content stream.   Step 5: Pick the SSL certificate corresponding to your VMware vCloud Director service in the “SSL Profile (Client)”. As we are using L7 load balancing, the SSL connection will actually be terminated on the F5 Big-IP LTM.   Step 6: Specify serverssl-insecure-compatible as the SSL Profile (Server).   Step 7: Choose Auto Map as the SNAT Pool to make use of Static NAT. This is for the case where your VMware vCloud Director cell’s IP addresses are not directly accessible by the clients.                              Figure 13: Virtual Server for HTTPS – Resources        Step 8: As the Default Pool choose the HTTPS pool created in step 2.2.   Step 9: As the Default Persistence Profile choose source_address   Although a once established VMRC connection remains connected through the same vCD irrespective of the above setting, new and re-established connections might be distributed by the F5 to another cell. If this connection is not a new VMRC connection, but a re-established one, vCD will complain in it’s log files and the connection might fail. The above setting will pin all connections originating from the same source address to the same vCD cell and thereby prevent any issues.   Next we will create the HTTP Virtual Server:                              Figure 14: Virtual Server for HTTP - General Properties and Configuration        Step 1: Specify the IP 1 of the Big-IP LTM under which you will serve the VMware vCloud Director.   Step 2: Specify HTTPS as the “Service Port”.   Step 3: Choose http as the “HTTP Profile”. This Virtual Server serves standard HTTPS content and can therefore make use of L7 load balancing.                              Figure 15: Virtual Server for HTTP - Resources        Step 4: As iRule choose the custom iRule created in step 2.3.   Last let’s create the Console Proxy Virtual Server:                              Figure 16: Virtual Server for Console Proxy - General Properties and Configuration        Step 1: Specify the IP 1 of the Big-IP LTM under which you will serve the VMware vCloud Director.   Step 2: Specify HTTPS as the “Service Port”.   Step 3: Choose none as the “HTTP Profile”. This Virtual Server serves the Console Proxy and can therefore only make usage of L3 load balancing (TCP).   Step 4: Choose Auto Map as the SNAT Pool to make use of static NAT. This is for the case where your VMware vCloud Director cell’s IP addresses are not directly accessible by the clients.                              Figure 17: Virtual Server for Console Proxy - Resources        Step 5: As the Default Pool choose the Console Proxy pool created in step 2.2.   4.0 Configuring VMware vCloud Director   As a last step the DNS names mapped to the IP addresses of the virtual servers above need to be configured within VMware vCloud Director under Administration -&gt; System Settings -&gt; Public Addresses. Refer to Figure 16 for an example.                              Figure 18: Configuring Public Addresses in vCD     4 Additional Comments   4.1 Session persistence   Although not required it is possible to configure the Virtual Server for HTTPS with session persistence (Figure 12), similar to the Console Proxy. Possible options would be Cookie-based or Source-IP address-based.   4.2 SSL termination   Unfortunately VMware vCloud Director does not allow disabling of HTTPS in favor of HTTP. This would allow termination of the SSL session on the F5 Big-IP LTM – as the configuration above showcases – while at the same time offloading the vCD cell from SSL encryption/decryption load. Thus traffic from the load balancer to the cells still need to be SSL encrypted.  ","categories": ["EdgeCloud"],
        "tags": ["F5","VMware"],
        "url": "https://www.edge-cloud.net/2013/05/20/configuring-f5-big-ip-with-vcd/",
        "teaser":null},{
        "title": "Infoblox vNIOS HA pair VIP unreachable when deployed on vSphere",
        "excerpt":"Yesterday I stumbled over an interesting networking problem while deploying an Infoblox vNIOS IPAM HA pair on a fresh installation of VMware vSphere: After setting up the vNIOS appliances to act as an HA pair, it’s floating virtual IP address was not reachable from the rest of the network. Yet, at the same time the individual IP addresses of the LAN interface were reachable.   Rootcause   The cause for this issue is rooted in the way Infoblox implements the HA functionality - which is similar to the implementation of HA in various other product - but especially to the default security settings of a vDS and vSwitch in vSphere.                              Figure 1: Setup of an Infoblox vNIOS HA pair     With Infoblox vNIOS both nodes in an HA pair share a single VIP address but also a single virtual MAC address. The node that is currently active is the one whose HA port owns the VIP address and virtual MAC address. When a failover occurs, these addresses shift from the HA port of the previous active node to the HA port of the new active node, as illustrated in Figure 1.   In detail, Infoblox uses the Virtual Router Redundancy Protocol (VRRP) with the MAC address 00:00:5e:00:01:vrrp_id. The last two hexadecimal numbers in the source MAC address indicate the VRID number for this HA pair. For example, if the VRID number is 143, then the source MAC address is 00:00:5e:00:01:8f (8f in hexadecimal notation = 143 in decimal notation).   The default settings of a vDS or vSwitch in vSphere only allow a single MAC address per vNIC. This behavior is similar to having port-security enabled on a physical switch. But in contrary to physical switches the allowed MAC address is not learned but is the MAC address assigned by vCenter to the given vNIC.   As a result frames from the above mentioned floating MAC address are discarded by the vDS or vSwitch, which causes the associated IP address to be unreachable.                              Figure 2: Allow MAC address changes and Forged Transmits on a vDS     Fix   In order to fix this issue, the port-profile to which the vNIOS HA and LAN ports connect to, have to allow more than one MAC address per vNIC. This can be done by changing the security settings of the port-group to accept “MAC address changes” and “Forged transmits”, as shown in Figure 2.  ","categories": ["EdgeCloud"],
        "tags": ["Infoblox","VMware","DNS"],
        "url": "https://www.edge-cloud.net/2013/05/21/infoblox-vnios-ha-pair-vip-unreachable-when-deployed-on-vsphere/",
        "teaser":null},{
        "title": "Network device configuration management with Rancid and Trac on Ubuntu 12.04 LTS",
        "excerpt":"Inroduction   Today we want to look at the possibility to automatically save the text-based configuration of network devices and make them browse-able via a web-based interface. The solution will also discover configuration changes and notify the network operations team of these changes.   To do so we will be using RANCID (Really Awesome New Cisco confIg Differ) from Shrubbery Networks as well as TRAC (Integrated SCM &amp; Project Management).   RANCID monitors a router’s (or more generally a device’s) configuration, including software and hardware (cards, serial numbers, etc) and uses CVS (Concurrent Version System) or Subversion to maintain history of changes and notify users of these. TRAC is a web-based wiki and issue tracking system for software development projects. It provides an interface to ​Subversion or ​Git, which is the primary reason for using it in this project.   Prerequisites   This documentation assumes that a healthy Ubuntu 10.04.4 LTS Server, fully functioning, up-to-date system is available. This e.g. includes a combination of sudo apt-get update, sudo apt-get upgrade, sudo apt-get dist-upgrade, and sudo apt-get autoremove being run. Please understand what the commands do before blindly running them as any system update has the potential to render a system inoperable. The server should be deployed with at least the software selection of OpenSSH server and LAMP server.   This documentation is accurate as of May 31, 2013. These steps have been performed on Ubuntu 10.04.4 Server systems and confirmed to work as described here.   Update from June 27, 2014: Trac has added support for Git in never versions. In order to unify configuration of the various version control systems in Trac, the syntax for specifying what system you are using has changed. In Trac 1.0 (trunk, starting from r11082), the components for Subversion support have been moved below tracopt. So you need to explicitly enable them in your Components section within trac.ini. See here fore more details. You will need this information in the “Final Customization” section below.   RANCID   Installing RANCID   Install RANCID as well as Subversion via the Ubuntu software repository:   sudo apt-get install rancid subversion   Configuring RANCID   The installation creates a new user and group named “rancid” with a home directory of /var/lib/rancid. Now, we must create at least one device group in RANCID to logically organize our devices. Groups can be based on any criteria you wish. So if you’ve got one physical location you could create “router”, “firewall”, and “switch” groups, or, in larger environments with multiple physical locations, group names such as “Los Angeles”, “San Francisco”, and “New York” may be a better choice. Or in smaller setups you could chose to use a single group. For this example we’ll create a single group called “network”.   Before editing the sample file, it’s good practice to start by making a backup copy of the original rancid.conf file.   sudo cp /etc/rancid/rancid.conf /etc/rancid/rancid.conf.ORIGINAL   Open the file “/etc/rancid/rancid.conf” in your favorite text editor, add a line similar to the following, and save and exit.   LIST_OF_GROUPS=\"network\"   By default RANCID uses CVS to store the retrieved device configuration. But we want to use Subversion (SVN) instead, as Trac is build around this source code repository system.   Therefore locate the current CVS configuration within the file /etc/rancid/rancid.conf and change it to SVN:   CVSROOT=$BASEDIR/CVS; export CVSROOT RCSSYS=cvs; export RCSSYS   Change these settings to SVN:   CVSROOT=$BASEDIR/SVN; export CVSROOT RCSSYS=svn; export RCSSYS   Configure E-Mail Notification   Next we want to configure the system, so that RANCID can send e-mail notifications for changes performed on each group of devices. For this a local MTA should be installed and configured.   For this documentation we will assume that this MTA will be Postfix, configured with as “Internet with smarthost”.   Install Postfix   sudo apt-get install postfix   As the mail configuration type choose “Internet with smarthost”:                              Figure 1: Postfix mail configuration type     Next, specify your e-mail domain as the system mail name:                              Figure 2: Postfix System Mail Name     Last, specify the hostname or IP address of your SMTP smart relay:                              Figure 3: Postfix SMTP smartrelay     Now that the MTA has been configured, we can create e-mail aliases in the MTA’s configuration file for each RANCID device group. By default on Ubuntu this is the “/etc/aliases” file.   For each group that you created, we need to add two aliases to the aliases file named “rancid-” and “rancid-admin-”. Open up the **/etc/aliases** file in a text editor and add lines similar to the following:   rancid-network:               your_email@address.com rancid-admin-network:         your_email@address.com   After saving your changes and exiting, you’ll need to let your MTA know about the changes by running:   sudo /usr/bin/newaliases   Subversion Repository   Your device’s configuration files will be stored in a Subversion (SVN). This provides a way to track changes over time as well as provides you with a bit of disaster recovery. In order to prepare SVN we must create a folder structure based off of the RANCID groups that we created earlier. This command needs to be run as the “rancid” user that was created when the RANCID software was first installed.   [user@netconf ~]$ sudo su - rancid [rancid@netconf ~]$ /var/lib/rancid/bin/rancid-cvs  Committed revision 1. Checked out revision 1. At revision 1. A         configs Adding         configs  Committed revision 2. A         router.db Adding         router.db Transmitting file data . Committed revision 3.   Assuming that runs without any errors, you should see one or more new directories created under “/var/lib/rancid”, named according to the RANCID groups you defined earlier (e.g. “/var/lib/rancid/network”). Inside each will be a file named “router.db”:   [rancid@netconf ~]$ find /var/lib/rancid -type f -name router.db /var/lib/rancid/network/router.db   Make the SVN readable by the www-data group - used by the Apache web server, so it can be accessed by TRAC:   [rancid@netconf ~]$ exit [user@netconf ~]$ sudo chgrp -R www-data /var/lib/rancid/SVN/ [user@netconf ~]$ sudo chmod -R g+r /var/lib/rancid/SVN/   Specify devices in the router.db Files   Inside each of these “router.db” files is where we let RANCID know what devices exist in each location. A single line in each file is used to identify a single device. The format of the definitions is of the format “hostname:type:status”, where “hostname” is the fully-qualified domain name or IP address, “type” defines the type of device (e.g. “cisco”, “hp”, “foundry”, etc.) and “status” is either “up” or “down”. If “status” is set to “down”, RANCID will simply ignore the device.   Sample entries might look like this:   router.edge-cloud.net:cisco:up firewall.edge-cloud.net:juniper:down switch.edge-cloud.net:arista:down   Refer to man router.db for more details   Device login credentials via cloginrc   Once you have successfully added your devices to the appropriate “router.db” files, we need to let RANCID know how to access the devices (telnet, SSH, etc.) and what credentials to use to login. This is done via the “.cloginrc” file that exists in the rancid user’s home directory (“/var/lib/rancid/.cloginrc”, by default).   It is a good security practice to never connect to devices via telnet, so this guide will only cover the SSH method of connecting to a device. Other connection methods are also supported. The way the .cloginrc file is configured also depends on how the end device is configured to authenticate users. Users can be configured locally or a device can authenticate users agains an enterprise system such as RADIUS, LDAP or Active Directory. It gets complicated quickly so make sure that you take your time and read the documentation all the way through.   Refer to man cloginrc to see the details of all the available options and keywords available for use.   This guide will assume the simplest setup in which local usernames and passwords are defined on the end devices themselves. Keep in that mind that the file .cloginrc should not be world readable/writable and be owned by the user rancid, created earlier.   Here’s some example information for a .cloginrc file:   #Firewall add method firewall.edge-cloud.net ssh add user firewall.edge-cloud.net rancid add password firewall.edge-cloud.net user_password enable_password   Testing   The basic test utilizes the clogin application to verify login into a device:   [user@netconf ~]$ sudo su -c \"/usr/lib/rancid/bin/clogin -f /var/lib/rancid/.cloginrc firewall.edge-cloud.net\" -s /bin/bash -l rancid   The clogin application will use the .clogin configuration file specified by the -f variable and will automatically login to the device named firewall.edge-cloud.net. When it’s all said and done you should end up in enable mode on the firewall device. If there are problems, clogin does an excellent job of providing pointed advice on what is wrong.   With RANCID now configured, it’s time to test it out! Let’s manually invoke “rancid-run” (as the “rancid” user) to see if everything works!   [user@netconf ~]$ sudo su -c /usr/lib/rancid/bin/rancid-run -s /bin/bash -l rancid   This command may take a while to run, depending on how many devices you have configured. Be patient and, when it finishes, review the logfiles created in “/var/log/rancid”.   Assuming all goes well, you should receive e-mails from RANCID sent to the addresses that you defined in earlier in “/etc/aliases”.   Automation   Once everything is working, it’s time to automate the collection and archiving. The easiest way to do this is to simply create a cronjob under the rancid user that calls “rancid-run” for us on a periodic basis. Here we have RANCID run every 15 minutes to ensure that all network changes are caught quickly.   [user@netconf ~]$ sudo su -c \"/usr/bin/crontab -e -u rancid\"   Modify the contents of the file so that you end up with something like this.      # m h  dom mon dow   command     */15 * * * * /usr/bin/rancid-run   Trac   Install Trac   Next we will install and configure Trac as a web-based GUI to browse the device configuration stores in SVN.   Install Trac and the necessary Apache modules via the Ubuntu software repository:   sudo apt-get install trac libapache2-mod-python   Configure a new Trac project environment:   sudo mkdir -p /var/trac/netconf cd /var/trac/netconf sudo trac-admin . initenv Netconf sqlite:db/trac.db sudo htpasswd -bc .htpasswd adminusername &amp;lt;mypassword&amp;gt; sudo trac-admin . permission add adminusername TRAC_ADMIN sudo chown -R www-data: . sudo chmod -R 775 .   Next, configure your Apache Webserver for Trac. In this example we will replace the default Apache Website and therefore replace the file /etc/apache2/sites-available/default with the following content:       &lt;VirtualHost *:80&gt;             ServerName netconf.edge-cloud.net             &lt;Location /&gt;                SetHandler mod_python                PythonInterpreter main_interpreter                PythonHandler trac.web.modpython_frontend                PythonOption TracEnv /var/trac/netconf                PythonOption TracEnvParentDir /var/trac/netconf                PythonOption TracUriRoot /                 # PythonOption TracEnvIndexTemplate /var/local/trac/templates/index-template.html                PythonOption TracLocale en_US.UTF8                PythonOption PYTHON_EGG_CACHE /tmp                Order allow,deny                Allow from all             &lt;/Location&gt;             &lt;Location /login&gt;               AuthType Basic               AuthName \"Netconf\"               AuthUserFile /var/trac/netconf/.htpasswd               Require valid-user             &lt;/Location&gt;     &lt;/VirtualHost&gt;  Restart the Apache service with sudo service apache2 restart.   Enabling SVN in TRAC   The TRAC SVN browser is disabled at this stage as the SVN path hasn’t been configured yet. Let’s configure the SVN path in TRAC now.   Edit the TRAC configuration file /var/trac/netconf/conf/trac.ini.   Add the SVN repository:   repository_dir = /var/lib/rancid/SVN/   Final customizations   Although your installation should be running at this point, we want to perform some final customization to “pretty it up”. This includes adding a logo to TRAC, enabling the SVN browser and disabling any module not used.   Make the SVN Browser the default module to load when accessing the web interface:   [trac] ... default_handler = BrowserModule   Disable all modules that you don’t need. TRAC’s Wiki and Ticket module could very well be used for network documentation purposes as well as tracking configuration change requests. In this documentation we will focus on solely using the SVN browser.   Add the following block yo your TRAC configuration file:   [mainnav] wiki = disabled timeline = disabled roadmap = disabled tickets = disabled newticket = disabled search = disabled  [metanav] login = disabled logout = disabled prefs = disabled help = disabled   Place the file of a logo - e.g. netconf.png - for your TRAC website into the folder /var/trac/netconf/htdocs/ and enable it within the TRAC configuration file /var/trac/netconf/conf/trac.ini:   [header_logo] alt = NetConf height = 72 link = / src = site/netconf.png width = 236   Results   The result of your TRAC website will look like this:                              Figure 4: TRAC source code browser     Drilling down into your device groups and actual device will then reveal the current configuration of a device:                              Figure 5: Current configuration of a device     TRAC offers multiple very interesting features for e.g. downloading the current configuration as a text file or comparing differences between revisions. Enjoy your new network device configuration management solution.  ","categories": ["EdgeCloud"],
        "tags": ["Management","Network"],
        "url": "https://www.edge-cloud.net/2013/05/31/rancidtrac-on-ubuntu-12-04-lts/",
        "teaser":null},{
        "title": "Measuring Network Throughput",
        "excerpt":"The topic of measuring network throughput between network devices comes up quite frequently: It ranges from users claiming (and sometimes almost blaming) that the 100 Mbps Internet uplink in reality is only 10 Mbps to being surprised why they can’t transfer that multi-gigabyte file via FTP faster between data center locations.   Let’s have a look behind the scenes of network throughput measurement and understand why users are often actually measuring something completely different, but also how to get more “performance” out of these connections.   Sliding window protocols   Most user utilize software based on the Transmission Control Protocol (TCP) for measuring the network throughput. It is very important to keep in mind that TCP is a sliding window protocol.                              Figure 1: Sliding Window Protocol     In order to guarantee reliable in-order delivery of packets, only a “window” of packets may be send without the receiver acknowledging them (See Figure 1). The size of this “window” is governed by the receiver and is referred to as the TCP Window Size. This way the receiver ensures that it can actually process the incoming data without “choking” on it.                              Figure 2: Sliding Window Protocol with increased tprop     Looking at Figure 1 it should become clear that while increasing the value of the signal propagation time tprop, the amount of data that can be transferred in the same time period is reduced. This is caused by the sender spending more time waiting for acknowledgements, before it will send further packages. (See Figure 2)   The propagation time tprop for a TCP packet can be determined by measuring the round-trip-time (RTT) of a packet. Here the round-trip-time is twice the propagation time for synchronous links. This can e.g. be done via the well known tool ping. The TCP window size is determined by the operating system. During a connection the receiver can also adapt the TCP Window Size - in both directions - if the situation changes due to packet loss or buffer fill levels.   Bandwidth-delay Product and buffer size   Now that we have identified the two most important variables for the performance of TCP based data transfers, let’s look at the math behind the sliding window concept:   An important formula is the one for the Bandwidth-delay product (BDP), which is the product of a data link’s capacity (in bits per second) and its end-to-end delay (in seconds). The result, an amount of data measured in bits (or bytes), is equivalent to the maximum amount of data on the network circuit at any given time, e.g. data that has been transmitted but not yet acknowledged.     The result of the BDP can also be interpreted as the required receiver TCP window size to maximize the performance on the data link.   Let’s use an example:   Round-Trip-Time between the US west coast (Las Vegas) and Europe (Germany): 173 ms Available bandwidth between the two sites: 100 Mbit/s     This means that we would need a TCP Window Size of at least 2.1625 MByte to fully utilize the 100 Mbit/s link.   We have seen, that in reality both the delay between sender and receiver as well as the TCP window size within the receiver are given. As we cannot change the laws of physics, the only value we can change is the TCP window size. Let’s shuffle the formula, to calculate the maximum bandwidth that can be achieved with a given RTT and TCP window size instead:     Let’s use another example:   Round-Trip-Time between the US west coast (Las Vegas) and Europe (Germany): 173 ms Standard TCP windows size on a Linux (Ubuntu) host: 64 KByte     Irrelevant of the actual link speed between the two sites above we will not be able to transfer more than 2.89 Mbit/s with a single TCP stream. Keep in mind that this is the theoretical maximum. In reality the value will be even lower due to packet loss and packet header overhead.   If you get tired of performing the math manually, have a look at the TCP throughput calculator from switch.ch.   Limit of TCP Windows field in the protocol header   The TCP window size field within the TCP header is 16 bit and therefore cannot be expanded beyond 64K. How is it then possible to specify a TCP window size higher than 64K? That’s where RFC 1323 defines a scaling factor, which allows scaling up to larger window sizes and thereby enables TCP tuning. This method increases the maximum window size from 65,535 bytes to 1 gigabyte.   The window scale option is used only during the TCP 3-way handshake at the beginning of the connection. The window scale value represents the number of bits to left-shift the 16-bit window size field. The window scale value can be set from 0 (no shift) to 14 for each direction independently. Both sides must send the option in their SYN segments to enable window scaling in either direction.   Here is a problem: Some routers and packet firewalls rewrite the window scaling factor during a transmission, which will cause sending and receiving sides to assume different TCP window sizes. The result is non-stable traffic that may be very slow. One can use packet sniffers such as Wireshark to ensure that the TCP scaling factor are negotiated correctly on sender and receiver side. Figure 3 shows an example of this in Wireshark.                              Figure 3: TCP Window scale in Wireshark     Hands-On Tests   Now it’s time to verify above’s theory in practice: For this we will use the tool Iperf, which is widely available on Linux. On Ubuntu you can e.g. install Iperf with sudo apt-get install iperf.   In this case the sender host is an Ubuntu machine located in a data center in Frankfurt, Germany and the receiver host is an Ubuntu machine located in a data center in Las Vegas, USA. The latency between the two machines is 173 ms with both machines being connected via an 100 Mbit/s uplink to the internet.   On the receiver host we will start iperf as a server and advice it to use the standard TCP window size of 64K:   user@receiver:~$ iperf -s -w 65536   From the sender side we start the test:   user@sender:~$ iperf -c receiver.edge-cloud.net ------------------------------------------------------------ Client connecting to receiver.edge-cloud.net, TCP port 5001 TCP window size: 64 KByte (default) ------------------------------------------------------------ [  3] local 1.2.3.4 port 48448 connected with 5.6.7.8 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0-10.6 sec  3.50 MBytes  2.77 Mbits/sec   The result is what we would expect from the theory and math in the previous section. It also shows nicely that the result is well below the 100 Mbit/s of the Internet links.   Let’s try to increase the TCP Window size on the receiver and run the test again. As Iperf is a user process it cannot actually increase the TCP Window size beyond what’s set in the Kernel. We therefore have to make these changes directly in the Kernel. As a first step it’s advisable to have a look at the current TCP Window settings on the receiver and make note of them, so that they can be restored.   The way the TCP Window works is that sender and receiver negotiate an optimal window size based on various factors. Therefore Linux has two values for the TCP Window. The default value, which is the starting window size and the max value, which is the upper bound of it:   The maximum TCP windows size (receiving) in bit from the TCP autotuning settings:   user@receiver:~$ cat /proc/sys/net/ipv4/tcp_rmem 4096    65536   65536   The first value tells the kernel the minimum receive buffer for each TCP connection, and this buffer is always allocated to a TCP socket, even under high pressure on the system.   The second value specified tells the kernel the default receive buffer allocated for each TCP socket. This value overrides the /proc/sys/net/core/rmem_default value used by other protocols.   The third and last value specified in this variable specifies the maximum receive buffer that can be allocated for a TCP socket. We want to manipulate this third value on the receiver side.   The maximum TCP windows size (sending) in bit from the TCP autotuning settings:   user@sender:~$ cat /proc/sys/net/ipv4/tcp_wmem 4096    65536   65536   This variable takes 3 different values which holds information on how much TCP sendbuffer memory space each TCP socket has to use. Every TCP socket has this much buffer space to use before the buffer is filled up. Each of the three values are used under different conditions. The first value in this variable tells the minimum TCP send buffer space available for a single TCP socket. The second value in the variable tells us the default buffer space allowed for a single TCP socket to use. The third value tells the kernel the maximum TCP send buffer space. Again we want to manipulate the third value. This time on the sender side   Let’s double the TCP Window size, thus reaching 128K. Using the TCP throughput calculator from switch.ch, we should expect a maximum TCP throughput of 5.92 Mbit/sec with these settings:   First, change the maximum TCP windows size (receiving) on the receiver:   user@receiver:~$ sysctl -w net.ipv4.tcp_rmem=\"4096 65536 131072\" net.ipv4.tcp_wmem = 4096 65536 131072 user@receiver:~$ sysctl -w net.core.rmem_max=131072 net.core.rmem_max = 131072   Next, change the default TCP windows size (sending) on the sender:   user@sender:~$ sysctl -w net.ipv4.tcp_wmem=\"4096 65536 131072\" net.ipv4.tcp_wmem = 4096 65536 131072 user@sender:~$ sysctl -w net.core.wmem_max=131072 net.core.rmem_max = 131072   On the receiver host we will start iperf as a server and advice it to use the TCP window size of 128K:   user@receiver:~$ iperf -s -w 131072   Now we run the test again:   user@sender:~$ iperf -c receiver.edge-cloud.net -w 131072 ------------------------------------------------------------ Client connecting to receiver.edge-cloud.net, TCP port 5001 TCP window size:  256 KByte (WARNING: requested  128 KByte) ------------------------------------------------------------ [  3] local 1.2.3.4 port 48448 connected with 5.6.7.8 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0-10.2 sec  6.50 MBytes  5.35 Mbits/sec   Again, the result is what we would expect from the theory and math in the previous section.   Be advised that there is a limit to this approach: If you keep doubling the TCP window size, you will at one point reach the buffer limits of your OS and therefore not experience any additional performance gains anymore.   Real “bandwidth” tests   Warning! You should perform “bandwidth” tests of your links only when they are not in use. Otherwise your results will not be meaningful. Also, while I show you how to use UDP to determine the bandwidth of a link, this protocol does not bring any form of congestion control. While this is a good thing for measuring the bandwidth on an un-utilized link, you will starve out other traffic on a utilized link. You will basically cause a denial of service attack on the link. Therefore proceed with uttermost care!   So far we have learned that the throughput of a single TCP is limited by the TCP window size and the RTT. But what happens if I use multiple TCP streams in parallel? Looking at how TCP works, each of these TCP streams should be able to create an individual maximum throughput as determined in the previous section. Furthermore they should share the available bandwidth fairly with each other until nothing is left.   Let’s see if this is really the case:   On the receiver host we again start Iperf as a server and advice it to use the TCP window size of 128K:   user@receiver:~$ iperf -s -w 131072   But on the server side we will advise Iperf to start 10 parallel test:   user@sender:~$ iperf -c receiver.edge-cloud.net -P 10 -w 131072 ------------------------------------------------------------ Client connecting to receiver.edge-cloud.net, TCP port 5001 TCP window size:  256 KByte (WARNING: requested  128 KByte) ------------------------------------------------------------ [  4] local 1.2.3.4 port 48524 connected with 4.3.2.1 port 5001 [  6] local 1.2.3.4 port 48526 connected with 4.3.2.1 port 5001 [  5] local 1.2.3.4 port 48525 connected with 4.3.2.1 port 5001 [  8] local 1.2.3.4 port 48527 connected with 4.3.2.1 port 5001 [  7] local 1.2.3.4 port 48529 connected with 4.3.2.1 port 5001 [  9] local 1.2.3.4 port 48528 connected with 4.3.2.1 port 5001 [ 11] local 1.2.3.4 port 48531 connected with 4.3.2.1 port 5001 [ 10] local 1.2.3.4 port 48530 connected with 4.3.2.1 port 5001 [  3] local 1.2.3.4 port 48523 connected with 4.3.2.1 port 5001 [ 12] local 1.2.3.4 port 48532 connected with 4.3.2.1 port 5001 [ ID] Interval       Transfer     Bandwidth [  5]  0.0-10.0 sec  5.62 MBytes  4.70 Mbits/sec [  8]  0.0-10.2 sec  5.75 MBytes  4.73 Mbits/sec [  7]  0.0-10.2 sec  5.75 MBytes  4.73 Mbits/sec [  6]  0.0-10.2 sec  6.00 MBytes  4.93 Mbits/sec [ 12]  0.0-10.2 sec  5.75 MBytes  4.72 Mbits/sec [ 11]  0.0-10.2 sec  5.88 MBytes  4.81 Mbits/sec [  4]  0.0-10.3 sec  6.12 MBytes  4.99 Mbits/sec [  3]  0.0-10.3 sec  6.12 MBytes  4.98 Mbits/sec [  9]  0.0-10.3 sec  5.88 MBytes  4.76 Mbits/sec [ 10]  0.0-10.3 sec  6.00 MBytes  4.87 Mbits/sec [SUM]  0.0-10.3 sec  58.9 MBytes  47.8 Mbits/sec   We now see that the throughput result of 47.8 Mbit/s is almost 10x of the throughput of an individual test. Keep in mind additional overhead that this test has to deal with.   Again, be advised that there is a limit to this approach: While you can keep increasing the number of parallel tests, there is a limit on how many parallel TCP streams your host can handle. At one point you will therefore not see a performance improvement anymore.   Time to bring out the big guns: UDP   So far we have only been able to verify that we can transfer with rates of about 47.8 Mbit/s between sender and receiver in our example. While this is already a huge increase to the 2.77 Mbit/s that we measured originally, it still falls short of the 100 Mbit/s that we should be getting.   Let’s change our so far strategy and switch over to a protocol that does not suffer from the limitations of a sliding window protocol: UDP to the rescue. UDP does not utilize a feedback channel to notice network congestion or receiver buffer exhaustion. It is a “fire and forget” protocol. That makes it very dangerous for sending data at a sustained high data rate, which data transfer would bring to the table. It will basically overrun any other TCP traffic and completely utilize any available bandwidth. Bottom line: Used on a shared link the following approach will equal to a denial of service attack. Therefore do not use it on a shared link, especially if it is carrying production traffic.   On the receiver host we again start Iperf as a server. But this time we start it in UDP mode:   user@receiver:~$ iperf -s -u   On the sender side we also start Iperf in UDP mode and ask it to attempt to sent 110 Mbit/s of traffic. We will attempt to send a bit more than the expected maximum bandwidth, to understand when the links max out:   user@sender:~$ iperf -c receiver.edge-cloud.net -u -b 110m ------------------------------------------------------------ Client connecting to receiver.edge-cloud.net, TCP port 5001 Sending 1470 byte datagrams UDP buffer size:  224 KByte (default) ------------------------------------------------------------ [  3] local 1.2.3.4 port 43816 connected with 5.6.7.8 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0-10.0 sec   132 MBytes   110 Mbits/sec [  3] Sent 93963 datagrams [  3] Server Report: [  3]  0.0-10.0 sec   117 MBytes  98.0 Mbits/sec   0.124 ms 10515/93962 (11%) [  3]  0.0-10.0 sec  114 datagrams received out-of-order   The results show that we can transfer 98.0 Mbit/s with UDP between sender and receiver host, which is close to the expected maximum of 100 Mbit/s. Again, you have to factor in protocol overhead why you will not achieve the full 100 Mbit/s throughput.   Now let’s see what would happen to a TCP transfer between the same hosts running at the same time.   For this we need to open two Terminal or SSH connections to the sender at the same time.   On the receiver host we start one instance of Iperf as a UDP server and one instance as a TCP server with TCP window size of 128K. They will both spawn to the background.   user@receiver:~$ iperf -s -u &amp; user@receiver:~$ iperf -s -w 131072 &amp;   Don’t forget to kill these processes once you are done!   On the sender side we will start two tests at exactly the same time. One test with Iperf in TCP mode. And another test with Iperf in UDP mode, again asking it to attempt to sent 110 Mbit/s of traffic. To showcase better the effect of UDP traffic flooding a link we will ask Iperf to run 10 UDP test in parallel. Make sure to start both tests at the same time:   UDP Test   user@sender:~$ iperf -c receiver.edge-cloud.net -u -b 110m -P 10 ------------------------------------------------------------ Client connecting to las-mgmt-ubu01.vmwcs.com, UDP port 5001 Sending 1470 byte datagrams UDP buffer size:  224 KByte (default) ------------------------------------------------------------ [  4] local 1.2.3.4 port 48228 connected with 5.6.7.8 port 5001 [  5] local 1.2.3.4 port 58444 connected with 5.6.7.8 port 5001 [  7] local 1.2.3.4 port 56308 connected with 5.6.7.8 port 5001 [  6] local 1.2.3.4 port 34767 connected with 5.6.7.8 port 5001 [  8] local 1.2.3.4 port 32790 connected with 5.6.7.8 port 5001 [  9] local 1.2.3.4 port 47212 connected with 5.6.7.8 port 5001 [ 10] local 1.2.3.4 port 46375 connected with 5.6.7.8 port 5001 [  3] local 1.2.3.4 port 51226 connected with 5.6.7.8 port 5001 [ 11] local 1.2.3.4 port 40858 connected with 5.6.7.8 port 5001 [ 12] local 1.2.3.4 port 51633 connected with 5.6.7.8 port 5001 [ ID] Interval       Transfer     Bandwidth [  4]  0.0-10.0 sec  34.9 MBytes  29.3 Mbits/sec [  4] Sent 24875 datagrams [  5]  0.0-10.0 sec  34.8 MBytes  29.2 Mbits/sec [  5] Sent 24814 datagrams [  7]  0.0-10.0 sec  34.5 MBytes  28.9 Mbits/sec [  7] Sent 24601 datagrams [  6]  0.0-10.0 sec  34.9 MBytes  29.3 Mbits/sec [  6] Sent 24878 datagrams [  8]  0.0-10.0 sec  34.6 MBytes  29.1 Mbits/sec [  8] Sent 24704 datagrams [  9]  0.0-10.0 sec  34.9 MBytes  29.3 Mbits/sec [  9] Sent 24882 datagrams [ 10]  0.0-10.0 sec  34.5 MBytes  29.0 Mbits/sec [ 10] Sent 24634 datagrams [  3]  0.0-10.0 sec  35.1 MBytes  29.4 Mbits/sec [  3] Sent 25015 datagrams [ 11]  0.0-10.0 sec  34.8 MBytes  29.2 Mbits/sec [ 11] Sent 24805 datagrams [ 12]  0.0-10.0 sec  34.5 MBytes  28.9 Mbits/sec [ 12] Sent 24581 datagrams [SUM]  0.0-10.0 sec   347 MBytes   291 Mbits/sec [  3] Server Report: [  3]  0.0-10.0 sec  12.1 MBytes  10.1 Mbits/sec   2.309 ms 16396/25014 (66%) [  3]  0.0-10.0 sec  14 datagrams received out-of-order [  8] Server Report: [  8]  0.0-10.0 sec  12.1 MBytes  10.1 Mbits/sec   2.273 ms 16104/24703 (65%) [  8]  0.0-10.0 sec  1 datagrams received out-of-order [  5] Server Report: [  5]  0.0-10.0 sec  12.2 MBytes  10.2 Mbits/sec   1.844 ms 16117/24813 (65%) [  5]  0.0-10.0 sec  1 datagrams received out-of-order [  6] Server Report: [  6]  0.0-10.0 sec  12.1 MBytes  10.2 Mbits/sec   1.737 ms 16217/24877 (65%) [  6]  0.0-10.0 sec  1 datagrams received out-of-order [  7] Server Report: [  7]  0.0-10.0 sec  12.1 MBytes  10.1 Mbits/sec   1.883 ms 16003/24600 (65%) [  7]  0.0-10.0 sec  1 datagrams received out-of-order [ 11] Server Report: [ 11]  0.0-10.0 sec  9.14 MBytes  7.66 Mbits/sec   1.618 ms 18283/24804 (74%) [  4] Server Report: [  4]  0.0-10.2 sec  12.1 MBytes  9.88 Mbits/sec  15.864 ms 16255/24863 (65%) [  4]  0.0-10.2 sec  1 datagrams received out-of-order [  9] Server Report: [  9]  0.0-10.3 sec  9.17 MBytes  7.50 Mbits/sec  16.409 ms 18339/24881 (74%) [  9]  0.0-10.3 sec  1 datagrams received out-of-order [ 12] Server Report: [ 12]  0.0-10.3 sec  12.0 MBytes  9.83 Mbits/sec  15.975 ms 16005/24575 (65%) [ 12]  0.0-10.3 sec  1 datagrams received out-of-order [ 10] Server Report: [ 10]  0.0-10.3 sec  12.1 MBytes  9.87 Mbits/sec  16.419 ms 16022/24632 (65%) [ 10]  0.0-10.3 sec  1 datagrams received out-of-order   Adding together the throughput of the 10 UDP connections we get a total throughput of 76.331 Mbit/s.   TCP Test   user@sender:~$ iperf -c las-mgmt-ubu01.vmwcs.com ------------------------------------------------------------ Client connecting to las-mgmt-ubu01.vmwcs.com, TCP port 5001 TCP window size: 23.5 KByte (default) ------------------------------------------------------------ [  3] local 78.47.152.89 port 48587 connected with 64.79.130.189 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0-10.6 sec  2.12 MBytes  1.68 Mbits/sec   On the other hand we see that these UDP connections starved our TCP connection from previously 5.35 Mbits/sec down to 1.68 Mbits/sec.   Finally we have been able to confirm that our available bandwidth between sender and receiver is indeed 100 Mbit/s. Yet at the same time we have seen how dangerous UDP can be on a link. Many enterprises therefore apply rate limiting to UDP on their WAN links or block it altogether.   It should also become clear that using a pure UDP based data transfer protocol on a shared link is a really bad idea.   Solutions to improve throughput   We have seen that increasing the TCP Window Size on the receiver side helps increasing the throughput that is possible with a single TCP stream. Unfortunately it is not possible to tweak this TCP Window Size on all Receivers for the encountered latency for all downloads. That is especially the case as increasing the TCP Window Size also brings drawbacks.   WAN Optimization Controller   Instead network architects usually deploy a pair of specialized devices - called WAN Optimization Controller (WOC) within the network stream. Placed as close as possible to server and client of the stream they act like a proxy in front of the actual server. While these WOC devices also utilize other improvement capabilities, one of their main capabilities is using an optimized transport mechanism with an increased TCP Window size between the.                              Figure 4: WAN Optimization Controllers     One vendor offering such devices is Silver Peak, which offers An interesting tool with its Throughput Calculator from Silver Peak. Similar to the tool from Switch.ch, it will show you the maximum transfer speed that is possible with a given RTT and packet loss rate, while assuming a default TCP window size of 64K. in addition it will also show you the throughput that would be possible over the same link using a Silver Peak WOC pair.   Content Distribution Networks (CDN)   While we can indeed not change the laws of physics to decrease the RTT in our equation, one could use other methods to decrease the RTT. One such method would be the usage of Content Distribution Network (CDN) to place the server from which a user wants to download a file closer to this user, thus reducing the RTT.                              Figure 5: Content Distribution Networks     Instead of requesting a file from e.g Los Angeles while being in Munich, Germany, the file could be requested from a CDN node in Frankfurt, Germany. This would reduce the RTT from e.g. ~170 ms to ~4 ms. This is often used by companies and organizations to deliver large software downloads. One such example is VMware’s software download site using Akamai’s CDN.   UDP-based file transfer   Last but not least I would like to point out that there are in fact UDP based file transfer solutions out there, such as the one from Asperasoft. They overcome the “dangers” of UDP with smartly using a TCP channel for congestion control. Yet at the same time the transfer limit of this pure protocol is bound by the actual link bandwidth. WOC on the other hand usually utilize additional optimization techniques beside TCP window adjustment, giving you effective throughput higher than the maximum link bandwidth. See the previously mentioned Silver Peak calculator for examples.  ","categories": ["EdgeCloud"],
        "tags": ["Network","Performance"],
        "url": "https://www.edge-cloud.net/2013/06/07/measuring-network-throughput/",
        "teaser":null},{
        "title": "Arista vEOS on VMware ESX",
        "excerpt":"Arista EOS is released as a single image that supports all of their hardware platforms. But that same single image can also be run in a virtual machine! While a great article on Building a Virtual Lab with Arista vEOS and VirtualBox already exists, I wanted to accomplish the same with vSphere 5.x.   Here’s how I did it.   Pre-Requisites   Besides at least one ESXi 5.x host you will need the following files from Arista Networks:      The bootloader: Aboot-veos-2.0.8.iso   The actual vEOS image as a VMDK: EOS-4.12.0-veos.vmdk   Note: This guide is current as of June 13, 2013. I have used the latest available vEOS VMDK file. You might want to check if a newer vEOS files was published in the meantime.   Installation   Build a base VM image in vSphere   Within your vSphere Client create a new Custom Configuration VM                              Figure 1: Create a new custom VM     Select a name - e.g. Arista vEOS 4.12.0 - for your VM and select the Host / Cluster, Resource Pool and Storage applicable to your specific setup.   As the Virtual Machine Version select Virtual Machine Version: 8.                              Figure 2: Virtual Machine Version     As the Guest Operating System choose Linux -&gt; Other 2.6.x Linux (32-bit).                              Figure 3: Guest Operating System     For the CPU settings choose 1 as the Number of Virtual Sockets and also 1 as the Number of cores per virtual socket.                              Figure 4: CPU settings     Increase the Memory Size to 2 GB.                              Figure 5: Memory Size     Increase the number of NICs to 4 and choose E1000 as the Adapter for all of them. Connect each NIC to a port-group applicable to your specific setup.   The NIC 1 will appear as the Mgmt interface within vEOS. If you didn’t create any port-groups specific to your vEOS setup so far, don’t worry: Just pick any available port-group. We will get a chance again to change this later.                              Figure 6: Network Interfaces     Leave the SCSI Controller at the default value of LSI Logic Parallel. We will not actually use a SCSI controller.                              Figure 7: SCSI Controller     At this point we will not create a disk. Therefore select Do not create disk under Select disk.                              Figure 8: Select a disk     Complete the “Create New Virtual Machine” wizard. Do bot power up the VM yet!!                              Figure 9: Complete Create New Virtual Machine wizard     Upload Aboot and vEOS files   Next upload the Aboot-veos-2.0.8.iso bootloader and EOS-4.12.0-veos.vmdk disk image file into the folder of your previously created VM within the vSphere data store.                              Figure 10: Upload Boot loader and disk image files     Change the Virtual Machine settings   Although we just created the VM for vEOS, we already need to change its settings again. Open the VM’s properties via Edit Settings.   Remove the Floppy drive 1 hardware item.                              Figure 11: Remove Floppy Drive     Map the CD/DVD drive 1 to the Aboot-veos-2.0.8.iso bootloader image residing on your datastore. This file needs to remain mounted. It is not only used for installation, but for all future boot cycles.                              Figure 12: Mount the Aboot-veos-2.0.8.iso bootloader file     Add a new device of the type Hard Disk.                              Figure 13: Add Hard Disk     Choose Use an existing virtual disk as the disk type.                              Figure 14: Use an existing virtual disk     Select the EOS-4.12.0-veos.vmdk disk image file residing on your datastore.                              Figure 15: Select vEOS disk image file     Accept the proposed Virtual Device Node with IDE (0:0).   Re-connect any NIC devices that you want to change and didn’t get a chance so far.   Close the “Add Hardware” wizard and the “Virtual Machine Properties” dialog.   Boot your vEOS VM   Now it’s time to test if things are working. Power up the vEOS VM and open the Console.                              Figure 16: vEOS booting on ESX     It will take a few moments for vEOS to complete the first boot during which the virtual Flash is being initialized.   Once the boot process is complete you can login with the username admin and e.g. list the physical interfaces.                              Figure 17: Working vEOS install on ESX     Enjoy your newly installed vEOS on ESX!   What’s next?   As a next step you could import the above vEOS instance into VMware vCloud Director and create simple or complex vApps of vEOS instances depicting various network architectures.                              Figure 18: Sample vEOS lab layout     Figure 18 shows a sample vEOS lab layout with 3 vEOS VMs and a Windows or Linux Jump Box VM. The VMs are interconnected via various vApp networks.  ","categories": ["EdgeCloud"],
        "tags": ["Arista"],
        "url": "https://www.edge-cloud.net/2013/06/13/arista-veos-on-vmware-esx/",
        "teaser":null},{
        "title": "\"Breaking\" Amazon AWS S3",
        "excerpt":"I have used various services from Amazon AWS for quite a while now and have always been amazed by what interesting things one can do with this services. Recently I took the AWS training course Architecting on AWS, which gives an awesome overview on what’s possible with the AWS service.   During the training some of the participants had quite some trouble with one of the hands-on exercises. As my exercise worked without a flaw, I used the time and started digging deeper what could be wrong. Turns out that Amazon AWS S3 allows users to specify S3 bucket names in the “US Standard” regions that are not allowed like this in any other zone. As most libraries building on top of S3 assume the naming restrictions for all non-“US Standard” regions are also enforced in “US Standard”, it breaks functionality of some of these libraries.   Let’s have a closer look:   Creating an S3 bucket outside “US Standard”   When creating an S3 bucket outside “US Standard” neither the S3 Management Console nor the underlying REST API accepts the name to contain upper case letters. Figure 1 shows an example of the error message in the Console.                              Figure 1: Upper case letters are not allowed in S3 bucket names in the Oregon region     This behavior makes sense as the above bucket name would become reachable under the URL http://ThisBucketWillLiveInOregon.s3.amazonaws.com/ which is equivalent to e.g. http://thisbucketwillliveinoregon.s3.amazonaws.com/. On this topic RFC1035 notes: Note that while upper and lower case letters are allowed in domain names, no significance is attached to the case. That is, two names with the same spelling but different case are to be treated as if identical.   Creating an S3 bucket in “US Standard”   For S3 buckets created in the AWS region “US Standard”, things look a bit different. As Figure 2 shows, it is very well possible to create two buckets with the same name, that only differentiate in case.                              Figure 2: The US Standard region differentiates case in S3 bucket names     On a first glimpse that isn’t that bad, as in contrary to other regions, buckets in the “US Standard” region are mapped to the URL http://s3.amazonaws.com//. And here there is indeed a difference between the URL _\"http://s3.amazonaws.com/DifferentiateBetweenUPPERcaseANDlowerCASE\"_ and the folder _\"http://s3.amazonaws.com/DifferentiateBetweenupperCASEANDLOWERCASE\"_.   Where the problems start   One problem starts when you want to use that bucket in the “US Standard” region for static website hosting. As Figure 3 shows, attempting to enable website hosting for an S3 bucket in the “US Standard” region that uses upper case letters will fail. Unfortunately the error message is quite useless. Here Amazon AWS should provide better feedback through a useful error message.                              Figure 3: Website hosting for an S3 bucket in the region US Standard fails     Yet again, this also makes sense: The above S3 bucket would be hosted under the URL http://DifferentiateBetweenUPPERcaseANDlowerCASE.s3-website-us-east-1.amazonaws.com which according to RFC1035 doesn’t differ from http://DifferentiateBetweenupperCASEANDLOWERCASE.s3-website-us-east-1.amazonaws.com which the other bucket - that we previously created - would receive. Thus two buckets would receive the same URL. That clearly shouldn’t happen.   Why does it matter?   Turns out that some libraries for making the S3 REST API available in various programming languages do not take into consideration that “US Standard” allows mixed case S3 buckets. The Python interface to Amazon Web Services “boto” for example assumes that bucket names are always in lower case. Using a bucket with a mixed case name will break uploads and thereby the functionality of the library.   That’s exactly what happened with other members of the training course: Their bucket name used non-lower case letters, breaking the provided Python script.   It’s all in the documentation   At the same time, the Amazon AWS documentation is pretty extensive and does mention the above special cases. Although one has to say that they are “well hidden”.   In the Amazon S3 Developer Guide, the section “Bucket Restrictions and Limitations” clearly states that in all regions except for the US Standard region a bucket name must comply with certain rules, that will result in a DNS compliant bucket name. While the rules for bucket names in the US Standard region are similar but less restrictive and can result in a bucket name that is not DNS-compliant.   The section “Virtual Hosting of Buckets” in the same guide then also states that only lower-case buckets are addressable using the virtual hosting method.   While it’s great that the documentation clearly mentions the restrictions, as an end-user I would prefer consistency between the regions and better error messages.  ","categories": ["EdgeCloud"],
        "tags": ["AWS"],
        "url": "https://www.edge-cloud.net/2013/06/21/breaking-amazon-aws-s3/",
        "teaser":null},{
        "title": "Network troubleshooting via Arista EOS shell",
        "excerpt":"A few days ago the startup Cumulus Networks emerged from the clouds with their Cumulus Linux OS for original design manufacturer (ODM) switches based on Broadcom silicon. While the average customer will have to wait a while to get their hands on a Cumulus Networks based device, users of Arista switches can already today use the benefits of a full Linux distribution running on a data center switch.   Arista EOS is based on a Linux kernel and provides full and open access to a Linux shell, allowing installation and use of Linux based management and troubleshooting tools.   In this short post I want to show you two use cases where this capability comes in extremely handy in the daily network management work: Network Troubleshooting.   TCPDump to PCAP   Quite frequently it happens that network devices aren’t behaving the way they should be. Let’s take the example of a virtual router that doesn’t want to form OSPFv3 adjacency with the core switch. What usually helps quite a bit are packet captures of the traffic between the involved network devices. In the past it could be quite challenging acquiring these packet captures, requiring the setup of a Switched Port Analyzer (SPAN) Remote Switched Port Analyzer (RSPAN) or even Encapsulated Remote Switched Port Analyzer (ERSPAN).   With Arista EOS it becomes much easier, as you can run TCPDump directly on the switch to capture a PCAP file for Wireshark:   First, change into the EOS shell from the priviliged CLI mode:   ams-core01a#bash  Arista Networks EOS shell  [user@ams-core01a ~]$   Next, start the pre-installed tcpdump in PCAP capturing mode on the desired interface. Here I run it on the VLAN interface vlan51 and capture the file into the flash. Once you’re done with the packet capture, press Ctrl + C to abort tcpdump:   [user@ams-core01a ~]$ tcpdump -i vlan51 -s 65535 -w /mnt/flash/int-vlan51.pcap tcpdump: listening on vlan51, link-type EN10MB (Ethernet), capture size 65535 bytes ^C10 packets captured 10 packets received by filter 0 packets dropped by kernel [user@ams-core01a ~]$   Next we copy the files to another host - here a NOC jumpbox - to open it in Wireshark. That can easily be done via the installed SSH SCP client:   [user@ams-core01a ~]$ scp /mnt/flash/int-vlan51.pcap root@noc01.edge-cloud.net:/tmp The authenticity of host 'noc01.edge-cloud.net (2a01:4f8:d12:11c4::2)' can't be established. RSA key fingerprint is df:6c:9d:dd:8f:45:f8:61:96:0f:e4:54:c9:2d:d3:94. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'noc01.edge-cloud.net,2a01:4f8:d12:11c4::2' (RSA) to the list of known hosts. Yubikey for 'root': int-vlan51.pcap                               100%  910     0.9KB/s   00:00 [user@ams-core01a ~]$   The capture PCAP file can be opened directly in Wireshark as shown in Figure 1:                              Figure 1: PCAP file captured via Arista EOS shell in Wireshark     Throughput testing with iperf   In the previous blog post Measuring Network Throughput, I already showcased how to use iperf to measure the TCP throughput between two hosts. The good news: Arista EOS has iperf pre-installed. You can therefore use an Arista device to perform network throughput tests for TCP and UDP.   Let’s have a look: If you are not yet in the EOS shell mode, change into it from the priviliged CLI mode:   ams-core01a#bash  Arista Networks EOS shell  [user@ams-core01a ~]$   By default iperf uses the port TCP/5001 in server mode for inbound connections. But Arista blocks this port by default. Therefore you have to temporarily add an iptables rule in the EOS shell to allow access to port TCP/5001. This is done with the command:   [user@ams-core01a ~]$ sudo iptables -I INPUT -p tcp -m tcp --dport 5001 -j ACCEPT   It can later be undone via:   [user@ams-core01a ~]$ sudo iptables -D INPUT -p tcp -m tcp --dport 5001 -j ACCEPT   Also keep in mind, that this command will not survive a reboot of the switch. Next you have the option to run iperf either in server or client mode.   iperf Server mode   Press Ctrl + C to exit the server mode.   [user@las-core01a ~]$ iperf -s ------------------------------------------------------------ Server listening on TCP port 5001 TCP window size: 85.3 KByte (default) ------------------------------------------------------------ [  4] local 172.31.3.4 port 5001 connected with 172.31.1.2 port 44589 [ ID] Interval       Transfer     Bandwidth [  4]  0.0-10.3 sec  89.6 MBytes  72.9 Mbits/sec ^C[user@las-core01a ~]$   iperf Client Mode   [user@ams-core01a ~]$ iperf -c las-core01a.edge-cloud.net ------------------------------------------------------------ Client connecting to las-core01a.edge-cloud.net, TCP port 5001 TCP window size: 16.0 KByte (default) ------------------------------------------------------------ [  3] local 172.31.3.4 port 60088 connected with 172.31.1.2 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0-10.0 sec  63.0 MBytes  52.7 Mbits/sec [user@ams-core01a ~]$   If you read the blog post Measuring Network Throughput, you will remember that TCP throughput depends on the link latency and the TCP window size.   In the above example we didn’t specify the TCP window size, but used the standard Linux auto-tuning TCP buffer limit. Here Arista has already done some tuning for us and set this auto-tuning TCP buffer limit to 4096 KByte.   [user@ams-core01a ~]$ cat /proc/sys/net/ipv4/tcp_rmem 4096    87380   4194304 [user@ams-core01a ~]$   That should be more than sufficient for most TCP performance test, even across WANs.   Outlook   This was just a simple example on how to use the Arista EOS shell in daily network operations. In the end the Linux powered EOS shell gives almost endless opportunities for usage. What would you use it for?  ","categories": ["EdgeCloud"],
        "tags": ["Arista","Network","Performance"],
        "url": "https://www.edge-cloud.net/2013/06/24/network-troubleshooting-via-arista-eos-shell/",
        "teaser":null},{
        "title": "TCP Throughput tests between Amazon AWS regions",
        "excerpt":"In a previous post I wrote about “Measuring Network Throughput”. Today I want to share a few quick performance test results that I assembled for the single TCP session throughput between the Amazon AWS Oregon and N. Virginia regions.   Amazon AWS   The test series uses one m1.medium instance in each region. The latency between the two instances gives us an RTT of 88 ms and therefore allows us to calculate the theoretical maximum throughput based on the bandwidth-delay product.   I’m again using iperf with varying TCP window sizes for this test.                              Figure 1: Single TCP stream throughput between AWS regions     Figure 1 shows the single TCP stream throughput between the two AWS regions. One can see that the throughput in both directions nicely ramps up with the increase of the TCP window size. Yet it stays behind the theoretical maximum, which is expected due to limitations of the OS (e.g. buffer sizes), the physical hardware running the OS and of course protocol overhead.   Looking at the traceroute between the two instances one can clearly see that Amazon uses its own links to connect the regions.   Side Note: The network team at Amazon AWS should brush up their skills on Reverse DNS lookups as almost none of the routing hops’ IP addresses resolves to DNS names. But the Autonomous System (AS) number of hops clearly shows that the IP addresses belong to Amazon.   “Broken” example, not Amazon AWS   Let’s have a look at another example, not Amazon AWS. This time the two workloads reside in data centers in Miami (Florida) and Las Vegas (Nevada). The RTT between the two workloads is 75 ms. In this case the sites are not connected via dedicated links, but instead both sites are connected to the internet via 100 MBits/sec uplinks. Thus the traffic between the sites traverses the Internet.                              Figure 2: Broken single TCP stream throughput (Not Amazon AWS)     Here Figure 2 shows that in this example there is certainly something wrong with the connection. Throughput from Las Vegas to Miami remains extremely low at around 4-5 MBits/sec with any TCP window size. Yet, throughput from Miami to Las Vegas scales up with increasing TCP window size to acceptable values.   This shows the clear benefit of Amazons own dedicated links between its AWS regions.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Performance"],
        "url": "https://www.edge-cloud.net/2013/06/28/tcp-throughput-amazon-aws-regions/",
        "teaser":null},{
        "title": "Terminating a 6in4 tunnel on an Arista switch",
        "excerpt":"In a previous post I have shown that Arista switches feature a full fledged Linux system underneath the CLI that is accessible to network administrators via the EOS shell.   Let’s use this Linux capability to do something out of the box: Terminate a 6in4 tunnel on an Arista switch to provide IPv6 access.   Via the CLI this is not possible as Arista does not support 6in4 tunnel in EOS. Therefore let’s use the Linux-based EOS shell.   Please keep in mind that the tunnel termination will be handled by the control plane in software and not in hardware. Thus don’t expect any miracles with regards to throughput.   Getting the 6in4 tunnel   Next we need a service provider delivering us the termination or such a tunnel. The easiest way to achieve this is via Hurricane Electric’s Tunnelbroker service. Within a few minutes you get a 6in4 tunnel this way. Figure 1 shows an example of the details for a 6in4 tunnel provided by Hurricane Electric.                              Figure 1: 6in4 tunnel details from tunnelbroker.net     Prerequisites   Before starting with the configuration of the 6in4 tunnel from within the Arista EOS shell, let’s make sure that we can actually contact the service provider’s tunnel endpoint via ping from the Arista CLI.   ams-core01a(config)#ping 216.66.84.46 PING 216.66.84.46 (216.66.84.46) 72(100) bytes of data. 80 bytes from 216.66.84.46: icmp_req=1 ttl=61 time=0.674 ms 80 bytes from 216.66.84.46: icmp_req=2 ttl=61 time=0.642 ms 80 bytes from 216.66.84.46: icmp_req=3 ttl=61 time=0.653 ms 80 bytes from 216.66.84.46: icmp_req=4 ttl=61 time=0.638 ms 80 bytes from 216.66.84.46: icmp_req=5 ttl=61 time=0.656 ms  --- 216.66.84.46 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4ms rtt min/avg/max/mdev = 0.638/0.652/0.674/0.030 ms, ipg/ewma 1.039/0.663 ms   Once we confirm connectivity, we can get started with the actual configuration. For this let’s enter into the EOS shell mode and become super user:   ams-core01a#bash  Arista Networks EOS shell  [user@ams-core01a ~]$ sudo -i  Arista Networks EOS shell  -bash-4.1#   Installing necessary modules   In order to configure a 6in4 tunnel under Linux the Simple Internet Transition (SIT) module needs to be loaded. This module is not loaded by default on Arista switches. Let’s do so:   -bash-4.1# modprobe sit   Now we can configure the 6in4 tunnel to Hurricane Electric and make it available under the interface name “he-ipv6”, enable IPv6 on this interface and assign the correct IPv6 address:   -bash-4.1# ip tunnel add he-ipv6 mode sit remote 216.66.84.46 local 212.123.xxx.xxx ttl 255 -bash-4.1# ip link set he-ipv6 up -bash-4.1# echo 0 &gt; /proc/sys/net/ipv6/conf/he-ipv6/disable_ipv6 -bash-4.1# ip addr add 2001:470:xxxx:xxxx::2/64 dev he-ipv6 -bash-4.1#   The 6in4 tunnel to Hurricane Electric should now be up and running.   Testing   Testing IPv6 connectivity from the EOS shell   Before declaring success, let’s ensure that we can actually carry IPv6 traffic across the tunnel. We will start by attempting to ping the IPv6 address of the tunnel’s remote interface at Hurricane Electric.   -bash-4.1# ping6 2001:470:xxxx:xxxx::1 PING 2001:470:xxxx:xxxx::1(2001:470:xxxx:xxxx::1) 56 data bytes 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=1 ttl=64 time=0.742 ms 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=2 ttl=64 time=0.739 ms 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=3 ttl=64 time=0.715 ms 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=4 ttl=64 time=0.693 ms 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=5 ttl=64 time=0.720 ms 64 bytes from 2001:470:xxxx:xxxx::1: icmp_seq=6 ttl=64 time=0.732 ms ^C --- 2001:470:xxxx:xxxx::1 ping statistics --- 6 packets transmitted, 6 received, 0% packet loss, time 5000ms rtt min/avg/max/mdev = 0.693/0.723/0.742/0.031 ms   Once this works successfully, let’s configure routing beyond this first hop via a default route:   -bash-4.1# ip route add ::/0 dev he-ipv6   Let’s test if this works and ping Google’s public DNS resolver:   -bash-4.1# ping6 2001:4860:4860::8888 PING 2001:4860:4860::8888(2001:4860:4860::8888) 56 data bytes 64 bytes from 2001:4860:4860::8888: icmp_seq=1 ttl=58 time=4.51 ms 64 bytes from 2001:4860:4860::8888: icmp_seq=2 ttl=58 time=4.62 ms 64 bytes from 2001:4860:4860::8888: icmp_seq=3 ttl=58 time=4.57 ms 64 bytes from 2001:4860:4860::8888: icmp_seq=4 ttl=58 time=4.59 ms 64 bytes from 2001:4860:4860::8888: icmp_seq=5 ttl=58 time=4.54 ms ^C --- 2001:4860:4860::8888 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4007ms rtt min/avg/max/mdev = 4.519/4.571/4.626/0.071 ms -bash-4.1#   It does, which means that we have successfully enabled IPv6 connectivity via a 6in4 tunnel.   Testing IPv6 connectivity from the Arista CLI   So far we have configured the 6in4 tunnel and tested its functionality from the Arista EOS shell. But does that mean that the changes we performed will actually be available to traffic that traverses the switch? In other words: Can we actually connect machines to my Arista switch that will gain IPv6 connectivity this way? Let’s try it out by testing network connectivity from the Arista CLI.   First exit the Arista EOS shell into the CLI.   -bash-4.1# exit logout [user@ams-core01a ~]$ exit logout   Let’s ping the Google DNS resolver again and see what happens:   ams-core01a#ping ipv6 2001:4860:4860::8888 PING 2001:4860:4860::8888(2001:4860:4860::8888) 72 data bytes 72 bytes from 2001:4860:4860::8888: icmp_seq=1 ttl=58 (truncated) 72 bytes from 2001:4860:4860::8888: icmp_seq=2 ttl=58 (truncated) 72 bytes from 2001:4860:4860::8888: icmp_seq=3 ttl=58 (truncated) 72 bytes from 2001:4860:4860::8888: icmp_seq=4 ttl=58 (truncated) 72 bytes from 2001:4860:4860::8888: icmp_seq=5 ttl=58 (truncated)  --- 2001:4860:4860::8888 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 20ms rtt min/avg/max/mdev = 4.516/4.554/4.623/0.093 ms, ipg/ewma 5.005/4.535 ms ams-core01a#   That works like a charm.   Of course there must be some drawback to all of this: As the interface “he-ipv6” is not known to the Arista CLI, it will not be displayed from within the CLI. Thus it appears like hidden from the network administrator and is only accessible via the EOS shell. This makes troubleshooting a bit more challenging as e.g. the IPv6 routing table will appear to be empty even though we just verified that we have IPv6 connectivity to the Internet.   ams-core01a#sh ipv6 route ::/0 IPv6 Routing Table - 35 entries Codes: C - connected, S - static, K - kernel, O - OSPF, B - BGP, R - RIP, A - Aggregate   ams-core01a#   Final remarks   Please keep in mind that due to the operational challenges around troubleshooting the above setup as well as the rather mediocre throughput of the 6in4 tunnel, this configuration is not recommended for production usage. Also keep in mind that the above changes would not survive a reboot of the switch.   Instead this article showed you how powerful Arista’s underlying Linux OS really is.  ","categories": ["EdgeCloud"],
        "tags": ["Arista","Network"],
        "url": "https://www.edge-cloud.net/2013/07/18/terminating-a-6in4-tunnel-on-an-arista-switch/",
        "teaser":null},{
        "title": "IPv6 deployment: Using IPv6 link-local addresses as default gateway",
        "excerpt":"One of the big benefits in IPv6 is the automatic configuration capability for hosts via Stateless address autoconfiguration (SLAAC). Yet sometimes even in IPv6 one wants to solely manually configure hosts in a L2 segment. For this use case one needs to provide basic network information to the user configuring the host. This usually includes at least the IP address of the host along with the prefix length, the default gateway and the DNS resolver. Wouldn’t it be great if we could tell users that the default gateway and the DNS resolver are always the same, no matter what network segment the host is in? And wouldn’t it be great if these IPv6 addresses for default gateway and DNS resolver were short and easy to remember?   IPv6 use case   While it’s already possible to achieve something similar for the DNS resolver in IPv4, in IPv6 link-local addresses enable us to use the same default gateway in every network segment. Simplified speaking link-local addresses translate into having the same subnet available on each L2 segment. Thus this allows network engineers to configure the default gateway for client machines to be always the same. Here it doesn’t matter whether this default gateway for static routing is provided via a single interface, via Hot Standby Router Protocol (HSRP) or Virtual Router Redundancy Protocol (VRPP).   As a result a network engineer can give users a very simple to follow statement for manually configuring their hosts with IPv6: “The default gateway is always fe80::1”. Thanks to Anycast we can already make similar statements for e.g. DNS, with “The DNS resolvers are always fd53::11 and fd53::12”.   Network Design   In the IPv4 world it is very common to use the first address within an IPv4 subnet as the default gateway, along with the second and third address potentially being used for individual HSRP/VRPP nodes. Thus on the network 192.168.0.0/24, the default gateway would usually be 192.168.0.1. In case of network architects using /24 networks for end-user facing usage, this often translates into the easy statement of “Always use the .1 as the default gateway.” With IPv6 we can simplify this statement further and use a link-local address as the default gateway.   In IPv6 Link-Local addresses are mandatory addresses according to RFC 4291. This means that all interfaces are required to have at least one Link-Local unicast address from the address block fe80::/10, which has been reserved for link-local unicast addressing. The actual link-local addresses are though assigned with the prefix fe80::/64.   Combining what many of us are used to from the IPv4 world with this new feature particular to IPv6, the link-local address fe80::1 appears to be the perfect candidate for a generic default gateway within a data center or campus network. This address is for sure easier to remember and especially type in as e.g. 20ba:dd0g:f00d:1234::1.   As each IPv6-enabled router interface will already have a link-local address generated based on the modified EUI-64 scheme, the address fe80::1 will either replace or augment this automatically generated address. At the same time an existing global unicast address on the interface will not be affected.   Implementation   After having decided to use the link-local address fe80::1 as the default gateway in all end-user facing subnets, we need to configure our network devices accordingly.   Most network vendors offer a way for manually configuring an additional link-local address on an interface that slightly differs from the way a regular global unicast address is configured.   Both Cisco and Brocade do not use a prefix-length as part of the configuration, but instead use the keyword “link-local”. Juniper on the other side treats configuring a link-local IPv6 address the same way as configuring a global unicast address.   With a Cisco IOS device, the configuration would look like this:   Cisco(config)# interface gigabitEthernet3/1 Cisco(config-if)# ipv6 address fe80::1 link-local Cisco(config-if)# end Cisco# show ipv6 interface gigabitEthernet3/1 gigabitEthernet 3/1 is up, line protocol is up   IPv6 is enabled, link-local address is FE80::1   No Virtual link-local address(es):   Global unicast address(es):     20BA:DD06:F00D:1234::1, subnet is 20BA:DD06:F00D:1234::/48   Joined group address(es):     FF02::1     FF02::2     FF02::1:FF00:2     FF02::1:FFD0:DEBF   MTU is 1500 bytes   ICMP error messages limited to one every 100 milliseconds   ICMP redirects are enabled   ICMP unreachables are sent   ND DAD is enabled, number of DAD attempts: 1   ND reachable time is 30000 milliseconds (using 30000)   ND advertised reachable time is 0 (unspecified)   ND advertised retransmit interval is 0 (unspecified)   ND router advertisements are sent every 200 seconds   ND router advertisements live for 1800 seconds   ND advertised default router preference is Medium   Hosts use stateless autoconfig for addresses.   We can clearly see that the IPv6 link-local address for the interface was successfully modified to fe80::1, while the global unicast address remains in place.   Usage   For end-users this approach simplifies manual configuration of hosts for IPv6 - as e.g. necessary for servers - dramatically. They can use the same set of easy to remember and especially quick to type values for the default gateway and the primary and secondary nameserver. Figure 1 shows how this could look like in Windows 2008R2.                              Figure 1: Using Link-Local IPv6 address as default gateway.     Troubleshooting   While we can simplify the life of end-users by using link-local addresses on router interfaces, we are slightly complicating the life of the network operations staff. As now multiple interfaces on a router can end up with the same link-local address of fe80::1, we need to be more explicit when using this interface in troubleshooting.   As an example: In order to ping the above interface, we need to specify the zone index - which usually corresponds to the interface name - besides the IP address of fe80::1.   This would look like this:   Cisco# ping fe80::1%gigabitEthernet3/1 Type escape sequence to abort. Sending 5, 100-byte ICMP Echos to FE80::1, timeout is 2 seconds: Packet sent with a source address of FE80::1%gigabitEthernet3/1 !!!!! Success rate is 100 percent (5/5), round-trip min/avg/max = 0/0/0 ms Cisco#   Be careful as Cisco IOS requires the interface name to be case sensitive. Also you cannot abbreviate it. This is definitely an annoyance that someone should file as a bug.   You might wonder why we didn’t have to specify a zone index when we entered fe80::1 as the default gateway in Windows above. The answer to this can be found in RFC 4007, section 6, where it states: “An implementation should also support the concept of a “default” zone for each scope”. Thus Windows above is using this “default” zone.   What about DNS via Anycast?   As mentioned earlier, Anycast - both in IPv4 and IPv6 - already gives us the possibility to provide end-users a single or single set of IP addresses for the DNS resolvers, irrespective of their physical location. But why should we “burn” a global unicast IPv6 address for this? Especially as these addresses can be quite long and hard to remember.   Here unique local addresses (ULA) come to the rescue. In most cases ULA can be treated like RFC 1918 addresses in IPv4. Prefixes in the fd00::/8 range have similar properties as those of the IPv4 private address ranges:      They are not allocated by an address registry and may be used in networks by anyone without outside involvement.   They are not guaranteed to be globally unique.   Reverse Domain Name System (DNS) entries (under ip6.arpa) for fd00::/8 ULAs cannot be delegated in the global DNS.   It’s a good idea to keep DNS resolvers to be accessible from within an organization only, which is another good reason to use ULA for this use case. Also not everyone has the luxury to own an easy to remember IP address such as Google with 8.8.8.8 for their public resolver.   Therefore let’s use the addresses fd53::11 and fd53::12 for our internal DNS resolver, making them easy to remember, by combining the non-changeable ULA address part with 53 as DNS uses the protocol TCP/53 and UDP/53. We can even make up the story for end-users that “fd” stands for “fixed DNS”, helping them to remember this service IP.   Yet, we will chose the last octect to be 11 and 12 instead of 1 and 2 to ensure that users don’t confuse these addresses with an default gateway.   With Anycast it’s actually not necessary to have two different IP addresses for a company-internal DNS resolver, as it’s a better idea to handle the failure of a DNS server via ther Anycast routing. But some applications and especially some humans are not happy, when they don’t see two different IP addresses for a DNS resolver.   Where is it used in the wild?   Two examples of service providers that use the IPv6 default gateway of fe80::1 and document it accordingly for their end-users are the German hosting provider Hetzner as well as the University of Wisconsin-Madison. Other examples exist, but without readily available documentation.   The German ISP Deutsche Telekom uses their own line of branded home DSL routers called Speedport. Turns out that the routers within this line that are IPv6 capable use fe80::1 as the default gateway, both for static use as well as via SLAAC. These devices then also offer DNS resolver capabilities via the same IPv6 address.   So far I have not seen anyone using ULA for internal DNS resolver.   Hall of shame   Unfortunately there are various vendors out there, crippling their product’s IPv6 support and thus preventing usage of the above IPv6 deployment pattern.   Examples for this are VMware ESXi, which dropped support for link-local addresses as default gateways in version 5.0 of the hypervisor. Even the Cisco documentation “Deploying IPv6 in the Internet Edge” picked up on this shortcoming.                              Figure 2: ESXi refuses to use a link local IPv6 address as a default gateway during static configuration     Another example is Arista, which doesn’t support manually configuring link-local addresses yet:   Arista(s2)(config-if-Vl51)#ipv6 address fe80::1/64 % Configuring Link local addresses not yet supported   But at least Arista was quick to react when pointing out this shortcoming and created a request for enhancement (RFE).   Analogy   The proposed solution is similar to using a vanity number such as 55555 (5 times 5) for the internal helpdesk through a companies PBX. While employees could just as well use a phone book every time to lookup the Helpdesk’s phone number, using an easy to remember number makes life much easier for end-users.   Also in this analogy the Helpdesk is for internal use only, why accomplishing the same with the global telephone number 1-800-HELP4ME would be possible but fulfill a different use case.  ","categories": ["EdgeCloud"],
        "tags": ["IPv6","Network"],
        "url": "https://www.edge-cloud.net/2013/08/07/ipv6-link-local-addresses-as-default-gateway/",
        "teaser":null},{
        "title": "Using F5 Big-IP LTM with IPv6",
        "excerpt":"A very simple way to enable legacy IPv4-based web applications to be reachable via IPv6 is to use an IPv4/IPv6-enabled load balancer to frontend the application. This is e.g. the approach that Netflix took in mid 2012 to enable their service for IPv6 via the AWS Elastic Load Balancers (ELBs).   Architecture   In this post we will use the F5 Big-IP Local Traffic Manager (LTM) load balancer to provide this capability. You can either use a physical device or even better the Virtual Edition.   Figure 1 shows how this approach would work: The End-User will connect to the load balancer via IPv6, which means that the load balancer needs to have an IPv6 address reachable by the end-users on its external facing interface. The load balancer then connects to the legacy IPv4 web application via IPv4. This means that no changes are necessary to this legacy web application.   If you are already using a load balancer to frontend your application for IPv4, this same load balancer can also terminate your IPv6 traffic. But you’re also free to use a separate “IPv6-only” load balancer, if your operational need dictates this.                              Figure 1: F5 Big-IP LTM brokering IPv6 traffic to legacy IPv4 Web App     F5 Big-IP and IPv6   On a first look at the GUI it doesn’t appear that the F5 Big-IP supports IPv6 addresses on its interfaces or for nodes. In the corresponding dialogues there are only fields for “IP Address” and “Netmask”. For IPv6 we would expect a field for a subnet prefix length instead of the netmask. It turns out that these dialogues gladly accept IPv6 addresses in the typical notation of eight groups of four hexadecimal digits separated by colons along with the subnet prefix length translated into a subnet mask following the same notation.   Although this appears to be a bit awkward at first sight, it will turn out to be much less of a hassle quite quickly: RFC 5375 (IPv6 Unicast Address Assignment Considerations) strongly recommends that in IPv6 the subnet prefix length should always be /64. With that we only need to convert this subnet prefix length of /64 into the legacy style netmask notation.   Using the mechanism known from IPv4, the IPv6 subnet mask for a /64 network would therefore be FFFF:FFFF:FFFF:FFFF:0000:0000:0000:0000 or in short FFFF:FFFF:FFFF:FFFF::. Especially the first notation lets us quickly verify that this netmask is correct:   IPv6 addresses are 128 bit long. If we want to mask out a subnet with the length of 64 bit, this would require us to mask out half of the bits. With the previously mentioned notation of eight groups of four hexadecimal digits separated by colons, this translates into the four first groups being FFFF in hex, which translates to all 1s in binary. And the remaining four groups being all zeros.   Note: In older versions of Big-IP, F5 has a bug that doesn’t allow you to use address shortening via double-colons (“::”) through the GUI or tmsh. Instead all IPv6 addresses need to be written out. Thus the address 20BA:DD06:F00D:1234::11 would need to become 20BA:DD06:F00D:1234:0:0:0:11.   Configuration   Configure the external interface for IPv6   In a first step we need to assign an IPv6 address to the external interface of the F5 Big-IP load balancer. In this example we will use the two IPv6 addresses 20BA:DD06:F00D:1234::11/64 and 20BA:DD06:F00D:1234::12/64 for the actual nodes and 20BA:DD06:F00D:1234::10/64 as the floating address.   Let’s start by creating a new Self-IP under the Network -&gt; Self IPs tab.                              Figure 2: Create new Self-IP     Next enter the IPv6 address for the individual node as the IP address, along with the Netmask of FFFF:FFFF:FFFF:FFFF::. Repeat the same for the second node in an HA setup.                              Figure 3: Use an IPv4-style subnet mask for the IPv6 address instead of the typical prefix length     Now we need to configure the floating IPv6 address in a similar way:                              Figure 4: Create a floating IPv6 address in a similar fashion     Configure an IPv6 Default Route   At this point we should be able to ping the previously created IPv6 interfaces from the same IPv6 network. Obviously we want more than this local-only connectivity and therefore need to configure an IPv6 default route on the F5 Big-IP devices. Do so by creating a new route under the Network -&gt; Routes tab.                              Figure 5: Create a new route for the IPv6 default route     While a default route in IPv4 can be written as 0.0.0.0/0, the IPv6 equivalent is even simpler with ::/0. Thus not only the actual IPv6 address is just ::, but so is also the Netmask. Remember to specify the correct IPv6 address for the gateway as shown in Figure 6.                              Figure 6: Specify the destination address for the IPv6 default route     Creating Virtual Servers   The creation of a virtual server for our legacy IPv4 web application differs only slightly in IPv6 from what you might have already configured for the IPv4 equivalent. Only the IP Address has to be specified as an IPv6 address on the previously configured IPv6 subnet. As Figure 7 shows, all other capabilities can be used the same way.                              Figure 7: Create the virtual servers similar to their IPv4 correspondent, but with an IPv6 address     VMware vCloud Director via IPv6?   In a previous post I have shown how to configure the F5 Big-IP LTM with VMware vCloud Director (vCD) in an IPv4 setup. These two posts combined raise the question whether one could use an F5 Big-IP load balancer to quickly and easily enable VMware vCloud director to be accessible via IPv6 without having to change anything within vCD itself.   It is straight forward and easy to apply the above to the mentioned post and indeed make the HTTP Redirect, the HTTPS traffic and even the Console Proxy available under an IPv6 address via the F5 Big-IP. The HTTP Redirect as well as the HTTPS traffic will work without a flaw, making the web interface of vCD available via IPv6.   Unfortunately it is currently not possible to use the VMware Remote Console (VMRC) via the Console Proxy and IPv6, due to the locally installed VMRC tool incorrectly handling the masking of IPv6 addresses. Thus you will only be able to provide the web portion of vCD via IPv6, while still having to rely on IPv4 for the VMRC part.  ","categories": ["EdgeCloud"],
        "tags": ["F5","IPv6"],
        "url": "https://www.edge-cloud.net/2013/08/12/f5-big-ip-ltm-with-ipv6/",
        "teaser":null},{
        "title": "Physical networks for VMware NSX",
        "excerpt":"During VMWorld 2013 the network virtualization platform VMware NSX was announced by VMware. While there is a plethora of information available on how NSX works or what benefits it brings to the table, the answer on how NSX affects the physical infrastructure remains mostly untouched. Even during various VMWorld presentations this design piece was only covered under the term “Enterprise-class data center network”. Let’s have a look at how the physical underpinning should look like, beyond of what the VMware Network Virtualization Design Guide already states.   Characteristics of Overlay Networks   By definition an overlay network is a virtual network of nodes and logical links that is built on top of an existing (physical) network with the purpose to implement a network service that is not available in the existing network. An example of such an overlay network is the Internet itself, which originally provided a packet-oriented network on top of connection-oriented phone lines. Another example is the Multicast Backbone (MBONE) for multicast deployment. Turning this definition upside down, shows us that capabilities not provided by the overlay network need to be provided by the network underneath.                              Figure 1: Overlay Networks     Before we look at some of the missing features of the overlay network in NSX, in order to determine what the physical network needs to provide, let’s look at some of the pain points in today’s data center networks and what NSX attempts to address.   Data Center Network headaches   Today’s network architects have to face the following challenges while designing data center networks:   Fault containment   Spanning Tree is widely considered a risky technology and for many network engineers it’s hard to master it within their data center network and maintain a stable network. IP routing on the other side, while also not trivial to implement, at least provides much better fault isolation in the case that something does go wrong. Simplified speaking: Using IP routing (L3) over switching (L2) increases the chance of not blowing away the entire data center network if something goes wrong.   Traffic isolation and multi-tenant security   Modern data centers - especially in the cloud age - cater to multiple tenants at the same time. Not only does this require separation of network traffic between various tenants, but also separation of tenant traffic from management or storage traffic. The last a data center operator wants, are customers digging into data of other customers. With tenants and workloads changing regularly, it should be able to change this traffic isolation within minutes or second and not days or hours.   Redundancy and efficiency   Last but not least an enterprise class data center network needs to support mission critical workloads. Therefore it needs to have redundancy built-in to support high availability, while at the same time this build-in redundancy shouldn’t be wasted but instead be used in normal operations. Reducing the throughput in a hardware failure scenario is usually a more efficient approach than keeping excess connections in place that only become available during a failure. Here spanning tree by design disables redundant links while routing supports Equal-cost multi-path routing (ECMP), where packet forwarding to a single destination can occur over multiple “best paths”.   NSX to the rescue?   NSX mainly addresses the pain point of traffic isolation and multi-tenant security by offering Overlay networks that can be brought up and down within minutes and include common network services such as Firewalls or Load Balancers. While the usage of VXLAN within NSX e.g. allows physical network designs that are optimized for redundancy and efficiency, it doesn’t enforce them or help with them in any way. This means that even with NSX deployed the underlying physical network needs to be optimized for fault containment, redundancy and efficiency.                              Figure 2: VMware NSX     Physical network design for NSX   The proposed physical network design is based on the well-known concept of a Spine and Leaf Architecture. Each leaf corresponds to a rack, where the Top-of-Rack (ToR) switches provides L2 connectivity towards the server or storage arrays within the rack. This simple design reduces the requirement for using Spanning Tree to within the rack. Leaf and Spine devices are interconnected via L3 (Routing) and can use the previously mentioned ECMP capability.   Connectivity from within the NSX overlay network to the outside world (WAN or Internet) is provided by VXLAN Tunnel End-Points (VTEP) within the Core layer switches. This capability is e.g. offered by Arista’s Network Virtualization feature. Thus core devices “translate” between VXLAN segments and VLANs.   As an alternative this connectivity can also be provided purely in software - e.g. via an “Edge Rack” - using the Edge devices within NSX.                              Figure 3: Physical Network Design for VMware NSX     The resulting physical network proves to be:      Simple   Highly scalable   Provide high bandwidth   Fault-Tolerant   Provide optional QoS   An Analogy   Why is the physical network still important in the age of Overlay Networks with VMware NSX? To give an analogy: If you want to provide a reliable and fast logistics service - such as FedEx or UPS - you need reliable streets and roads in good shape for the delivery trucks to run on.   For VMware NSX a solid enterprise class physical network - as outlined above - is therefore necessary.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","VMware"],
        "url": "https://www.edge-cloud.net/2013/09/04/physical-networks-for-vmware-nsx/",
        "teaser":null},{
        "title": "Jumbo Frames with VMware ESXi",
        "excerpt":"Ethernet’s default maximum size for data transmission is 1500 bytes due to legacy compatibility reasons. Unfortunately with newer high-speed networks - such as 10 Gigabit Ethernet (10 GigE) - breaking up data into chunks of 1500 bytes - also called frames - creates a lot of overhead with a high header to payload ratio. This not only creates a higher than necessary overhead on the Hypervisor’s CPU, but also prevents one from utilizing the full capabilities of the network hardware. For example: On a 10Gbit link it is possible that you will only be able to transmit about 3-4 Gbps of data between machines using 1500 byte frames, while you can easily saturate 10Gbit when using jumbo frames.   Background   Modern full-featured network equipment can transmit larger frame sizes if configured to do so, usually up to a value of at least 9000 bytes. Even though almost every vendor of network equipment has a different maximum value of what their products can be configured with for MTU, it is recommended to stick to a MTU of 9000 bytes for Jumbo Frames. This value is easy to remember and all network products that support Jumbo frames usually support at least this value.   Jumbo frames should only used in a controlled and segmented environment, for example within an iSCSI or NFS SAN, as they can cause additional overhead from the need for packet fragmentation at the point when converting between a 9000 byte frame network and a standard 1500 byte frame network.   Goal   One of the main use case for Jumbo Frames within VMware vSphere is between an ESXi host and an iSCSI or NFS storage array. Another common use case is between ESXi hosts for VMotion traffic. While this post focuses on the former use case, it is easily transferable to the later.   VMware ESXi uses so-called VMkernel ports to connect the Hypervisor to a storage array. You can think of this port as the Hypervisor kernel’s equivalent to a vNIC for Virtual Machines. In order to use Jumbo Frames between the Hypervisor and the storage array, all elements in between need to be configured for at least the desired MTU size - here 9000. (See Figure 1).                              Figure 1: Jumbo Frames for storage arrays with VMware ESXi     This includes the physical storage array, the physical switch, potentially the physical server (e.g. for the Cisco UCS, where this server includes networking capabilities), the VMware Virtual Switch (both default vSwitch and Distributed vSwitch) and last but not least the VMkernel port itself.   It is important to note that the network elements only need to be configured for at least the desired MTU size. If the MTU size ends up being higher, it will not impact functionality. As an example: The Arista Networks switches have a default MTU size of 9216 bytes, which can remain as is. No need to make any changes here.   I’ll leave the exercise of figuring out how to configure your physical equipment to a MTU size of 9000 to you. Unfortunately almost every vendor has a different approach for achieving this.   Configuring ESXi   Support for jumbo frames has been included in ESX and ESXi for quite a while. But up until version 5.0 of vSphere is was not exposed in the GUI for both the default vSwitch and Distributed vSwitch implementation. It had to be configured via the ESX CLI for a default vSwitch and VMkernel port. Since vSphere 5.1 it is possible to configure the MTU settings via the GUI for all three elements.   Keeping in mind Figure, we will need to change the MTU settings both for the virtual Switch (irrespective of whether this is a default vSwitch or a Distributed vSwitch) and the VMKernel port.   Default vSwitch   For the Default vSwitch navigate to host to which this vSwitch belongs to and choose Manage -&gt; Networking -&gt; Virtual Switches -&gt; Name of Virtual Switch -&gt; Edit. (See Figure 2).                              Figure 2: Jumbo Frames in Default vSwitches     Next change the MTU (Bytes) setting to 9000 under Properties and confirm your choice with a click on OK. (See Figure 3)                              Figure 3: Changing MTU settings for a Default vSwitch     You will have to repeat these steps for all Default vSwitches on all your hosts that you want to leverage Jumbo Frames on.   Distributed vSwitch   To change the MTU settings of a Distributed vSwitch navigate to the desired vDS. Next choose Manage -&gt; Settings -&gt; Properties -&gt; Edit. (See Figure 4).                              Figure 4: Jumbo Frames with Distributed vSwitch     Under the NIC settings tab change the value for MTU to 9000. (See Figure 5).                              Figure 5: Change MTU settings for a Distributed vSwitch     VMKernel ports   Next change the MTU setting of the VMKernel that you want to use with Jumbo Frames. This could e.g. be the VMkernel port used for iSCSI or NFS storage, or the VMKernel port used for VMotion.   Select the ESXi host that hosts the desired VMkernel port and navigate to Manage -&gt; Networking -&gt; Virtual adapters -&gt; Desired VMKernel Device -&gt; Edit. (See Figure 6).                              Figure 6: Jumbo Frames with VMkernel ports     Under the NIC settings tab, change the MTU value to be 9000. (See Figure 7).                              Figure 7: Changing MTU settings of a VMkernel port     You will have to repeat these steps for all VMkernel ports on all your hosts that you want to leverage Jumbo Frames on.   Testing functionality   With all the configuration changes needed at so many different places, it is easy to miss a spot or two. Therefore it is highly recommend to test your settings, before declaring victory.   For this purpose we will use the ESXi tool vmkping to send test traffic from an ESXi host to the storage array or another ESXi host - depending on your use case.                              Figure 8: Options for the tool vmkping     Looking at the options of the vmkping tool, we see two options that are important for this test. (See Figure 8).   One is the ability to set the so-called DF bit or “Do not fragment” with the -d switch. The other is the ability to specify the desired payload size for the ping packet with the -s switch.   Unfortunately the build-in documentation for this tool isn’t very good, as it doesn’t tell you that the -s switch specifies the payload size of the ICMP packet. You therefore need to add 8 bytes of ICMP headers and 20 bytes of IP headers before you end up with the frame size that will actually hit the wire. (See Figure 9).   Also the “Do not fragment” will instruct the network layer of the ESXi host as well as all other network elements not to fragment - therefore not to split up - the packet.                              Figure 9: IP Datagram     In order to assemble a test frame with the size of 9000 bytes, you only need to specify an ICMP payload of 8972 bytes.   If you configured your network correctly, using vmkping -d -s 8972 &lt;IP of storage array&gt; should therefore generate test traffic which is transported to the storage array and back. (See Figure 10).                              Figure 10: Successful vmkping with Jumbo Frames     In case you forgot to configure one of the network elements or specified the wrong payload size, vmkping will indicate that it cannot send the desired test frame. (See Figure 11).                              Figure 11: Unsuccessful vmkping with Jumbo Frames     Configuring Jumbo Frames via CLI   In case you want to configure Jumbo Frames via the CLI for a Default vSwitch or a VMKernel ports, here is what you need to do:   Example for vSwitch   List available Default vSwitches:   esxcfg-vswitch --list   Change the MTU setting for vSwitch0:   esxcfg-vswitch --mtu 9000 vSwitch0   Example for VMkernel interface   List available VMKernel ports:   esxcfg-vmknic --list   Change the MTU settings for a VMKernel port:   esxcfg-vmknic --mtu 9000 \"VMkernel Portgroup Name\"   Jumbo Frames on guests (Virtual Machines)   The default emulated Intel E1000 network device presented to guests in VMware ESX does not support Jumbo Frames, one must install VMware Tools and enable a VMXNET3 or newer interface to take advantage of Jumbo Frames.  ","categories": ["EdgeCloud"],
        "tags": ["VMware"],
        "url": "https://www.edge-cloud.net/2013/11/11/jumbo-frames-vmware-esxi/",
        "teaser":null},{
        "title": "IPv6 Address management of hosts",
        "excerpt":"IPv6 brings a few new address management concepts to the table, that are unknown in IPv4. This article will shed some light on these mechanism along with some guidance on using them as well as giving examples with Linux and Windows hosts.   While IPv4 knew the address management mechanism of None, Manual and DHCP, IPv6 offers the following options:                  None                       Manual                       Stateless Address Auto Configuration (SLAAC) (RFC 4862)                    3.1 Nameserver configured manually           3.2 Nameserver via RDNSS (RFC 6106)           3.3 Nameserver via DHCPv6 (RFC 3736)                                       Statefull with DHCPv6 (RFC 3315)           IPv6 Address Management   1. None   Unfortunately I’ve seen a few cases where products didn’t allow disabling IPv6 on their network interface. And I’ve also seen more than enough networks where architects and engineers decided to “disable” IPv6 by just ignoring it.   Both poses a fundamental security risk, as it exposes an attack vector that is unmanaged or even unknown to the organization.   Therefore network devices and hosts need to offer the option to disable the IPv6 stack.   From a router perspective on the other hand it is not sufficient to just leave away an IPv6 address on an interface. You need to actively suppress IPv6 Router Announcements (RA) and DHCPv6 Replys as well as filter out IPv6 tunnel protocols.   Tools that come in handy here are IPv6 Router Advertisement Guard (RFC 6105) as well as DHCPv6 Shield, also known as DHCPv6 Guard.   2. Manual   The next approach is manually configuring an IPv6 address. This will require manual assignment and configuration of IPv6 addresses along with a prefix length and nameservers on the the client side.   While this approach is straightforward on a router for IPv4 by solely specifying an IPv4 address, it requires a bit more in IPv6. Here we need to ensure that the router will not send out Router Announcements (RA), including those responding to a router solicitation. Otherwise devices in a network will learn about the used prefix and automatically generate an IPv6 address. That’s what SLAAC does and it is described in the next section.   Here is an example for configuring an interface for static IPv6 addressing mode on an Arista Networks device.   interface Vlan5    description IPv6-Only (Manual)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 nd ra suppress all    ipv6 ospf 1 area 0.0.0.0   Let’s attach a Windows 2012 host to the above network segment and see what happens. Figure 1 shows that the Windows 2012 machine fails to automatically acquire a global IPv6 address as expected. Solely the link local address is generated.                              Figure 1: Windows fails to acquire a global IPv6 address     If we want to connect this Windows 2008R2 machine to the network via IPv6, we need to configure the IPv6 network settings manually as shown in Figure 2.                              Figure 2: Manual IPv6 address configuration in Windows     Let’s do the same on Linux with an Ubuntu 13.10 machine. As expected Figure 3 shows us that also the Ubuntu machine will fail to acquire a global IPv6 address automatically as expected. Only the link local address is automatically generated.                              Figure 3: Ubuntu fails to acquire an IPv6 address     Configuration of a manual IPv6 address is done in Ubuntu 13.10 via the file /etc/network/interfaces as shown in Figure 4.                              Figure 4: Configure static IPv6 address in Ubuntu via /etc/network/interfaces     3. Stateless Address Auto Configuration (SLAAC)   The next mechanism is completely unknown in IPv4 and therefore new to IPv6. Stateless Address Auto Configuration (SLLAC) is a mechanism described in RFC 4862, which uses ICMPv6 packets to let routers in a network regularly announce the configured IPv6 prefix. Upon receiving of such an ICMPv6 packet - called Router Advertisement (RA), hosts will automatically generate an IPv6 address based on their own MAC address and this prefix. The mechanism is called “Stateless” as it doesn’t require any state to be kept within the router to avoid IPv6 address collision. Prevention of collisions is solely achieved by utilizing a modified EUI-64 mechanism.   Figure shows how such a Router Advertisement packet looks like in Wireshark. We can clearly see the advertised prefix as well as the prefix length.                              Figure 5: Router Announcement packet in Wireshark     Before we can dive into the configuration of SLAAC, we need to understand that SLAAC by itself only provides a mechanism for assigning a host an IPv6 address as well as a default gateway. It does not provide information for a nameserver to be used as a resolver. To provide this information we need to combine SLAAC with either RDNSS or DHCPv6.   For Linux to pick up IPv6 via SLAAC, the interface has to be configured in /etc/network/interfaces via iface eth0 inet6 auto.   3.1 Stateless - Nameserver configured manually   In this approach we will combine SLAAC for automatic configuration of the hosts address and gateway information along with manually configuring the Nameserver on each host.   An example router configuration for an Arista Networks device would look like this:   interface Vlan5    description IPv6-Only (Stateless with manual DNS)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 ospf 1 area 0.0.0.0   Figure 6 shows an example of how this would look like in Windows.                              Figure 6: SLAAC with Manual Nameserver in Windows     The manual configuration under Ubuntu 13.10 is quite simple: Just add the line dns-nameservers fd80::10 fd80::11 to the corresponding interface section within the file /etc/network/interfaces.   3.2 Stateless - Nameserver via RDNSS   Recursive DNS Server (RDNSS) and DNS Search List (DNSSL) as defined in RFC 6106 are basically an extensions to the RA mechanism.   Figure 7 shows the additional options within an ICMPv6 RA packet to carry the nameserver information.                              Figure 7: RA packet with RDNSS and DNSSL options     Configuration of RDNSS is straight forward on an Arista Networks device as the example below highlights.   interface Vlan5    description IPv6-Only (Stateless with RDNSS)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 nd ra dns-server fd80::10    ipv6 nd ra dns-server fd80::11    ipv6 nd ra dns-suffix ipv6.vmwcs.com    ipv6 ospf 1 area 0.0.0.0   But what about the clients? Windows does not support acquiring the nameserver information via RDNSS and DNSSL.   And also with Ubuntu 13.10 you are out of luck in an out-of-the-box installation. But with Linux you can at least easily retrofit the RDNSS and DNSSL capability. Here is how:   Install the RDNSS package via sudo apt-get install rdnssd and add the line *.rdnssd at the top of the file /etc/resolvconf/interface-order. After restarting the network with sudo service networking restart, your Ubuntu machine will acquire the nameserver information along with the IP address information as shown in Figure 8.                              Figure 8: Acquiring nameserver information via RDNSS     As very few clients support RDNSS and DNSSL today, this addressing approach is not recommended for production usage.   3.3 Stateless - Nameserver with DHCPv6   The next approach involves still using Router Advertisements for the address assignment, but relying on DHCPv6 to hand out the Nameserver information. This approach is still stateless as the DHCPv6 server solely hands out static information about a networks domain information. It does not keep any state about a DHCPv6 lease for a client.   This approach is accomplished by setting the so called “Other” or just “O” flag within the Router Advertisements as shown in Figure 9. This will instruct clients to generate their IPv6 address based on the included prefix, but use DHCP for the nameserver - or “other” - information.                              Figure 9: RA with Other flag set     Before we can configure the “Other” flag on our router, we need to setup a DHCP server, which will serve the nameserver information. In this example I’m using an Infoblox vNIOS grid to do so. Figure 10 shows the configuration.                              Figure 10: Infoblox DHCPv6 basic configuration     Besides setting the “Other” flag within RA, we now also need to configure a DHCP relay on our router’s interface.   interface Vlan5    description IPv6-Only (Stateless with DHCPv6)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 nd other-config-flag    ipv6 dhcp relay destination fdd2:3a59:6db7:3310::240    ipv6 ospf 1 area 0.0.0.0   This approach would be very handy if there weren’t these pesky clients. Both Windows and Linux don’t behave at all as we expect them to.   For Windows if a DHCPv6 server is available but doesn’t offer IPv6 addresses (which is what we want with the Stateless DHCPv6 setup), Windows will ignore the results from the server altogether and not take over the nameserver information. However, if the DHCPv6 server returns an IPv6 address along with the nameserver information, Windows will add the address to the interface and use the additional information. That means your Windows system now has two IPv6 addresses and can use and be reached on either address. Worse, both addresses will be published in DNS.   Linux doesn’t behave that much better as it will ignore the “Other” flag and not ask for nameserver information via DHCPv6. At least here we can fix it quickly by forcing Linux to call “dhclient” once the interface goes up (See Figure 11) and configure “dhclient” to only ask for the IPv6 namesever relevant information, but not for an address (See Figure 12).                              Figure 11: Force Linux to request additional information via DHCPv6                                Figure 12: Instruct Linux only to ask for nameserver-relevant information via DHCPv6     4. Stateful with DHCPv6   Last but not least we have the option to use stateful address assignments via DHCPv6. This approach isn’t very different from what we know from IPv4. You can either specify a pool of addresses from which addresses are randomly drawn for a client. Or you can assign IP addresses fixed to a given host based on it’s NIC’s MAC address.   What’s different with DHCPv6 is that this fixed mapping isn’t based on the MAC address anymore, but on a DHCP Unique Identifier (DUID). There are three types of DUIDs:      Link-layer address plus time   Vendor-assigned unique ID based on Enterprise Number   Link-layer address, based on universally unique identifier (UUID)   This makes finding the DUID not trivial.   Another difference from IPV4 is that we can actually instruct the clients to use DHCP via the Router Advertisements. Besides setting the “Other” flag for obtaining the nameserver information via DHCP, we now also set the “Managed” or “M” flag. This will tell clients to acquire an IPv6 address via DHCPv6 instead of generating one based on the local prefix.   Below is an example for configuring this with an Arista Networks device:   interface Vlan5    description IPv6-Only (Stateful with DHCPv6)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 nd managed-config-flag    ipv6 nd other-config-flag    ipv6 dhcp relay destination fdd2:3a59:6db7:3310::240    ipv6 ospf 1 area 0.0.0.0   This ends up actually being a hybrid between SLAAC, Nameserver via DHCPv6 and Statefull with DHCPv6:   Once again, these pesky clients make it a bit complicated as they now acquire an IPv6 address via SLAAC and DHCPv6. The solution for this is not to set the (A)utonomous flag when advertising the prefix.   This would change the configuration into:   interface Vlan5    description IPv6-Only (Stateful with DHCPv6)    ipv6 address fdd2:3a59:6db7:3340::1/64    ipv6 nd prefix fdd2:3a59:6db7:3340::/64 no-autoconfig    ipv6 nd managed-config-flag    ipv6 nd other-config-flag    ipv6 dhcp relay destination fdd2:3a59:6db7:3310::240    ipv6 ospf 1 area 0.0.0.0   With this Windows will behave as expected and only acquire a single IPv6 address via DHCPv6 along with the nameserver information (See Figure 13).                              Figure 13: Stateful IPv6 address assignment via DHCPv6     For Linux to pick up IPv6 via DHCPv6, the interface has to be configured in /etc/network/interfaces via iface eth0 inet6 dhcp. Once this is done, it will also acquire a single IPv6 address via DHCPv6 along with the nameserver information (See Figure 14).                              Figure 14: Stateful address assignment via DHCPv6     Last but not least this approach has the benefit that IPv6 address usage is tracked within the DHCPv6 server (See Figure 15).                              Figure 15: IPv6 address leases within Infoblox     DHCPv6 Prefix Delegation (DHCPv6-PD)   One addressing scheme that we will glance over in this article is DHCPv6 Prefix Delegation (DHCPv6-PD). This approach is used to assign entire prefixes to downstream routers, so that they can be re-assigned to the downstream router’s subnet. This is e.g. used in an ISP setup where each customer is delegated a /60 prefix, which can the be split into 16x /64 networks within the customer premises.  ","categories": ["EdgeCloud"],
        "tags": ["Arista","Infoblox","IPv6"],
        "url": "https://www.edge-cloud.net/2013/11/18/ipv6-address-management-hosts/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX",
        "excerpt":"In a previous post I have already written about Physical networks for VMware NSX. Now it’s time to put everything together and showcase you how VMware vSphere, VMware NSX and OpenStack come together for a cloud with network virtualization via overlay networks.   As this includes quite a few steps, I’ll split the posts into a series with this one serving as the introduction.   Goal   The goal of this series will be to deploy an OpenStack cloud that leverages VMware vSphere - along with its well-known enterprise-class benefits such as VMotion - as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via VMware NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world (See Figure 1).                              Figure 1: Logical Setup of an OpenStack cloud, leveraging VMware vSphere and VMware NSX     In summary we will end up with:      VMware vSphere 5.5 cluster serving as Hypervisors for our cloud. Well-known features such as VMotion, HA or DRS will still be usable.   VMware NSX for network virtualization, allowing us to create multiple isolated L2 segments per tenant and providing the ability to interconnect them between each other and with the outside world via L3 services.   OpenStack as the Cloud Management System (CMS) providing users a well-known interface via a web-based GUI and easy-to-use API as the frontend of our cloud.   Benefits   One might wonder why to choose VMware vSphere as the Hypervisor of choice for such a setup and not use e.g. KVM instead. Two main reasons come to mind, why the presented architecture is a viable solution:      Usage of Enterprise-class features   Using VMware vSphere with OpenStack will present the entire cluster as a single “node” to OpenStack, allowing Administrators to rely on well-known enterprise class features of the VMware vSphere Hypervisor. This includes e.g. Dynamic Resource Scheduling (DRS) to better distribute the workload across Hypervisors, VMotion to free up a Hypervisor in order to perform preventive maintenance or High Availability (HA) to restart workloads in case of hardware failures.   The predominant model for cloud computing assumes that all components can fail at any time. Thus the application within the workloads need to ensure redundancy. Using VMware vSphere as the Hypervisor of choice with OpenStack, one can deviate from this model and offer a highly reliable cloud instead, known from managed service provider offerings using virtualization today. But it’s also possible to create a hybrid approach, offering both a pure cloud experience as well as a highly available experience within the same cloud.   Ease of deploying VMware vSphere vs Openstack with KVM   Deploying OpenStack with KVM is not easy. Instead it is quite a challenging task, which is why various companies - such as e.g. Mirantis - try to fill this void and offer deployment services or products for OpenStack installation. Deploying a VMware vSphere cluster on the other hand is pretty simple and there are numerous books, hands-on labs or other forms of documentation out there to help. Thus using VMware vSphere as your Hypervisor of choice greatly simplifies the deployment of OpenStack.   We will later also see vSphere OpenStack Virtual Appliance (VOVA). VOVA is an appliance that was built to simplify OpenStack deployment into a VMware vSphere environment for test, proof-of-concept and education purposes. VOVA runs all of the required OpenStack services (Nova, Glance, Cinder, Neutron, Keystone, and Horizon) in a single Ubuntu Linux appliance.   Setup   Please remember that this setup is for test, proof-of-concept and education purposes only. Do not use this in production and do not use any production element in it.   For this setup we will assume the following prerequisites are already in place:      VMware vSphere cluster            Version 5.5 or higher.       vCenter can either be on Windows or as VMware vCenter Server Appliance (VCSA). I will use VCSA.       At least one free vmnic for binding the NSX vSwitch       A single “Datacenter” should be configured in vCenter (This is a temporary limitation as safety precaution).       DRS should enabled with “Fully automated” placement turned on.       The cluster should have only Datastores that are shared among all hosts in the cluster. It is recommended to use a single shared datastore for the cluster.           As part of this walk-through series, we will add the following components:      VMware NSX cluster            A single NSX Controller. Note that VMware NSX requires three or five NSX controller deployed as a cluster on physical hardware in a production environment. As this setup is for test, proof-of-concept and education purposes only, it is sufficient to deploy a single controller inside a VM.       A NSX Manager instance inside a VM.       A NSX service node instance inside a VM.       A NSX gateway instance inside a VM.           vSphere OpenStack Virtual Appliance (VOVA)            A single instance of the vSphere OpenStack Virtual Appliance (VOVA).           The resulting setup will look like Figure 2.                              Figure 2: Physical Setup of an OpenStack cloud, leveraging VMware vSphere and VMware NSX     In this setup we will use a very simple physical network setup. All components will attach to a common Mgmt / VM Network. Only Storage (iSCSI/NFS) and vMotion will use dedicated isolated networks (e.g. VLAN) according to VMware vSphere best practices. As indicated in Figure 2, the Hypervisors as well as the NSX Controller, NSX Gateway and NSX Service Node will form an overlay network via STT tunnels. Please do not use such a simple network setup, sharing management and tenant traffic on the same network segment, in a production environment!   Steps   The required installation and configuration steps include:      Install and configure the VMware NSX appliances   Create and configure the VMware NSX cluster   Install and configure the Open vSwitch inside the ESXi hosts   Import and configure the VMware vSphere OpenStack Virtual Appliance (VOVA)   Create virtual networks and launch a VM instance in OpenStack   Configure the VMware vCenter Plugin for Openstack and look behind the scenes of OpenStack on vSphere  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2013/12/12/openstack-vsphere-nsx/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 1: Install and configure the VMware NSX appliances",
        "excerpt":"Welcome to part 1 of the series on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of an OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   VMware NSX components and their role   A VMware NSX setup consists of various components. Each of them has a specific role in the overall setup. Some components are deployed in form of an appliance, other are installed as a module into the Hypervisor:      NSX Controller  The NSX Controllers implements a network control plane for controlling the Open vSwitch (OVS) devices that perform packet forwarding. Controller Cluster nodes cooperate to manage all OVS devices and enforce consistency between the logical network view (defined via the NSX API) and the transport network view (implemented by OVS-enabled access switches).   Transport Nodes  Hypervisors, NSX Service Nodes, and NSX Gateways are represented in NSX as transport nodes. A transport node is any physical or virtual device that runs Open vSwitch and is managed by the NSX Controller to implement logical networks. How the NSX Controller manages the transport node depends on the role of that transport node:            Hypervisors  Leverage an Open vSwitch to provide network connectivity for VM-based workloads. Like Service Nodes and Gateways, hypervisors are represented in NSX using the transport node entity.       Gateways  An NSX Gateway connects logical networks to the data center’s physical network or to physical applications.       Service Node  NSX Service Nodes offload network packet processing from hypervisor Open vSwitches, such as broadcast, unknown unicast and multicast (BUM) replication and CPU-intensive cryptographic processing           NSX Manager  The NSX Manager provides a GUI for operators to setup and configure an NSX network. It is not used by OpenStack itself and could be removed in the case that the operator uses CLI commands for all setup and configuration steps.   If you want to learn more about the architecture of VMware NSX, check out the VMware NSX Network Virtualization Design Guide (PDF).   Install Virtual Appliances   As a first step we will install the NSX appliances as form of a virtual appliance, thus inside a VM (See figure 1). VMware provides the software for the NSX appliances as an ISO file.                              Figure 1: NSX appliances as part of the setup     Inside VMware vSphere create four virtual machines with the following settings:      Guest Operating System: Ubuntu Linux (64-bit)   CPU: 1   Memory: 2 GB   Network: 1 vNIC (E1000)   Disk Size: 16 GB   Note that these values are well below the supported minimum requirements for VMware NSX in a production environment. As this setup is for test, proof-of-concept and education purposes only, it is sufficient to deploy it with the above settings.   Next mount the corresponding ISO image to each of the four appliances. The VMs should boot via CD-ROM and display an installation screen (See Figure 2). Choose Automated Install to proceed.                              Figure 2: NSX Installer     An automated installation of the NSX appliance will start. This can take several minutes. (See Figure 3)                              Figure 3: NSX Installation Process     At the end of the installation you will be greeted with a login screen. Login with the username admin and the password admin. (See Figure 4)                              Figure 4: Login into NSX appliance after successful install     Basic Configuration   After you are logged in, we will perform the basic configuration. This basic configuration will be the same for all four appliances. I therefore recommend, that you first complete the installation of all four appliances.   First, configure a new admin password for each of the appliances with the command set user &lt;username&gt; password:   nsx-controller # set user admin password Enter new password: Retype new password: Password updated successfully. nsx-controller #     Next, configure the appliance’s hostname with the set hostname &lt;hostname&gt; command:   nsx-controller # set hostname nsxc-l-01a nsxc-l-01a #   Although NSX appliances are based on Ubuntu Linux they do not use eth0 as the network interface. Instead they use a so-called integration bridge interface for network connectivity. This device is called breth0. In this step we will configure an IP address on this physical interface via the command set network interface breth0 ip config static &lt;IP address&gt; &lt;Netmask&gt;. Notice that the NSX appliance will clear the DNS settings if it had acquired an IP address and DNS settings via DHCP before.   nsxc-l-01a # set network interface breth0 ip config static 192.168.110.101 255.255.255.0 Setting IP for interface breth0... Clearing DNS configuration... nsxc-l-01a #   You can verify the network settings with the command show network interface breth0:   nsxc-l-01a # show network interface breth0 IP config: static Address: 192.168.110.101 Netmask: 255.255.255.0 Broadcast: 192.168.110.255 MTU: 1500 MAC: 00:50:56:12:34:56 Admin-Status: UP Link-Status: UP SNMP: disabled nsxc-l-01a #   Next, it’s time to add a default route via the command add network route 0.0.0.0 0.0.0.0 &lt;gateway&gt;:   nsxc-l-01a # add network route 0.0.0.0 0.0.0.0 192.168.110.1 nsxc-l-01a #   Again, we can verify the settings via the command show network route:   nsxc-l-01a # show network route Prefix/Mask          Gateway          Metric  MTU     Iface 0.0.0.0/0            192.168.110.2    0       intf    breth0 192.168.110.0/24     0.0.0.0          0       intf    breth0 nsxc-l-01a #   Now add the DNS servers via the command add network dns-server &lt;Server name&gt;:   nsxc-l-01a # add network dns-server 192.168.110.10 nsxc-l-01a #   Last, but not least add the NTP server via the command add network ntp-server &lt;Server name&gt;:   nsxc-l-01a # add network ntp-server 192.168.110.1 * Stopping NTP server ntpd                                               [ OK ] Synchronizing with NTP servers. This may take a few seconds ... 12 Dec 20:34:51 ntpdate[3241]: step time server 192.168.110.1 offset - 7.140147 sec * Starting NTP server ntpd nsxc-l-01a #   All the above basic configuration steps are the same for all four NSX appliances, except that each appliance needs to receive its own IP address and hostname. Therefore ensure that you complete the above steps before moving on.   Node-specific configuration   Next we will perform the node specific configuration on the Controller node, the Gateway node and the Service node. The configuration of the Management VM is already complete with the steps above.   Controller Node   We need to specify for the controller node which IP address should be used as the management address as well as the API address. This is necessary for the case that an NSX controller is deployed with multiple IP addresses in different subnets. As mentioned earlier I’ll keep it single, utilizing only a single subnet.   First, set the IP address the controller should use for management traffic with the command set control-cluster management-address &lt;IP address&gt;   nsxc-l-01a # set control-cluster management-address 192.168.110.101 nsxc-l-01a #   Next tell the NSX controller which IP address to use for the switch manager traffic (this is the traffic that communicates with OVS interfaces) via the command set control-cluster role switch_manager listen-ip &lt;IP address&gt;   nsxc-l-01a # set control-cluster role switch_manager listen-ip 192.168.110.101 nsxc-l-01a #   Then instruct the controller which IP address to use for the API interface traffic (this is the interface that handles northbound REST API traffic) with the command set control-cluster role api_provider listen-ip &lt;IP address&gt;   nsxc-l-01a # set control-cluster role api_provider listen-ip 192.168.110.101 nsxc-l-01a #   Once you have completed these steps, you can turn up the controller cluster with the command join control-cluster &lt;own IP address&gt;. Notice that for the first controller you are basically joining the controller to itself. In the case that the controller has multiple IP addresses, the address to use is the one you specified when you set the management IP address earlier.   nsxc-l01a # join control-cluster 192.168.110.101 Clearing controller state and restarting Stopping nicira-nvp-controller: [Done] Clearing nicira-nvp-controller's state: OK Starting nicira-nvp-controller: CLI revert file already exists mapping eth0 -&gt; bridge-pif ssh stop/waiting ssh start/running, process 3908 mapping breth0 -&gt; eth0 mapping breth0 -&gt; eth0 ssh stop/waiting ssh start/running, process 4057 Setting core limit to unlimited Setting file descriptor limit to 100000  nicira-nvp-controller [OK] ** Watching control-cluster history; ctrl-c to exit ** =================================== Host nsxc-l-01a Node a57282ce-87fe-44ed-9db1-9dcc21204e33 (192.168.110.101)   ---------------------------------   12/12 20:53:20: Joining cluster via node 192.168.110.101   12/12 20:53:20: Waiting to join cluster   12/12 20:53:20: Joined cluster; initializing local components   12/12 20:53:24: Initializing data contact with cluster   12/12 20:53:32: Fetching initial configuration data   12/12 20:53:34: Join complete nsxc-l-01a #   Note that for the second and third controllers in a cluster, you would point them to the IP address of any existing controller in the cluster. But as we are only using a single controller here, this is not necessary.   Gateway Node and Service Node   The Gateway nodes as well as the service nodes need to be made aware of the controller cluster.   On both nodes this is done with the command add switch manager &lt;IP address of a controller node&gt;:   nsxg-l-01a # add switch manager 192.168.110.152 Waiting for the manager CA certificate to synchronize... Manager CA certificate synchronized nsxg-l-01a #   This completes the installation of the NSX appliances. Next in the series on OpenStack with vSphere and NSX is Part 2 with the creation and configuration the VMware NSX cluster via the NSX manager. While we finished the installation and basic configuration of the VMware NSX appliances in this post, the next post will show how to use the NSX Manager’s web-based GUI to join these parts together and build the basic functionality of an NSX installation.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2013/12/17/openstack-with-vsphere-and-nsx-part1/",
        "teaser":null},{
        "title": "Handling the VMware vSphere 5.5 Active Directory integration error",
        "excerpt":"While attempting to integrate the VMware vSphere 5.5 vCenter Server Appliance (vCSA) with Microsoft Active Directory you might have stumbled over the error message “Error: Idm client exception: Failed to establish server connection”. This article will show you how to work around this issue and still use Active Directory with your vCSA.   Reproduce the error   The following steps show you how to reproduce the error. In a subsequent step you will then see how to prevent it.   We start with the VMware vCenter Server Appliance (vCSA) joined to the Active Directory (See Figure 1).                              Figure 1: VMware vCenter Server Appliance with Active Directory integration enabled     Login with the vCSA SSO credentials Administrator@vsphere.local. It has a default password of vmware (See Figure 2). Note that this account is different from the user root.                              Figure 2: Login to vCSA with the SSO administrator credentials     Navigate to Home -&gt; Administration -&gt; Single Sign-On -&gt; Configuration (See Figure 3).                              Figure 3: Navigate to Administration -&gt; Single Sign-On -&gt; Configuration     Next, try to create a new Identity Source (See Figure 4).                              Figure 4: Add a new SSO Identity Source     As the vCSA is already joined to the Active Directory domain the expectation would be to use the Integrated Windows Authentication for Active Directory along with using the vCSA machine account. In the end, that’s why we added vCSA to the Active Directory in the first place. Unfortunately we will soon see that we are let down with this expectation.   For now select as the Identity source type the value Active Directory (Integrated Windows Authentication), confirm your domain name and select Use machine account (See Figure 5).                              Figure 5: Configure an Active Directory identity source with default parameters     After closing the previous dialog with OK, you should see the new identity source (See Figure 6).                              Figure 6: Active Directory identity source     Next, navigate to Administration -&gt; Configuration -&gt; Single Sign-On -&gt; Users and Groups -&gt; Users and pick under Domain the domain you just created (See Figure 7).                              Figure 7: Attempt to view the users in the Active Directory identity source     Instead of displaying the users from Active Directory as expected, vCSA shows the error message “Error: Idm client exception: Failed to establish server connection”. Thus it appears as if the default Active Directory integration in vCSA is broken (See Figure 8).                              Figure 8: Error message while attempting to view the users in the Active Directory identity source     The Workaround   Now that we have seen the issue, let’s work around it. This way we can still achieve the goal of using Active Directory as an identity source for SSO in VMware vSphere.   Start by removing the Active Directory identity source that you previously created (See Figure 9).                              Figure 9: Remove the Active Directory identity source     Create a new identity source. This time select for the Identity source type the value Active Directory as a LDAP server. Fill out the remaining fields as follows (See Figure 10):      Name: Your AD domain name; E.g. “corp.local”   Base DN for users: Split your domain name in pieces along the dots (“.”) and prefix each part with a “dc=”. Place commas “,” in between each part; E.g. “dc=corp,dc=local”   Domain name: Your AD domain name; E.g. “corp.local”   Domain alias: Your netbios name of the AD domain; E.g. “CORP”   Base DN for groups: Same a the Base DN for users; E.g. “dc=corp,dc=local”   Primary Server URL: The Active Directory server as a URL with the protocol “ldap://” and the port 389.; E.g. ldap://192.168.110.10:389   Secondary Sever URL: Another Active Directory server as a URL if you have one. Otherwise leave it blank; E.g. ldap://192.168.110.20:389   Username: An Active Directory username in netbios notation with privileges to read all users and groups; E.g. “CORPAdministrator”   Password: The password of the above user.                              Figure 10: Add an identity source with Active Directory as a LDAP Server     Test your settings by clicking on the Test Connection button. This will attempt to connect to the Active Directory as a LDAP server with the provided settings (See Figure 11).                              Figure 11: Test the connectivity of the new identity source     If the connection test was successful, save the settings. You will notice the new identity source, you just added (See Figure 12).                              Figure 12: Active Directory identity source via LDAP     Now, if you return to Administration -&gt; Configuration -&gt; Single Sign-On -&gt; Users and Groups -&gt; Users and pick under Domain the domain you just created, you will see users and groups from this Active Directory (See Figure 13). With this we have achieved our goal.                              Figure 13: Users in an Active Directory identity source successfully displayed     Don’t forget to add users from your Active Directory to corresponding vCSA groups to grant access to these users (See Figure 14).                              Figure 14: Add Users to the Groups for assigning permissions     Also don’t forget to add users from your Active Directory to the vCenter Permissions (See Figure 15).                              Figure 15: Add Users to the vCenter     Windows Session Authentication   The Windows Session Authentication for the vSphere Web Client (See Figure 16) will still work with this workaround. The reason is that vCSA uses Kerberos to authenticate the Windows Client against the vSphere Web Client (via an installable plugin). Once that authentication is successful it solely checks in the identity source matching the domain name if that user exists. No Password is ever transferred from the Windows Client to the vCSA or the identity source.                              Figure 16: The Windows session authentication still works     Final remarks   One would hope that after joining vCSA to the Active Directory domain, it is possible use the Integrated Windows Authentication for Active Directory along with using the vCSA machine account. While we are being let down on this expectation, the above workaround will at least allow you to use an Active Directory as an identity source for SSO in VMware vSphere.   Keep in mind that for adding Active Directory as an identity source via LDAP it is not necessary to join the vCSA to Active Directory. But you might have other independent reasons to do so.  ","categories": ["EdgeCloud"],
        "tags": ["VMware"],
        "url": "https://www.edge-cloud.net/2013/12/20/handling-vmware-vsphere-5-5-active-directory-integration-error/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 2: Create and configure the VMware NSX cluster",
        "excerpt":"Welcome to part 2 of the series on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of a OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   In Part 1: “OpenStack with vSphere and NSX – Part 1: Install and configure the VMware NSX appliances” we finished the installation and basic configuration of the VMware NSX appliances. In this post you will learn how to use the NSX Manager’s web-based GUI to join these parts together and build the basic functionality of an NSX installation consisting of a NSX Cluster, NSX Gateway and NSX Service Node as transport nodes as well as a Transport Zone and a Gateway Service.   Connect the NSX Manager to the NSX Cluster   Let’s start by making the NSX Manager aware of the NSX Cluster to be used. This will allow us in subsequent steps to manage this NSX cluster and add further components.   First login to the NSX Manager with the credentials that were set in the previous post (See Figure 1). By default this would be the username “admin” along with the password “admin”.                              Figure 1: Login to NSX Manager     After the initial login, NSX Manager will indicate that it is currently not connected to any cluster and therefore not much of any use (See Figure 2). Click on Add Cluster to add the first NSX cluster to the NSX Manager.                              Figure 2: NSX Manager without a cluster     Provide the IP address along with the access credentials of any node within the NSX cluster (See Figure 3). Remember that in this case we have only a single NSX controller available.                              Figure 3: Connect to NSX Controller Cluster - Step 1     Next, provide a name for this NSX cluster (See Figure 4).                              Figure 4: Connect to NSX Controller Cluster - Step 2     Click on Use This NSX Manager, to specify the NSX Manager as the syslog collector of this NSX cluster. Leave all other settings as is (See Figure 5). In a production environment you would obviously adapt these settings, but for this setup these settings are just fine.   Complete your selection by clicking on Configure.                              Figure 5: Connect to NSX Controller Cluster - Step 3     You should see the NSX controller cluster successfully added with the Connection status as “Up” (See Figure 6).                              Figure 6: NSX Controller Cluster successfully added     Going to the NSX Manager Dashboard will show you the NSX Controller without any transport nodes added (See Figure 7).                              Figure 7: NSX Controller Cluster without any Transport Nodes     Keep in mind that a single NSX Manager can manage multiple NSX controller cluster.   Add a new Transport Zone   In VMware NSX a transport zone corresponds to the underlying physical network used to interconnect transport nodes (hypervisors, service nodes, gateways). A simple VMware NSX deployment will have a single transport zone that represents the physical network connectivity within the data center. But more complex topologies with multiple networks within and outside the data center are possible via multiple transport zones. Here we will use a single transport zone as all transport nodes connect to the same underlying network.   On the NSX Manager Dashboard, within the Summary of Transport Components section, click on Add within the Zones row (See Figure 8).                              Figure 8: Add a new Transport Zone     Give the new transport zone a name (See Figure 9). Click on Save &amp; View to finish the creation of a new transport zone.                              Figure 9: Create Transport Zone     Note down the UUID of the newly created transport zone (See Figure 10). We will need this UUID in a later step to configure the vSphere OpenStack Virtual Appliance (VOVA).                              Figure 10: UUID of the new Transport Zone     Add a new Gateway node   An NSX Gateway connects logical networks to the data center’s physical network or to physical applications. While the NSX gateway appliance was installed and configured in a previous step, it now needs to be added to the NSX cluster.   Return to the NSX Manager Dashboard, within the Summary of Transport Components section, click on Add within the Gateways row (See Figure 11).                              Figure 11: Add a new Gateway     Confirm that the pre-selected transport type is Gateway (See Figure 12).                              Figure 12: Create Gateway - Step 1     Give the gateway node a name (See Figure 13). I usually pick the hostname.                              Figure 13: Create Gateway - Step 2     Here leave the settings as is (See Figure 14). A Management Rendezvous Client would be necessary if NSX Controller and NSX Gateway do not have direct network connectivity, the Tunnel Keep-alive Spray would randomize TCP source ports for STT tunnel keep-alives for packet spray across active network path and VTEP enabled would be for physical NSX gateways as e.g. offered by Arista EOS. As none of this applies here, we will stick to the default settings.                              Figure 14: Create Gateway - Step 3     Before we can complete the next step, we need to extract the SSL certificate from the NSX gateway appliance. To do so, connect via SSH or console to the NSX gateway appliance and login with the previously defined password and the username admin. Use the command show switch certificate to display the required certificate (See Figure 15).                              Figure 15: Extract NSX Gateway certificate - Step 1     Copy the certificate including the lines ----BEGIN CERTIFICATE---- and ----END CERTIFICATE---- (See Figure 16). We will need this certificate in the next step.                              Figure 16: Extract NSX Gateway certificate - Step 2     Back in the NSX Manager select the Credential Type of Security Certificate and paste the previously copied certificate into the Security Certificate field (See Figure 17).                              Figure 17: Create Gateway – Step 4     Next, we need to create a transport connector for the NSX Gateway. A transport connector does two things:      It specifies the transport type or tunnel protocol to be used by the transport node. This transport type has to match for nodes to be able to form an overlay tunnel and communicate.   It maps the (physical) interface of a transport node to a transport zone. With this it would be possible to use a single transport node (e.g. gateway) in multiple transport zones.   We will use STT as the transport type in this setup. And remember that we are only using a single transport zone.   Let’s get started by clicking on Add Connector (See Figure 18).                              Figure 18: Create Gateway – Step 5     Select STT as the Transport Type, ensure that the Transport Zone UUID matches our single transport zone and enter the IP address of the NSX gateway as the IP address (See Figure 19).                              Figure 19: Create Gateway – Step 6     This concludes the setup of the NSX Gateway in NSX Manager. Click on Save to finish the configuration (See Figure 20).                              Figure 20: Create Gateway – Step 7     Return to the NSX Manager Dashboard, where you will see the new Gateway within the Summary of Transport Components section, within the Gateways row. Click on the number for active gateways to see more details (See Figure 21).                              Figure 21: NSX Gateway successfully added     You should see the NSX gateway successfully added with the Connection status as Up (See Figure 22).                              Figure 22: NSX Gateway displaying status of Connected     Add a new Service node   NSX Service Nodes offload network packet processing from hypervisor Open vSwitches, such as broadcast, unknown unicast and multicast (BUM) replication and CPU-intensive cryptographic processing. While the NSX service node appliance was installed and configured in a previous step, it now needs to be added to the NSX cluster.   Return to the NSX Manager Dashboard, within the Summary of Transport Components section, click on Add within the Service Nodes row (See Figure 23).                              Figure 23: Add a new Service Node     Confirm that the pre-selected transport type is Service Node (See Figure 24).                              Figure 24: Create Service Node - Step 1     The rest of the dialog and workflow for adding the service node is equivalent to what we have already seen while adding the gateway node:      Specify the Display Name   Extract the SSL certificate from the NSX service node appliance   Paste the SSL certificate into the add dialog   Specify a transport connector with the type STT and the IP address of the service node   We will therefore skip ahead to the result.   Return to the NSX Manager Dashboard, where you will see the new service node within the Summary of Transport Components section, under the Service Nodes row (See Figure 25).                              Figure 25: NSX Service Node successfully added     Add a new Gateway Service   A Gateway node by itself does not yet offer any functionality. For that we need to configure a gateway service that will leverage the gateway node. Two types of gateway services exist in VMware NSX:      Layer 2 (L2) Gateway Services - allows VMs to be connected at Layer 2 (L2) to an external network   Layer 3 (L3) Gateway Services - lets you connect logical router ports to physical IP networks via network interfaces on NSX Gateway nodes   We will configure a L3 Gateway Service.   Return to the NSX Manager Dashboard, within the Summary of Transport Components section, click on Add within the Gateway Services row (See Figure 26).                              Figure 26: Add a new Gateway Service     Select L3 Gateway Service as the Gateway Service Type (See Figure 27).                              Figure 27: Create Gateway Service - Step 1     Enter a name for the newly created gateway service (See Figure 28).                              Figure 28: Create Gateway Service - Step 2     Now we need to specify the gateway node that will execute this gateway service. Start by clicking on Add Gateway (See Figure 29).                              Figure 29: Create Gateway Service - Step 3     Select the gateway node that was previously created as well as breth0 for the Device ID. The device ID is the interface on the NSX gateway node that connects to the external (upstream) network (See Figure 30).                              Figure 30: Create Gateway Service - Step 4     Verify your Gateway node configuration and finish the installation of the gateway service by clicking on Save &amp; View (See Figure 31).                              Figure 31: Create Gateway Service - Step 5     Note down the UUID of the newly created gateway zone (See Figure 32). We will need this UUID along with the UUID of the transport zone in a later step to configure the vSphere OpenStack Virtual Appliance (VOVA).                              Figure 32: UUID of NSX Gateway Service     Return to the NSX Manager Dashboard, where you will see the new Gateway Zone within the Summary of Transport Components section, within the Gateway Zones row (See Figure 33).                              Figure 33: NSX Setup with Gateway, Service Node, Transport Zone and Gateway Services     This completes the basic configuration of the NSX cluster via the NSX Manager. Next in the series on OpenStack with vSphere and NSX is Part 3 with the installation and configuration of the Open vSwitch inside the ESXi hosts. While we finished adding the NSX gateway and NSX service node to the NSX cluster in this post, the next post will show how to add the two ESX hypervisor to the NSX cluster. This is done by installation and configuration of the Open vSwitch inside ESX.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2013/12/27/openstack-with-vsphere-and-nsx-part2/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 3: Install and configure the Open vSwitch inside the ESXi hosts",
        "excerpt":"Welcome to part 3 of the series on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of an OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   In the previous article: “OpenStack with vSphere and NSX – Part 2: Create and configure the VMware NSX cluster” we completed the basic configuration of the NSX cluster via the NSX Manager, by adding the gateway and service node to the NSX cluster. This article will highlight the installation and configuration of the Open vSwitch inside the ESXi hosts.   Verify pre-requisites   The NSX vSwitch is a virtual switch for the VMware vSphere platform, similar to its brothers the Standard vSwitch and the Virtual Distributed Switch. As such the NSX vSwitch needs a dedicated physical uplink (vmnic) to connect to the upstream network. Before proceeding to the actual installation, ensure that you have a vmnic interface available on all your ESXi hosts (See Figure 1). In this guide I will be using vmnic1 for all ESXi hosts.                              Figure 1: Ensure a free vmnic is available on ESXi hosts     Install the NSX vSwitch   The NSX vSwitch is provided as a vSphere Installation Bundle (VIB) that needs to be installed on each ESXi hosts that you plan on using. While various methods exist for installing a VIB on an ESXi host, this article will showcase the installation via an SSH connection.   First make the VIB file available to the ESXi hosts via e.g. shared storage (See Figure 2). This will greatly simplify the work associated with copying the VIB file to the ESXi hosts.                              Figure 2: Upload the NSX vSwitch vib to storage accessible by ESXi     Next temporarily enable SSH access to the ESXi hosts (See Figure 3). After we are done with the installation of the VIB file, you can turn off the SSH daemon again.                              Figure 3: Enable SSH access to ESXi hosts     After you have enabled SSH access the ESXi hosts, connect to your first ESXi host via SSH. Start the installation of the NSX vSwitch VIB file via the command esxcli software vib install --no-sig-check -v &lt;path and filename&gt;:   ~ # esxcli software vib install --no-sig-check -v /vmfs/volumes/SiteA-IPv6-NFS/vmware-nsxvswitch-2.0.1-30494-release.vib Installation Result    Message: Operation finished successfully.    Reboot Required: false    VIBs Installed: VMware_bootbank_vmware-nsxvswitch_2.0.1-30494    VIBs Removed:    VIBs Skipped: ~ #   Ensure that the VIB is installed successfully.   Configure the NSX vSwitch   While the configuration of the Standard vSwitch and the virtual Distributed Switch is usually done via vCenter, the NSX vSwitch is configured via the CLI. Therefore let’s go ahead and configure the NSX vSwitch for this host.   Start by linking the NSX vSwitch to a physical uplink interface (vmnic). This is done via the command nsxcli uplink/connect &lt;interface&gt;:   ~ # nsxcli uplink/connect vmnic1 ~ #   Next we configure the IP address for the transport endpoint. This transport endpoint creates overlay tunnels with other transport endpoints, such as Hypervisors, Gateway nodes and Service Nodes. The NSX vSwitch uses a separate IP stack for this, which means that the VMWare NSX transport endpoint has its own default gateway.   Set the IP address of the transport endpoint with the command nsxcli uplink/set-ip &lt;interface&gt; &lt;ip address&gt; &lt;netmask&gt;:   Note: If the physical switchport that this vmnic connects to is not configured as an access port but as a trunk, you will need to also specify the correct VLAN to be used with the command nsxcli uplink/set-ip &lt;interface&gt; &lt;ip address&gt; &lt;netmask&gt; vlan=&lt;id&gt;   ~ # nsxcli uplink/set-ip vmnic1 192.168.110.121 255.255.255.0 ~ #   Next, set the default gateway with the command nsxcli gw/set tunneling &lt;ip address of default gateway&gt;   ~ # nsxcli gw/set tunneling 192.168.110.2 ~ #   Next is the creation of a Transport-Net Bridge to which Virtual Machines will later connect to. The name of this Bridge needs to be known to our OpenStack installation for the architecture to work. As we will be using vSphere OpenStack Virtual Appliance (VOVA) this uuid and name must be NSX-Bridge.   Create the NSX bridge with the command nsxcli network/add &lt;UUID&gt; &lt;Name&gt;:   ~ # nsxcli network/add NSX-Bridge NSX-Bridge nsx.network manual success ~ #   Similar to the NSX appliances, the next step registers the NSX vSwitch with the NSX controller. First use the command nsxcli manager/set ssl:&lt;IP address of a NSX controller node&gt; to point the NSX vSwitch to the NSX controller. In the case of an NSX controller cluster you can specify any IP address of a cluster member.   ~ # nsxcli manager/set ssl:192.168.110.101 ~ #   Next extract the SSL certificate from the NSX vSwitch with the command cat /etc/nsxvswitch/nsxvswitch-cert.pem.   Copy the text including the line ----BEGIN CERTIFICATE---- and ----END CERTIFICATE---- (See Figure 4). You will need this text in the next step.                              Figure 4: NSX OVS SSL certificate displayed for an ESXi host     Don’t close the SSH session yet. We will need to come back.   Return to the NSX Manager Dashboard. Within the Summary of Transport Components section, click on Add within the Hypervisor row (See Figure 5).                              Figure 5: Add a new Hypervisor in NSX Manager     Confirm that the pre-selected transport type is Hypervisor (See Figure 6).                              Figure 6: Create Hypervisor - Step 1     Give the gateway node a name (See Figure 7). I usually pick the hostname.                              Figure 7: Create Hypervisor - Step 2     As the Integration Bridge Id specify br-int (See Figure 8). Leave the other values with the default setting. The Tunnel Keep-alive Spray would randomize TCP source ports for STT tunnel keep-alives for packet spray across active network path.                              Figure 8: Create Hypervisor - Step 3     Select the Credential Type of Security Certificate and paste the previously copied certificate into the Security Certificate field (See Figure 9).                              Figure 9: Create Hypervisor - Step 4     Create a transport connector for the NSX vSwitch using STT as the transport type and the IP address that you configured a few steps earlier (See Figure 10).                              Figure 10: Create Hypervisor - Step 5     Return to the NSX Manager Dashboard, where you will see the new Hypervisor within the Summary of Transport Components section, within the Hypervisors row. Click on the number for active hypervisors to see more details (See Figure 11).                              Figure 11: ESXi successfully added as Hypervisor     You should see the ESXi host with the NSX vSwitch successfully added as a hypervisor with the Connection status as “Up” (See Figure 12).                              Figure 12: Hypervisor displaying status of “Connected”     As a last step we need to instruct VMware NSX to export the OpenStack virtual machine virtual interface (vif) UUID as extra information besides the VMware vSphere one. This is necessary as OpenStack uses a different UUID than VMware vSphere does. Without this setting OpenStack wouldn’t “recognize” a VM that it created for further operations via the Neutron API.   Instruct NSX to allow custom vifs with the command nsxd --allow-custom-vifs. When asked for a username and password, enter the username and password for the ESXi host.   ~ # nsxd --allow-custom-vifs 2013-12-18T19:50:15Z|00001|ovs_esxd|INFO|Normal operation username : root Password: WARNING: can't open config file: /etc/pki/tls/openssl.cnf nsxd: NSXD will be restarted now. Killing nsxd (227588). 2013-12-18T19:50:21Z|00001|ovs_esxd|INFO|Normal operation WARNING: can't open config file: /etc/pki/tls/openssl.cnf Starting nsxd. ~ #   You can safely ignore the warning message about the config file /etc/pki/tls/openssl.cnf.   Instead verify that the configuration change has been applied with the command nsxcli custom-vifs/show   ~ # nsxcli custom-vifs/show Custom-VIFs: Enabled ~ #   Return to the vSphere Web Client where you can see vmnic1 connected to the NSX vSwitch (See Figure 13).                              Figure 13: NSX vSwitch for ESXi visible in vSphere Web Client     Repeat the above steps for any additional ESX host that you want to use with this setup.   Verify the setup   After you have installed and configured the NSX vSwitch on all Hypervisors, you can see the results in the NSX Manager Dashboard (See Figure 14).                              Figure 14: Two ESXi hosts added the NSX cluster     This completes the installation of the NSX cluster, including ESXi as Hypervisors. Next in the series on OpenStack with vSphere and NSX is Part 4: “OpenStack with vSphere and NSX – Part 4: Import and configure the VMware vSphere OpenStack Virtual Appliance (VOVA)” with the import and configuration of the VMware vSphere OpenStack Virtual Appliance (VOVA). While it would be possible to use vSphere and NSX without OpenStack, it would require either another Cloud Management System (CMS) or manual creation of virtual network and NIC bindings via the ESXi CLI. As this goes beyond the simple setup that I want to showcase here, I will not include it.   Instead the next article highlights the easy to use VMware vSphere OpenStack Virtual Appliance (VOVA), an easy-to-use appliance that was built to simplify OpenStack deployment into a VMware vSphere environment for test, proof-of-concept and education purposes. VOVA runs all of the required OpenStack services (Nova, Glance, Cinder, Neutron, Keystone, and Horizon) in a single Ubuntu Linux appliance. Therefore you need to be a bit more patient before we can reap the fruits of our labor.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2014/01/03/openstack-with-vsphere-and-nsx-part3/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 4: Import and configure the VMware vSphere OpenStack Virtual Appliance (VOVA)",
        "excerpt":"Welcome to part 4 of the series: “OpenStack with vSphere and NSX” on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of an OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   In the previous post we completed the installation of the NSX cluster, including ESXi as Hypervisors. In this article we will see the import and configuration of the VMware vSphere OpenStack Virtual Appliance (VOVA).   VMware vSphere OpenStack Virtual Appliance (VOVA)   VOVA is an appliance that was built to simplify an OpenStack deployment into a VMware vSphere environment for test, proof-of-concept and education purposes. VOVA runs all of the required OpenStack services (Nova, Glance, Cinder, Neutron, Keystone, and Horizon) in a single Ubuntu Linux appliance and can easily be deployed with an existing VMware vSphere cluster. VOVA is a Ubuntu Linux (64-bit) VM with 1 vCPU, 2 GB of RAM and 40 GB of disk space.   It is available for download from the VMware OpenStack community site.   Important Notes:      The version currently available for download (VOVA 0.2.0) is not what I will be using. I’m using a special version that is not (yet?) publicly available and already includes the OpenStack neutron plugin for VMware NSX.   The network configuration required for VOVA 0.2.0 differs fundamentally from what you will see here, as it uses OpenStack Flat Networking instead of Neutron.   In this setup we will use a very simple physical network setup. All components will attach to a common Mgmt / VM Network. This means that VOVA uses a single interface only. Please do not use such a simple network setup, sharing management and tenant traffic on the same network segment, in a production environment!   After importing the VOVA appliance it will communicate with the NSX controller cluster and the VMware vSphere vCenter (See Figure 1). But it will also house the OpenStack Neutron plugin and thus provide the DHCP server capability to the OpenStack cloud. This will require VOVA to join the NSX overlay network as a transport node. How this is done will also be shown later on in this article. For now, let’s focus on importing and configuring VOVA.                              Figure 1: VMware OpenStack Virtual Appliance (VOVA) in the overal setup     Gather the pre-requisites   VOVA is provided as an Open Virtualization Format image (OVA). During installation one needs to enter the initial configuration parameters, which consists of IP address information for the appliance itself, but also for the services that it interacts with. Before starting the actual OVA import, let’s make sure we have all the information available that we need.   In particular we need:      IP addressing information for VOVA, including default gateway and DNS resolver.   IP address and account credentials of the vSphere vCenter.   IP address and account credentials of the NSX Controller Cluster.   Name of the “Datacenter”, “Clusters” and “Datastores” in vCenter for the cluster that VOVA will manage.   UUID of the NSX Transport Zone and UUID of the Gateway Service.   Within the vSphere Web Client look up the Name of the “Datacenter”, “Clusters” and “Datastores” in vCenter (See Figure 2). If you are only using a single datastore that is accessible from all ESXi servers within the cluster, you do not need to look up its name.                              Figure 2: Datacenter and Cluster name for configuration of VOVA     Next we need to return to the NSX Manager in order to look up the UUID of the NSX Transport Zone and Gateway Service. On the Dashboard click on the registered number for the corresponding entry (See Figure 3).                              Figure 3: Lookup Transport Zone and Gateway Services UUID in the NSX manager     Note down the UUID of the NSX Gateway Service (See Figure 4).                              Figure 4: UUID of the NSX Gateway Service     Next, note down the UUID of the NSX Transport Zone (See Figure 5).                              Figure 5: UUID of the NSX Transport Zone     Ensure that you collected the above information without any errors or mistakes. During the VOVA import you will not be able to verify if the information entered are correct. You will only find out after the VOVA import has completed, when things are not working as expected. Therefore please take your time now and ensure that the gathered information are correct.   installation   Import of VOVA OVA   Next comes the import of the OVA image for the VMware vSphere OpenStack Virtual Appliance. This step does not differ dramatically from what we already know about OVA imports.   Let’s get started by picking the correct file for the import (See Figure 6).                              Figure 6: OVA Import of VOVA - Step 1     Choose the default settings or your site specific settings for the rest of the import. Once you get to the Customize template tab, specify the VMware vSphere vCenter information (See Figure 7). This includes:      IP address and port of the vCenter Server.   Accound credential for the vCenter Server with sufficient privileges.   Name of the “Datacenter” that contains the clusters and name of the clusters that will be managed by OpenStack.   Search pattern for datastores that will be used by OpenStack. Leave this field blank to use all datastores or if you only have a single data store.                              Figure 7: OVA Import of VOVA - Step 2     Specify the OpenStack Neutron plugin for VMware NSX information (See Figure 8). This includes:      One of the IP addresses for the NSX Controller cluster.   Account credentials for the NSX Controller cluster with sufficient privileges.   UUID of the L3 Gateway Service.   UUID of the Transport Zone.                              Figure 8: OVA Import of VOVA - Step 3     Specify the networking properties for the VOVA appliance itself (See Figure 9).                              Figure 9: OVA Import of VOVA - Step 4     Complete the OVA import and boot up the VM. The startup procedure can take a few minutes, after which you can connect with a Web Browser to the IP address or hostname of the VOVA appliance (See Figure 10). Don’t login yet as we are not done yet.                              Figure 10: VMware OpenStack Virtual Appliance (VOVA) login screen     Firewall settings to allow VNC access to VM workloads   OpenStack uses the Virtual Network Computing (VNC) protocol to access the VM workloads from the web interface Horizon. In a VMware vSphere environment this requires that the port range TCP/5900 through TCP/6000 is available for VNC connections on every ESXi host in all the clusters that will be managed by the appliance. This in return requires that these ports are allowed within the ESXi firewall of all ESXi hosts. The easiest way to do this in a non-production is setting is just to abuse the existing firewall profile for gdbserver, since this opens everything VNC needs and more.   Navigate to the ESXi host and select Manage -&gt; Settings -&gt; System -&gt; Security Profile -&gt; Edit (See Figure 11).                              Figure 11: Edit the Security Profile of the ESXi hosts     Ensure the checkbox for the security profile gdbserver is ticked and confirm with OK (See Figure 12).                              Figure 12: Enable the gdbserver security profile to enable VNC access to guests     This completes the import and configuration of the VMware vSphere OpenStack Virtual Appliance (VOVA). While we might be tempted login to VOVA and start playing around, we are not quite there yet. The network setup of VOVA in combination with the OpenStack Neutron plugin for VMware NSX is not yet complete.   Install Open vSwitch on VOVA   Next we need to install the Open vSwitch (OVS) on VOVA. This is necessary as the Neutron plugin of OpenStack serves as the DHCP server on the L2 segments that will be created. As the Neutron plugin resides on VOVA, it needs to have connectivity to the NSX overlay network.   The OVS is not yet installed in VOVA out-of-the-box, as it needs to match the NSX version that is being used. In the future it might be possible that the following steps become largely redundant as VOVA might take the NSX version as a configuration parameter and install the correct corresponding OVS version. Let’s do it manually for now.   Login via SSH to the VOVA host and use the default credentials with the username root and the password vmware.   Ensure that you have the NSX OVS for Ubuntu package available on VOVA that matches your NSX version. For me the file nsx-ovs-2.0.0-build30176-ubuntu_precise_amd64.tar is the correct one.   Next extract the OVS Tar archive into a folder:   root@openstack:/tmp/ovs# ls nsx-ovs-2.0.0-build30176-ubuntu_precise_amd64.tar root@openstack:/tmp/ovs# tar xvzf nsx-ovs-2.0.0-build30176-ubuntu_precise_amd64.tar ./ ./openvswitch-datapath-dkms_2.0.0.30176_all.deb ./nicira-ovs-hypervisor-node_2.0.0.30176_all.deb ./openvswitch-switch_2.0.0.30176_amd64.deb ./nicira-flow-stats-exporter/ ./nicira-flow-stats-exporter/nicira-flow-stats-exporter_4.0.0.28083_amd64.deb ./openvswitch-common_2.0.0.30176_amd64.deb root@openstack:/tmp/ovs#    We will now install all packages within the created folder. The package nicira-flow-stats-exporter in the sub-folder with the same name is not needed here. We will therefore skip it.   Use the command dpkg -i *.deb to install all NSX OVS packages:   root@openstack:/tmp/ovs# dpkg -i *.deb Selecting previously unselected package nicira-ovs-hypervisor-node. (Reading database ... 98439 files and directories currently installed.) Unpacking nicira-ovs-hypervisor-node (from nicira-ovs-hypervisor-node_2.0.0.30176_all.deb) ... Selecting previously unselected package openvswitch-common. Unpacking openvswitch-common (from openvswitch-common_2.0.0.30176_amd64.deb) ... Selecting previously unselected package openvswitch-datapath-dkms. Unpacking openvswitch-datapath-dkms (from openvswitch-datapath-dkms_2.0.0.30176_all.deb) ... Selecting previously unselected package openvswitch-switch. Unpacking openvswitch-switch (from openvswitch-switch_2.0.0.30176_amd64.deb) ... Setting up openvswitch-common (2.0.0.30176) ... Setting up openvswitch-datapath-dkms (2.0.0.30176) ...  Creating symlink /var/lib/dkms/openvswitch/2.0.0.30176/source -&gt;                  /usr/src/openvswitch-2.0.0.30176  DKMS: add completed.  Kernel preparation unnecessary for this kernel.  Skipping...  Building module: cleaning build area....(bad exit status: 2) ./configure --with-linux='/lib/modules/3.5.0-45-generic/build' &amp;&amp; make -C datapath/linux............ cleaning build area....(bad exit status: 2)  DKMS: build completed.  openvswitch: Running module version sanity check.  - Original module  - Installation    - Installing to /lib/modules/3.5.0-45-generic/updates/dkms/  depmod............  DKMS: install completed. Processing triggers for man-db ... Setting up openvswitch-switch (2.0.0.30176) ...  * Inserting openvswitch module  * /etc/openvswitch/conf.db does not exist  * Creating empty database /etc/openvswitch/conf.db  * Starting ovsdb-server  * Configuring Open vSwitch system IDs  * Starting ovs-vswitchd  * Enabling remote OVSDB managers Processing triggers for ureadahead ... ureadahead will be reprofiled on next reboot Setting up nicira-ovs-hypervisor-node (2.0.0.30176) ...  * successfully generated self-signed certificates.  * successfully created the integration bridge.  * Starting ovs-l3d root@openstack:/tmp/ovs#    Similar to the NSX appliances and the ESXi hosts, the next step registers the NSX vSwitch with the NSX controller. First use the command ovs-vsctl set-manager ssl:&lt;IP address of a NSX controller node&gt; to point the NSX vSwitch to the NSX controller. In the case of an NSX controller cluster you can specify any IP address of a cluster member.   root@openstack:/tmp/ovs# ovs-vsctl set-manager ssl:192.168.110.101 root@openstack:/tmp/ovs#    Next extract the SSL certificate from the NSX vSwitch with the command cat /etc/openvswitch/ovsclient-cert.pem.   Copy the text including the line ---BEGIN CERTIFICATE--- and ---END CERTIFICATE--- (See Figure 13). You will need this text in the next step.                              Figure 13: NSX OVS SSL certificate displayed for VOVA     Return to the NSX Manager Dashboard. Within the Summary of Transport Components section, click on Add within the Hypervisor row (See Figure 14).                              Figure 14: Add a new Hypervisor in NSX Manager     What follows now should look familiar from when you added the ESXi hosts as hypervisors to the NSX Manager in the previous post: “OpenStack with vSphere and NSX – Part 3: Install and configure the Open vSwitch inside the ESXi hosts”:      Confirm that the pre-selected transport type is Hypervisor.   Give the gateway node a name. I usually pick the hostname.   As the Integration Bridge Id specify br-int. Leave the other values with the default setting.   Select the Credential Type of Security Certificate and paste the previously copied certificate into the Security Certificate field.   Create a transport connector for the NSX vSwitch using STT as the transport type and the IP address of your VOVA.   Return to the NSX Manager Dashboard, where you will see the new Hypervisor of type Ubuntu within the Hypervisor Software Version Summary section (See Figure 15).                              Figure 15: VOVA successfully added as Hypervisor     Summary   Congratulations! This completes the installation of VOVA and NSX. Now you are ready to go and use your OpenStack cloud leveraging VMware vSphere – along with its well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor as well as network virtualization provided via VMware NSX as a Neutron plugin.   Next in the series on OpenStack with vSphere and NSX is Part 5: “OpenStack with vSphere and NSX – Part 5: Create virtual networks and launch a VM instance in OpenStack” where we will take our new cloud for a quick spin and see what we can do with it, creating virtual networks within OpenStack that consist of L2 segments and interconnecting them via L3 to each other or the outside world. Also we will create our first VM instance via OpenStack.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2014/01/08/openstack-vsphere-nsx-part4/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 5: Create virtual networks and launch a VM instance in OpenStack",
        "excerpt":"Welcome to part 5 of the series: “OpenStack with vSphere and NSX” on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of an OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   In the previous post: “OpenStack with vSphere and NSX – Part 4: Import and configure the VMware vSphere OpenStack Virtual Appliance (VOVA)” we completed the import and configuration of the VMware vSphere OpenStack Virtual Appliance (VOVA). In this article we will take our new cloud for a quick spin and see what we can do with it, creating virtual networks within OpenStack that consist of L2 segments and interconnecting them via L3 to each other or the outside world. Also we will create our first VM instance via OpenStack. In the next post we will then dig a bit deeper and look behind the scenes.   Initial Login to OpenStack   Let’s start with the initial login into OpenStack via VOVA. Point your preferred browser to the IP address or associated DNS name that you gave VOVA and login with the standard credentials. The username is admin and the password is vmware (See Figure 1).                              Figure 1: Login to OpenStack Horizon     After successful login as the user admin you will end up in the admin view of the OpenStack Dashboard Horizon. This is the view that the operator of the OpenStack cloud would see and use (See Figure 2).                              Figure 2: OpenStack Horizon - Admin View     Let’s change over to the project view and see what a tenant would see. While the admin account allows you to see both, a regular tenant would only see the project view and not the admin view (See Figure 3).                              Figure 3: OpenStack Horizon - Project View     Next, let’s have a look at the initial virtual network topology available to us. Click on the Network Topology tab within the Project area (See Figure 4).                              Figure 4: Tenant View - Initially empty network topology     Initially the Network Topology is empty. This means that we do not have any network available to connect Virtual Machines to. As this situation is not very useful to us, let’s start by creating some networks. Here’s what we need at a base level (See Figure 5):      An external (sometimes also called public) network that corresponds to the physical network segment providing us external connectivity.   An internal network per tenant to which we can attach VMs. These per tenant VM can use this internal network to communicate with each other. But also we don’t necessarily want to connect every VM to the outside world.   In order for the VMs, connected to the internal network, to reach the outside world (e.g. Internet) we also need a router providing Source NAT (SNAT) capability between internal and external network.                              Figure 5: Simple Virtual Network for OpenStack     The first component will need to be provided by the cloud operator, while component 2 and 3 are created by the individual tenant. Let’s therefore consciously split these two jobs into their own sections.   Admin view   Creating an external network   As mentioned earlier the cloud operator of the OpenStack cloud will need to create the external network. The admin user that you are currently logged in with has the ability to perform operations as such a cloud operator. But we will later also use it to perform pure tenant operations.   Return to the Admin view, choose the Networks tab and click on Create Network (See Figure 6).                              Figure 6: Admin View - Create new network     Give the new network a useful name such as External and tick the External Network box to designate it as an external network. You need to specify a project when you create a new network. Yet, an external network will be visible from all projects. It therefore doesn’t really matter which project you assign this network to. A good project to pick is the service project, as it is a core part of OpenStack and therefore won’t go away. Finalize the creation by clicking on Create Network (See Figure 7).                              Figure 7: Create External Network     Next click on the network name - here External - to configure additional settings such as the subnet (See Figure 8).                              Figure 8: Change network settings     Within the Network Detail view, click on Create Subnet to associate a subnet with this network (See Figure 9).                              Figure 9: Create new subnet     Give the Subnet a meaningful name. I usually use the same name for the Subnet as the Network. Although keep in mind that you can have multiple subnets per network. Furthermore specify the IP range of your external or public network along with the default gateway. In this simple setup, this is the network that all our components (ESXi hosts, vCenter, NSX and VOVA) connect to.   In a realistic, production oriented network, this would be the subnet for the external network that the NSX gateway connects to.   Click on Subnet Detail to continue.                              Figure 10: Create Subnet - Step 1     Unselect the Enable DHCP checkbox as this is an external network which either already has an existing DHCP service available, or on which you don’t want OpenStack to supply DHCP capabilities. Specify the Allocation pools specific to your environment with an IP range that is not already in use within the selected subnet. Specify the DNS Name Servers and click Create to finalize the creation of the subnet (See Figure 11).                              Figure 11: Create Subnet - Step 2     Verify that the external network has been successfully created and is in the UP state (See Figure 12).                              Figure 12: Successfully created External Network     Tenant’s view   Creating networks   Let’s return to the tenant’s view and see how the previously created external network will look like. Choose the Project view, then click on the Network Topology tab. You can see the external network available to the tenant (See Figure 13).                              Figure 13: Tenant View - Create internal network     Next we will create the internal network. This task will be completed by the tenant within a project. Therefore still within project view, click on the Create Networks button (See Figure 13).   Enter a useful name as the Network Name and click on the Subnet to specify additional information (See Figure 14).                              Figure 14: Create Network - Step 1     Specify a Subnet Name - e.g. the value Internal - along with the Network Address. Click on Subnet Detail to continue (See Figure 15).                              Figure 15: Create Network - Step 2     Enter the value of the DNS Name Server and finish the dialog with a click on Create (See Figure 16).                              Figure 16: Create Network - Step 3     The result is now an external network, which was created and is owned by the cloud administrator and an internal network, which was created and is owned by a project tenant.   But we are not done yet: If we connect a workload to the internal network, it will obviously not have outbound connectivity as internal and external network are not connected. We can fix this by creating a router between the two.   Click on Create Router to get started (See Figure 17).                              Figure 17: Network Topology with Internal and External network     Give the Router a useful Router Name and finish the creation of the router with a click on Create Router (See Figure 18).                              Figure 18: Create Router dialog     Next we need to create the router’s interfaces on the two networks. Click on the router and choose view router details to get started (See Figure 19).                              Figure 19: Network Topology with unconnected router     Now click on Add Interface to add the internal interface first (See Figure 20).                              Figure 20: Router Overview     As the Subnet choose the internal network that you created in an earlier step (See Figure 21). In case you are tempted to think that it doesn’t matter which interface we add first: The interface to the external network is an uplink for this router and is therefore added differently. Click on Add interface to finish your selection.                              Figure 21: Add interface to router     Notice that the result will include an interface with a fixed address from the subnet that is part of the internal network, but also an interface with an IPv4 link local address (See Figure 22). This special interface and address can be used by VMs in OpenStack to query an API in order to learn more about themselves.                              Figure 22: Router with connection to internal network     Next we need to configure the external network as the upstream network. This is done by setting the gateway for the router. For some reason that capability is not available from within the Router Detail view, adding one more step to our setup.   Click on the Routers tab to leave the Router Detail view (See Figure 22).   Under Actions for the router click on Set Gateway (See Figure 23).                              Figure 23: Specify the Router Gateway     As the External Network chose the network that is provided by the cloud operator and confirm the selection by clicking on Set Gateway (See Figure 24).                              Figure 24: Set Router Gateway dialog     Return to the Network Topology view to see the result (See Figure 25).                              Figure 25: Network Topology with Internal Network, External Network and Router between them     Finally our virtual network in OpenStack as shown in Figure 5 has been completed and we are ready to deploy our first virtual machine.   Deploying a Virtual Machine   Now with the network in place, we are finally ready to deploy a first VM in our OpenStack cloud. From the Network Topology tab click on the Launch Instance button (See Figure 25).   Give your new instance a meaningful name as the Instance Name and select an appropriate Flavor - e.g. m1.tiny. As the Instance Boot Source choose Boot from image and select as the Image Name the image debian-2.6.32-i686 (1 GB), which comes packaged with VOVA. Click on the Networking tab to continue (See Figure 26).                              Figure 26: Launch Instance - Step 1     Move the network Internal from the Available networks pool to the Selected networks, by either clicking on the plus icon or using drag-and-drop (See Figure 27).   This connect the internal network that we previously created to the first NIC of the new VM instance. Click on Launch to finalize the creation of your VM instance.                              Figure 27: Launch Instance - Step 2     Wait for the new VM instance to be created and powered up. Once the Status indicates Active and the Power State shows running, you’re all set (See Figure 28). The first deployment of a VM instance can take up to a few minutes, as vSphere needs to import the above selected Debian image from VOVA into its local storage. Subsequent deploys will just use a clone of this imported image and are therefore much faster. Click on the name of the VM to see the Instance Detail.                              Figure 28: Successful Launch of Instance     You can see information about the running VM instance - such as the ID, the status or the IP address. Click on the Console tab to connect to the VM (See Figure 29).                              Figure 29: Instance Details     Via the Console you can access your VM instance. Login with the standard username root and the password vmware to this provided Debian image. Afterwards you can control your VM and e.g. test connectivity by pinging the Google Public DNS Resolver at 8.8.8.8 (See Figure 30).                              Figure 30: Instance Console     Congratulations! You have gotten your feet wet using your OpenStack cloud, implementing your first virtual network and attaching a new VM instance to it. All this while leveraging VMware vSphere as the underlying Hypervisor as well as network virtualization provided via VMware NSX as a Neutron plugin.   Summary   Next in the series: “OpenStack with vSphere and NSX” on OpenStack with vSphere and NSX is Part 6: “OpenStack with vSphere and NSX – Part 6: Install the VMware vCenter Plugin for Openstack and look behind the scenes” where we will look behind the scenes into VMware vSphere to see what’s happening during the operation of OpenStack on vSphere. We will also install the VMware vCenter Plugin for Openstack to gain more insight into OpenStack from vSphere, as well as use some of the well-known enterprise-class benefits of vSphere - such as VMotion – along with OpenStack. Furthermore we will see how to use OpenStack’s legendary APIs to automate the deployment of a VM.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2014/01/24/openstack-vsphere-nsx-part5/",
        "teaser":null},{
        "title": "OpenStack with vSphere and NSX - Part 6: Install the VMware vCenter Plugin for Openstack and look behind the scenes",
        "excerpt":"Welcome to the final part of the series: “OpenStack with vSphere and NSX” on installing OpenStack with VMware vSphere and VMware NSX. This series shows the deployment of an OpenStack cloud that leverages VMware vSphere – along with it’s well-known enterprise-class benefits such as VMotion – as the underlying Hypervisor. In addition, network virtualization within OpenStack will be provided via NSX as a Neutron plugin. This allows the creation of virtual networks within OpenStack that consist of L2 segments and can be interconnected via L3 to each other or the outside world.   In the previous post: “OpenStack with vSphere and NSX – Part 5: Create virtual networks and launch a VM instance in OpenStack” we took our new cloud for a quick spin and saw what we can do with it, creating virtual networks within OpenStack that consist of L2 segments and interconnecting them via L3 to each other or the outside world. Also we created our first VM instance via the OpenStack Horizon Web Interface.   In this post we will dig a bit deeper and look behind the scenes of our Cloud. First we will configure the VMware vCenter Plugin for OpenStack that provides operators using the vSphere Web Client insight into the OpenStack layer running on top. Next we will explore one of the main benefits of merging the enterprise class vSphere platform with OpenStack, by performing a vMotion operation on a VM that was created via OpenStack. Furthermore we will create a floating IP address in OpenStack and show how to connect to our VM with this IP address. Last but least, we will have a quick glance at the API capabilities of OpenStack.   Configure the VMware vCenter Plugin for OpenStack   The idea behind the VMware vCenter Plugin for OpenStack is to provide operators of the virtualization layer insight into the OpenStack layer running on top. This can greatly enhance troubleshooting capabilities, especially for the case that the vSphere virtualization layer and the OpenStack layer are managed by different people or teams.   The VMware vCenter Plugin for OpenStack is part of VOVA and is automatically installed once VOVA is pointed to a vCenter. For some reason it currently doesn’t auto-configure itself, but requires manual intervention in the vSphere Web Client. Let’s therefore perform this manual configuration:   First you need to lookup the Identity API Endpoint information within your OpenStack deployment. Navigate to the Project view and the Access &amp; Security tab. Under the API Access tab lookup the Service Endpoint for the Identity service (See Figure 1).                              Figure 1: OpenStack API Endpoint Information     Within the vSphere Web Client navigate to the Administration tab (See Figure 2).                              Figure 2: vSphere Web Client Administration tab     Next navigate to the OpenStack item in the left-hand navigation tree. Clicking the + button on the top of the grid to start the endpoint configuration (See Figure 3).                              Figure 3: Unconfigured vSphere Web Client Openstack Plugin     Configure the vCenter Endpoint first, providing the URL https://vcenter_ip_or_fqdn/sdk and the required credentials.   Next configure the OpenStack Keystone Endpoint. Click on the + button on the top of the grid, select Keystone and provide the URL - that you have looked up in the previous step - and the required credentials. The URL should look like http://vova_ip_or_fqdn:5000/v2.0. In my case the Endpoint configuration proved to be very finicky and random. I had to repeat these steps multiple time until both endpoint showed as Active with a green checkmark (See Figure 4).                              Figure 4: Fully configured vSphere Web Client Openstack Plugin     Before you can use the vSphere Web Client OpenStack plugin, you need to logout of the vSphere Web Client and log back in.   Once this has been done you can navigate inside the vSphere Web Client to a VMs summary tab. A new portlet named “OpenStack VM” displays the data properties related to OpenStack instances including the Server name, tenant name of flavor (See Figure 5).                              Figure 5: Information provided by the vSphere Web Client OpenStack plugin     Perform a VMotion of an OpenStack VM   One of the most heavily used features in VMware vSphere is probably VMotion. Together with the core concept of server virtualization it allows operators to migrate virtual machines within a cluster off a specific hardware host and perform maintenance on this host. This in return leads to a huge increase of uptime for workloads. We will now demonstrate exactly this capability by moving a VM that was created on vSphere via the OpenStack interface from one host to another host. This would allow us to perform maintenance on the physical host without affecting the workloads presented in OpenStack. Let’s get started.   Within the vSphere Web Client navigate to a VM that was created via OpenStack. Note down the current host that this VM is running on. Perform a right-click and choose Migrate… (See Figure 6).                              Figure 6: Perform a VMotion of an OpenStack VM - Step 1     As the migration method choose Change Host in order to move the VM to another host (See Figure 7).                              Figure 7: Perform a VMotion of an OpenStack VM - Step 2     Select the destination cluster and tick the box for Allow host selection within this cluster in order to pick a specific host (See Figure 8).                              Figure 8: Perform a VMotion of an OpenStack VM - Step 3     Select the specific host to which you want to migrate the VM (See Figure 9). This host should obviously differ from the initial host.                              Figure 9: Perform a VMotion of an OpenStack VM - Step 4     Wait for the VMotion to complete successfully. Note that the VM now resided on a different host (See Figure 10).                              Figure 10: Perform a VMotion of an OpenStack VM - Step 5     You will not notice any difference from within OpenStack about the performed VMotion. In fact, the above operation remains completely invisible and unnoticed from OpenStack, which is exactly what we want.   Configure a floating IP   Next we will configure a floating IP address for a VM in OpenStack. A floating IP address allows you to reach VMs connected to an internal network from the outside or external network. This is accomplished by OpenStack configuring a Destination NAT (DNAT) rule on the L3 gateway between the external and internal networks.   Within the OpenStack Web interface navigate to the Project view and there pick the Instances tab. Select a VM and click on More within the Actions column. Now select Associate Floating IP (See Figure 11).                              Figure 11: Associate a floating IP in OpenStack - Step 1     Initially no floating IP address is available within the project to be used. We therefore need to allocate a new address. For this click on the “+” icon next to the IP address field (See Figure 12).                              Figure 12: Associate a floating IP in OpenStack - Step 2     Now select the pool from which you want to select the floating IP address. In this setup only the External pool, which corresponds to the external network is available. Click on Allocate IP to finish the allocation (See Figure 13).                              Figure 13: Associate a floating IP in OpenStack - Step 3     Now that an IP address from the External pool has been successfully allocated, it can be associated with the internal IP address of the VM. Click on Associate to finish this association (See Figure 14).                              Figure 14: Associate a floating IP in OpenStack - Step 4     The instances view will now display the associated external IP address for the VM besides the internal address (See Figure 15).   In case you would try to connect to this external IP address now, the result would be disappointing: It won’t work. That’s because the default Security Group settings associated with the VM prevent this access. The idea here is to prevent any kind of external access by default and require users to explicitly grant specific access. Therefore we need to e.g. allow SSH access to the VM, before we can fire up Putty and connect.                              Figure 15: Successfully associate a floating IP in OpenStack     Navigate to the Access &amp; Security tab and select Security Groups. You will find the security group default, which is the only security group currently in use. Click on Edit Rules to start editing (See Figure 16).                              Figure 16: Edit Security Groups in OpenStack - Step 1     Next click on Add Rules to add a rule for SSH (See Figure 17).                              Figure 17: Edit Security Groups in OpenStack - Step 2     As Rule select SSH. For Remote select CIDR and for CIDR enter 0.0.0.0/0. This will allow SSH from anywhere. Click on Add to finalize the wizard. (See Figure 18).                              Figure 18: Edit Security Groups in OpenStack - Step 3     Notice the new rule for SSH, which indicates the correct TCP port 22 for SSH (See Figure 19).                              Figure 19: Edit Security Groups in OpenStack - Step 4     Now you can use Putty or any other SSH client and successfully connect to the external floating IP address of the VM (See Figure 20).                              Figure 20: Connect via SSH to the floating IP address     API-driven creation of a VM in OpenStack   One of the big benefits of OpenStack is the simple, yet very powerful API along with various SDK for all kinds of programming languages. Let’s use the Python SDK for creating a new VM via the OpenStack API.   Below is a simple Python script, which will connect to your OpenStack cloud, create a new VM and start it.   import novaclient.v1_1.client as nclient import time creds = {\"username\":\"admin\",          \"api_key\":\"VMware1!\",          \"project_id\":\"admin\",          \"auth_url\":\" http://localhost:5000/v2.0/ \" } print \"Booting a debian VM from python...\" nova = nclient.Client(**creds) print nova.images.list() image = nova.images.find(name=\"debian-2.6.32-i686\") print nova.flavors.list() flavor = nova.flavors.find(name=\"m1.tiny\") print nova.networks.list() network = nova.networks.find(label=\"Internal_Shared\") instance = nova.servers.create(name=\"created-from-python\",image=image, flavor=flavor, nics=[{'net-id': network.id}]) while instance.status != 'ACTIVE':         print \"Waiting for VM... (current status '%s')\" % instance.status         time.sleep(5)         instance = nova.servers.get(instance.id) print \"VM booted to status '%s'\" % instance.status   The elements that you need to adapt to your own environment are:      username: Your OpenStack username   api_key: Your OpenStack password   project_id: The name of the project in which the VM should be created.   auth_url: Change it to the IP address of your local OpenStack cloud.   image: The name of the available image you want to use.   flavor: The flavor of the VM you want to use.   network: The name of the network to which this VM should get connected to.   You can run this script directly from VOVA. As VOVA is based on Ubuntu you can use the command sudo apt-get install python-novaclient to install the required SDK.   Wrap-Up   Congratulation! We successfully looked behind the scenes into VMware vSphere to see what’s happening during the operation of OpenStack on vSphere. We also installed the VMware vCenter Plugin for Openstack to gain more insight into OpenStack from vSphere, as well as used some of the well-known enterprise-class benefits of vSphere – such as VMotion – along with OpenStack. Furthermore we took a glimpse at how to use OpenStack’s legendary APIs to automate the deployment of a VM. This completes the series: “OpenStack with vSphere and NSX” on OpenStack with vSphere and NSX.   If you are at VMware Partner Exchange (PEX) from February 10-13 2014, head over to the Hands-On Labs and check out the lab “HOL-SDC-1320 - OpenStack on VMware vSphere” in order to get hands-on experience with the setup described in this setup. After PEX this lab will also become available for general usage within the VMware Hands-On labs.   Also if you want to learn more about using OpenStack with VMware vSphere, have a look at the VMware whitepaper “Getting Started with OpenStack and VMware vSphere”.  ","categories": ["EdgeCloud"],
        "tags": ["Network","NSX","OpenStack","VMware"],
        "url": "https://www.edge-cloud.net/2014/02/08/openstack-vsphere-nsx-part-6/",
        "teaser":null},{
        "title": "Deploy WordPress with AWS OpsWorks",
        "excerpt":"AWS OpsWorks is an application management service that makes it easy for DevOps users to model and manage the entire application from load balancers to databases. It offers a very powerful solution for users to deploy their application easily in AWS without giving up control.   In this post I want to show you how easy it is to use AWS OpsWorks for deploying WordPress - a typical LAMP application. This includes deploying a fresh blank WordPress install as well as re-creating a WordPress site from a backup for Dev/Test or Disaster Recovery purposes. In all cases it should take you only a few minutes to have a running WordPress site.   While I use a Webhoster for running Edge Cloud, I do use the described approach to test changes to WordPress before deploying them into production. And I thereby also know that I could restore Edge Cloud as part of a Disaster Recovery (DR) plan via this approach.   About Opsworks   With the availability of AWS OpsWorks Amazon Web Services now has a number of different Application Management Services that address the different needs of Administrators and Developers (See Figure 1).                              Figure 1: OpsWorks fit in the AWS Application Management Solutions        AWS Elastic Beanstalk is an easy-to-use solution for building web apps and web services with popular application containers such as Java, PHP, Python, Ruby and .NET. You upload your code and Elastic Beanstalk automatically does the rest. Elastic Beanstalk supports the most common web architectures, application containers, and frameworks.   AWS OpsWorks is a powerful end-to-end solution that gives you an easy way to manage applications of nearly any scale and complexity without sacrificing control. You model, customize, and automate the entire application throughout its lifecycle. OpsWorks provides integrated experiences for IT administrators and ops-minded developers who want a high degree of productivity and control over operations.   AWS CloudFormation is a building block service that enables customers to provision and manage almost any AWS resource via a domain specific language. You define JSON templates and use them to provision and manage AWS resources, operating systems and application code. CloudFormation focuses on providing foundational capabilities for the full breadth of AWS, without prescribing a particular model for development and operations.   Use case   The goal for this post will be to use AWS OpsWorks to deploy a WordPress site, automating as much of the deployment as possible. We will include two distinct use cases:      Fresh blank site: Create a fresh blank WordPress site that you can use to start hosting your own blog.   Re-Created site from an existing backup: If you have an existing WordPress site, you can use a WordPress Backup Plugin, such as BackWPup. Using AWS OpsWorks will allow you to recover your WordPress site from a backup as a Dev/Test site. This way you can e.g. test installing a new plugin before doing so in the production site. But you can also use the recovered WordPress site for a disaster recovery scenario in case your primary site is hard down. The benefit here is that the recovery can happen almost entirely automated, which is always a good idea for a DR solution.   Even though we will run the WordPress site in AWS, we will not make use of AWS cloud concepts in order to achieve high availability for coping with AWS failures. Instead we will keep it very simple (See Figure 2).                              Figure 2: Architecture for a simple WordPress deployment in AWS     The deployed architecture will include:      WordPress PHP App: An EC2 server running Ubuntu Linux with an Apache webserver to host the WordPress PHP application. End-users will be able to access the site via an Elastic IP, which guarantees that the IP address - or an associated DNS entry - will remain in place, even if the environment is rebuild.   MySQL Server: An EC2 server running Ubuntu Linux with MySQL. MySQL will host the database for the WordPress application.   Existing Backup (Optional): A full backup of an existing WordPress site as a Zip file in a S3 bucket.   WordPress source code (Optional): The WordPress source code - available at www.wordpress.org for a fresh install of WordPress.   Getting Started with AWS OpsWorks   Let’s head over to the AWS OpsWorks console at console.aws.amazon.com/opsworks to get started. Login with your existing AWS credentials. AWS OpsWorks itself is a global service and you therefore do not need to pick a region at this point.   Creating a service or application in AWS OpsWorks includes 4 steps (See Figure 3):      Add a stack: Define a stack for an application which includes information about e.g. the AWS region. You can have multiple stacks in various regions.   Add layers: Each stack consist of one or more layers, with each layer having a certain function. Here we will use a PHP App Server layer and a MySQL database layer.   Add an app: Define the application to be run via a source code repository or file bundle. This includes the ability to use Github and Subversion, a simple Zip file via a HTTP or HTTPS URL or a ZIP file in a S3 bucket.   Deploy and manage: Deploy the application by starting the layer’s instances. Manage further capabilities such as deploying another application version at runtime.                              Figure 3: AWS OpsWorks Deployment Steps     Add a stack   First we will deploy a new stack, which will correspond to the WordPress application. AWS Opsworks also lets you clone an existing stack, which provides capabilities for additional interesting use cases.   Within the AWS OpsWorks console click on Add Your First Stack (See Figure 4).                              Figure 4: Add your first stack     Next configure the basic information of your new stack. Give it a useful Name such as WordPress and select your Region. Select your preferred Default Operating System. Personally I prefer Ubuntu over Amazon Linux. If you want to login to the created EC2 instances via SSH, make sure to select a valid Default SSH key. All other settings you can leave as is for now. Click on the Advanced link at the bottom of the page for further configuration options.                              Figure 5: Add Stack - Step 1     AWS OpsWorks uses the configuration management tool Chef to configure the EC2 instances within each layer. For this it provides so-called Chef “recipes” that describe how server applications (such as Apache or MySQL) are managed and how they are to be configured. These recipes describe a series of resources that should be in a particular state: packages that should be installed, services that should be running, or files that should be written.   While AWS OpsWorks provides many useful recipes out of the box we want to add a few minor custom recipes. In particular we will use two custom recipes which can be found at https://github.com/chriselsen/opsworks-cookbooks:      AWS-Ubuntu: Configure an AWS Opsworks Ubuntu image with a swap space. This is aimed at t1.micro instances to prevent “out of memory” issues.   WordPress: Configure WordPress via the wp-config.php file to interact with the MySQL server. It can be used for a fresh install or a restore from a Backup using BackWPup. The wp-config.php will be filled with the IP address and credentials to access the MySQL server.   You don’t need to understand or even recreate these recipes. I have provided them in a form that allows you to directly use them yourself.   Within the Configuration Management section make sure the selected Chef version is 11.4 and that Use custom Chef cookbooks is selected with Yes. Specify the Repository type with Git and the Repository URL with https://github.com/chriselsen/opsworks-cookbooks.git (See Figure 6).                              Figure 6: Add Stack - Step 2     Next we want to perform some tuning for the components as we will be using the t1.micro instance later on.   The two things to tune will be:      WWW Document Root permission: We want to change the www document root permission to the default user www-data under Ubuntu.   Apache Prefork and Keepalive tuning: As we will be using the memory constraint EC2 flavor t1.micro, we want to make changes to the Apache Prefork and Keepalive settings to better adapt to this flavor type.   In AWS Opsworks the setup and configuration of Apache is performed by Chef recipes that use various parameters which can be controlled by the user via a simple JSON file. This way we don’t have to create a custom Chef recipe or even manually perform changes of our servers. Instead we can look up the apache2 attributes which are configurable and create a custom JSON file.   This JSON file will look as follows:   {   \"opsworks\" : {     \"deploy_user\" : {       \"user\" : \"www-data\"     }   },    \"apache\" : {     \"timeout\" : 40,     \"keepalive\" : \"On\",     \"keepaliverequests\" : 200,     \"keepalivetimeout\" : 2,     \"prefork\" : {       \"startservers\" : 3,       \"minspareservers\" : 3,       \"maxspareservers\" : 10,       \"serverlimit\" : 32,       \"maxclients\" : 32,       \"maxrequestsperchild\" : 2000     }   } }   Paste above Chef JSON code into the Custom Chef JSON field. Then click on Add Stack to finalize the creation of the stack (See Figure 7).                              Figure 7: Add Stack - Step 3     This completes the creation of the stack.   Add Layers   Next is the creation of the application stack layers. One for the PHP App Server layer and one for the MySQL database layer. Let’s start with the PHP App Server layer.   After you finished creating the Stack you’ll end up on the Stack tab. There under the Add your first layer section click on Add a layer (See Figure 8).                              Figure 8: Add Layer - Step 1     As the Layer type select PHP App Server and click on Add Layer (See Figure 9).                              Figure 9: Add Layer - Step 2     You will see your first layer successfully created. Click on + Layer to create the next layer (See Figure 10).                              Figure 10: Add Layer - Step 3     As the Layer type select MySQL. If you later want to manually connect to the MySQL server - e.g. for troubleshooting - note down the automatically created MySQL root password. Next click on Add Layer (See Figure 11).                              Figure 11: Add Layer - Step 4     We are almost done with the layers. We only need to perform some minor changes on the PHP App Server layer. Therefore in the PHP App Server layer row under Actions click on Edit (See Figure 12).                              Figure 12: Edit PHP Layer - Step 1     Although we have already pointed AWS OpsWorks to our custom Chef cookbooks, we still need to assign the individual recipes to the correct layer and lifecycle event. In AWS OpsWorks a layer has a sequence of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer’s instance, AWS OpsWorks automatically runs the appropriate set of recipes.   For the PHP App Server layer we need to define the following two recipes to lifecycle events (See Figure 13).      Setup lifecycle event: Recipe aws-ubuntu::setup   Configure lifecycle event: Recipe wordpress::configure                              Figure 13: Edit PHP Layer - Step 2     Next scroll down to the Automatically Assign IP Addresses section and set Elastic IP addresses to Yes (See Figure 14).   This assigns a so-called elastic IP address to the PHP App Server EC2 instance, an IP address that remains available even if the concrete EC instance is later replaced. It therefore allows you to place a DNS record on this Elastic IP address.                              Figure 14: Edit PHP Layer - Step 3     Scroll down to the Auto Healing section and make sure that Auto healing enabled is set to No (See Figure 15). As we will be using EC2 t1.micro instances, these instances can generate a very high load and/or memory usage - especially at boot time. With auto healing enabled it is possible that AWS OpsWorks interprets this as an issue and attempts to rectify it by recreating the corresponding EC2 instance. Therefore with this simple setup it’s safer to leave this turned off.                              Figure 15: Edit PHP Layer - Step 4     Next we need to create EC2 instances, one instance per layer in our case. Start with the PHP App Server layer and select Add an instance (See Figure 16).                              Figure 16: Add an instance - Step 1     As the Size select t1.micro and click on Add instance (See Figure 17).                              Figure 17: Add an instance - Step 2     Now for the MySQL Server layer select Add an instance (See Figure 18).                              Figure 18: Add an instance - Step 3     As the Size select t1.micro and select the same or a different Availability Zone depending on your preference. Click on Add instance (See Figure 19).                              Figure 19: Add an instance - Step 4     Notice that each layer now has one EC2 instance associated. At this point both instances are still in the Stopped state (See Figure 20). Before we can power them on, we need to define the actual application that is being run.                              Figure 20: Successfully created instances     Add an App   Next we need to tell AWS OpsWorks which application to run on our stack. This is done by pointing to a source code repository or file bundle. Supported repositories include Github and Subversion. The bundles can be a simple Zip file via a HTTP or HTTPS URL or a ZIP file in a S3 bucket.   Navigate to the Apps tab and click on Add an app (See Figure 21).                              Figure 21: Add an App - Step 1     For the case of installing a fresh blank WordPress install you can therefore choose the Repository type to be Http archive and use for the Repository URL the URL http://wordpress.org/latest.zip (See Figure 22). This will allow you to install the latest version of WordPress.   In case you want to restore a backup from the WordPress Backup Plugin BackWPup, you can use Amazon S3 to store the backup files. This will allow you to directly specify this backup file in S3 here. When using BackWPup include the MySQL database as a Zip file inside the Backup file. The Chef recipe for WordPress mentioned above will then automatically import the database dump file back into MySQL.                              Figure 22: Add an App - Step 2     This completes the configuration of the app, but also the layers and stack itself. Your new WordPress application is now ready for deployment.   Deploy your stack   So far we have not created any EC2 instance yet, but only defined Meta data in AWS OpsWorks. In order to actually use the WordPress application, we need to deploy it by starting the layer instances.   Return to the Instances tab and click on Start All Instances (See Figure 23). This will create, boot and configure all instances in all layers. It can take up to 10 minutes for the operation to complete. Therefore please be patient.                              Figure 23: Start all instances     Once the instances are fully deployed, you’ll see the instances to move from the stopped state to the online state. Look up the IP address for the instance in the PHP App Server layer. This IP address should be marked with EIP for Elastic IP (See Figure 24). This is the IP address under which your WordPress installation will be available.                              Figure 24: All instances are active     Connect with your favorite Web Browser to the IP address that you retrieved in the previous step. Or create a DNS name and connect to the DNS name. You will see your WordPress ready to be used (See Figure 25).   In case of a fresh blank install you start with the basic configuration of the site via the WordPress web interface. If on the other side you re-created the site from a backup, you should find the site running as it was during the backup run.                              Figure 25: Fully installed WordPress site     This completes this guide on using AWS OpsWorks to deploy a fresh blank WordPress install or to recover as WordPress site as part of a Dev/Test or Disaster Recovery (DR) use case.   Shutdown and Redeploy   You can shutdown instances and restart them if you want to take a break in your work and keep working where you left off.   But in the case you want to keep the stack for disaster recovery purposes, you want the instances to be rebuild with the latest version of your application and database during the recovery. Therefore you will want to discard the EC2 instances while keeping the Elastic IP address.   Therefore first power down all layer instances. Then delete the PHP App Server instance. Make sure to untick the box for Delete Instance’s Elastic IP for the PHP App Server instance (See Figure 26). This way you can reuse the Elastic IP address when you spin up new instances again.                              Figure 26: Keep the Elastic IP when deleting a PHP App Server instance     Next delete the MySQL instance. Here select Delete instance’s EBS volumes to delete the current database content (See Figure 27). The database content will be restored as part of the restore process from the backup in the S3 bucket.                              Figure 27: Delete the instances EBS volume for the MySQL server     Now recreate the instances as described above.   References   If you want to find out more about AWS OpsWorks, have a look at the AWS re:Invent 2013 presentation Zero to Sixty: AWS OpsWorks (DMG202) or the excellent AWS OpsWorks Documentation.  ","categories": ["EdgeCloud"],
        "tags": ["AWS"],
        "url": "https://www.edge-cloud.net/2014/02/14/deploy-wordpress-aws-opsworks/",
        "teaser":null},{
        "title": "Architecture Design: vSphere with IPv6",
        "excerpt":"In this post I will present an architecture design recommendation for using VMware vSphere 5.x with IPv6. Instead of taking an “all has to be IPv6” approach, we will look at the use case for IPv6 along with the enterprise hypervisor platform VMware vSphere and create a recommended design.   Afterwards we will shed some light on whether it is currently possible to implement such design with VMware vSphere 5.x, or not and what could be interim steps.   Motivation   I have come across various cases where customers want to use IPv6 with VMware vSphere. Unfortunately the question about the desired use case for IPv6, or the question about what components or features of vSphere should be used with IPv6 is often answered with: “All of it!”. Such an approach is unrealistic at this point and will most likely not lead to any success in using IPv6 with vSphere. As sad as it is, but IPv6 is not yet en par with IPv4 when it comes to product support. Most vendors - and VMware is one of them - lag behind IPv4 with their IPv6 support today. Instead let’s shed some light on the reasons of why one would want to use VMware vSphere with IPv6 and how to design - potentially in a phased approach - an architecture design to fulfill the use case needs.   Use Cases   Before we dive into the use cases for IPv6 with VMware vSphere, let’s ensure we understand that VMware vSphere as a platform has a very well defined purpose: Provide the ability to host virtual machines (VMs) - servers and desktops - and offer them necessary resources (compute, storage and network) from a shared pool.   With this we will quickly arrive at our first and most important use case:   Use Case 1: Providing IPv6 access to guest VMs / tenants   Virtual machines running on a VMware vSphere environment are typically “owned” by an internal or external stakeholder and serve a well defined purpose. This can reach from hosting an application on one or multiple server to providing a remote desktop. These VMs also typically require some kind of network access as part of their duty. Here the requirement for IPv6 is driven by the owner of the VM - potentially a customer of our virtualization platform - who wants to or needs to leverage IPv6 for the specific use case of the application. It is also possible that despite any current demand, we need to proactively provide IPv6 connectivity for VMs, so that application owners can plan ahead and incorporate IPv6 capabilities into their offerings.   The second use case is derived from the fact the actual platform to host virtual machines also needs to be managed and maintained. This task requires the platform components to be accessible via a network.   With this we arrive at our second use case:   Use Case 2: Using IPv6 for management to save IPv4 addresses for production usage   The idea here is that IPv4 addresses - whether public or private (RFC1918) - are treated as a precious commodity, thus reserving it for where it is really badly needed. One such place could be the customer or tenant networks that host (public facing) applications. All other non essential networks with only a support function - such as internal management - should be freed up from the precious commodity and use IPv6 instead. Such an approach might eventually led to the concept of a IPv6-only data center.   vSphere Cluster   Before we dive into the individual design components let’s recall how a vSphere Cluster is typically designed when it comes to a network-centric perspective. Such a cluster usually includes up to 5 groups of network types - some internal only to the cluster, some leaving the cluster (See Figure 1):                              Figure 1 : Typical Architecture Design for a vSphere 5.x Cluster        Management Network: This network is used for the administrative staff to interact with the vSphere cluster, e.g. connecting to the vSphere Web Client from a browser. But it is also used for communication with other corporate management systems, ranging from time synchronization via NTP to central log management via Syslog.   VM / Tenant Network: This network is used for workload VMs residing on the cluster to connect to the outside world. It is very well possible that multiple such networks exist, e.g. in the case where multiple tenants share a vSphere cluster and each tenant shall connect to a dedicated network.   VXLAN Transport Network: This network is used to transport the VXLAN overlay traffic, also often called the outer VXLAN traffic. If VXLAN are to span across clusters, this transport network also needs to span across clusters. Unless the controller-based VXLAN implementation is chosen, this network requires to be enabled for transporting Multicast traffic.   Storage Network: In the case that a network based storage solution - such as NFS or iSCSI - is used, this network carries the storage traffic between a central storage array and the ESXi servers within a vSphere cluster. All major network vendors recommend that this network doesn’t extend beyond a L2 boundary and is therefore not routed. The storage network often remains local to the vSphere cluster only, with the storage array assigned to the specific cluster. Also there might be multiple storage networks per cluster, depending on how many storage arrays are used.   VMotion / FT Network: This network is used for VMotion and/or Fault Tolerance (FT) traffic between the ESXi hosts. It is highly recommended that this network also doesn’t extend beyond a L2 boundary and is therefore not routed. Instead it is local to a vSphere cluster only.   Design Components   Now we will have a closer look at our use cases again with the above typical vSphere Cluster setup in mind.   Phase 1   Let’s start with the use case of providing IPv6 access to guest VMs / tenants, being the more crucial one and therefore the one to tackle first.   Here a simple network connectivity for a VM will include multiple elements, which all need to be enabled to support carrying and/or processing both IPv4 and IPv6 traffic (See Figure 2).   The following capabilities in each of these design elements are necessary to support the use case.                              Figure 2: Simple network connectivity for VMs        Workload: The workload itself needs to support IPv4 and/or IPv6 and needs to be configured for it. But also the Virtualization platform needs to support IPv6 besides IPv4 in the VM, e.g. as part of the OS guest customization or by ensuring the DHCP Unique Identifier (DUID) is either maintained or regenerated during a cloning operation - similar to the MAC address. Windows had some problems re-generating the DUID when setting up a new machine via Sysprep.   vNIC: The vNIC connects the workload to the virtual switch. In order to achieve the same performance with IPv6 traffic as with IPv4 traffic, the vNIC should not only support IPv4 as part of the TCP offload engine, but also IPv6. VMware vSphere has received numerous performance improvements in version vSphere 5.5. The vNIC might also be the enforcement point for a network security policy. Such a policy also needs to support IPv6.   Virtual Switch: As a layer 2 device the virtual switch should not have any difficulty transporting IPv6 packets. But this is not enough. As the vSwitch acts as the access layer switch for the VM workloads it also needs to support common First Hop Security Mechanism such as IPv6 RA Guard and IPv6 ND Inspection. With a virtualization platform in the picture it is not sufficient to implement these features in the physical switch, as this switch is usually not aware of the individual workloads but only the Hypervisor host. And enforcing security at this level is clearly not sufficient.   Physical Switch: The physical switch will connect the individual Hypervisor hosts with each other and the upstream router. With regards to IPv6 this device usually does not need to provide any special capabilities.   Layer 3 Gateway / Router: The Layer 3 Gateway - also known as the router - is the boundary of the local layer 2 and provides the ability to reach other hosts via L3 - also known as routing. This gateway can either be a physical device or a virtual appliance, such as the VMware vShield Edge. As such, this device needs to support the L3 protocol of IPv6. It needs to be able to configure interfaces with IPv6 addresses and support static and dynamic routing of the IPv6 protocol. For IPv6 address management it needs to support the functionality of acting as a DHCPv6 relay and/or provide DHCPv6 functionality itself. Support for Stateless Address Auto Configuration (SLLAC) is also crucial within this device.   A slight modification of the above setup comes into play when overlay networks - such as VXLAN - are used. In this case the communication between two workload VMs might traverse the overlay tunnel as an encapsulated packet (See Figure 3). While the tunnel transport protocol - the outer encapsulated packets - does not need to use IPv6, it does need to provide the ability to transport IPv6 traffic. In the case of VXLAN this is possible as VXLAN can transport any kind of L3 protocol including IPv6. But one needs to keep RFC 2460 in mind, which asks for a minimum MTU of 1280 bytes for an IPv6 path. Along with a IPv6 header usually being 20 bytes larger than the IPv4 header, VXLAN must be configured to transport this increased payload size. This is usually the case as the MTU settings for VXLAN default to 1600.                              Figure 3: Advanced network connectivity for VMs     Phase 2   Next we will look at the use case of using IPv6 for management to save IPv4 addresses for production usage. In this case all management traffic leaving the vSphere cluster needs to be IPv6, while all traffic remaining within the cluster can remain IPv4. The reason for not having to move everything to IPv6 is that traffic such as VMotion will not only stay within the vSphere cluster but is anyways not routed beyond the cluster boundaries. It is therefore no problem to re-use the same IP address range for e.g. VMotion in all clusters or even re-use the same IP range outside of the VMware vSphere setup for completely different purposes.   The networks that do not leave the cluster and can therefore remain operating solely with IPv4 are for the VMotion / FT Network as well as the storage network.   On the other hand the network that need to carry IPv6 traffic - as they connect outside the cluster - are the VM / Tenant Network - as discussed already, the Management Network and the VXLAN Transport Network.   Over the Management Network all functionality that is required for managing the cluster needs to support IPv6. What this means in particular depends on the existing design of the environment:      vCenter server: All communication with the vCenter Server needs to be able to use IPv6. This e.g. includes using the vSphere Web Client interface, but also communication with outside components such as the database used for vCenter or Active Directory for authentication and authorization. Last but not least, the communication between the vCenter server and the ESXi hosts also needs to support IPv6.   ESXi hosts: Any management functionality where the vSphere cluster integrates with third party components needs to support IPv6. This includes integration with network management tools via SNMP, central log aggregation via Syslog or time synchronization via NTP.   The VXLAN Transport Network usually not only needs to span an overlay network within the same cluster, but also across a cluster (See Figure 4). For this it is necessary that the VTEPs support IPv6 and the VXLAN Transport traffic can use IPv6.                              Figure 4: VXLAN transport traffic between vSphere cluster     Final Architecture   The final proposed architecture would cover both presented use cases - potentially in a phased approach - and look as follows (See Figure 5):   Phase 1:      VM / Tenant Network: Support both IPv4 and IPv6 in a Dualstack setup to give tenants and individual workloads the maximum choice.   Phase 2:      Management Network: Support for handling all management traffic between vSphere cluster and external systems via IPv6-only.   VXLAN Transport Network: Support for IPv6 only to create an overlay network across multiple vSphere cluster.   Other:      Storage Network: This traffic can remain IPv4 as it is unrouted and will never leave the vSphere cluster.   VMotion / FT Network: This traffic can also remain IPv4 as it is unrouted and will never leave the vSphere cluster.                              Figure 5 : Architecture Design for vSphere 5.x with IPv6 - Protocol mapping     Reality   Now that we have a solid design for a vSphere setup with IPv6, let’s have a look if this can actually be implemented today:   Phase 1:      VM / Tenant Network: This is mostly working and supported today. Small limitations do exist around Guest Customization via Sysprep. You can also expect to stumble over one or two bugs here. But the largest limitation exist in the virtual switch. Neither the VMware standard nor the distributed vSwitch support IPv6 first hop security capabilities. At least the vCloud Networking and Security Edge supports basic IPv6 features with the ability to manually configure its interfaces for IPv6, configure static routing in IPv6 and use the Firewall and Load Balancer feature with IPv6. And also the vCloud Networking and Security App Firewall supports IPv6. But you have to be careful here, because all of this IPv6 doesn’t come with vCNS - which is part of the vCloud Suite - but only with NSX for vSphere - which has to be licensed separately.   Phase 2:      Management Network: Here your mileage will vary depending on the components you’re using and the features you are attempting to use: While a Windows-based vCenter does support IPv6, the vCenter Server Appliance does not. The mandatory use of vSphere SSO will limit you further, as SSO does not work on a IPv6-only host. While the Windows version of the vSphere client supports IPv6, the Web Client does not. Also while ESXi can export Syslog data via IPv6, vSphere can not ingest them via the Syslog and Dump collector. At least basic network protocols such as NTP and SNMP for the ESXi hosts work over IPv6.            VXLAN Transport Network: The original VXLAN Internet-Draft addressed only IPv4 and dealt with IPv6 only via the sentence “Use of VXLAN with IPv6 transport will be addressed in a future version of this draft”. Needless to say that this is pretty sad for a protocol proposal published in August 2012. Luckily that future version came in February 2013. But so far VXLAN over IPv6 has not been implemented by any of the major vendors.           Other:      Storage Network: While we stated that this traffic can remain IPv4-only, it is possible to use both iSCSI (at least via the software initiator) as well as NFS via IPv6. Unfortunately doing so is considered experimental or not even supported depending on your vSphere version. Unfortunately this also highlights the dilemma that some features might appear to “work” with IPv6, but their full breadth hasn’t been tested by VMware’s engineering organization, which in return means that the feature is not officially supported by VMware GSS   VMotion / FT Network: Here we also stated that this traffic can remain IPv4-only. But also here it is possible to use at least VMotion over a IPv6-only network and it even appears to be supported.   Summary   Today it is unfortunately not possible to implement the full IPv6 for vSphere architecture design above. Only parts of it can be implemented with a greatly reduced functionality set. If IPv6 with vSphere is for you greatly depends on your use case and what functionality you are willing to give up.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","IPv6","VMware"],
        "url": "https://www.edge-cloud.net/2014/03/11/architecture-design-vsphere-ipv6/",
        "teaser":null},{
        "title": "Monitoring IPv6 websites and SaaS apps via ThousandEyes",
        "excerpt":"Recently I stumbled over a small startup in San Francisco called ThousandEyes. They provide a service for complete, end-to-end visibility of cloud applications and infrastructure telling you where and why services are breaking or under-performing. This allows SaaS providers to ensure that their application can be leveraged by customers at various locations throughout the world with an optimal or at least acceptable performance. But it also allows Enterprise customers to ensure that the SaaS applications used by the company are available from all campuses and branch offices at the desired performance, taking into consideration all elements of the delivery chain (See Figure 1).                              Figure 1: End-to-end visibility of cloud applications and infrastructure telling you where and why services are breaking     Getting started   Deploying Agents   ThousandEyes leverages so-called agents that probe a target via various tests which can be individually defined. These agents can either be deployed at the network edge or even better deep inside the network, close to the actual end-user of the monitored service. Agents are offered as either a virtual appliance or as a Linux package for various distributions (See Figure 2).                              Figure 2: Setting up local monitoring agents     While it’s great to see that ThousandEyes supports IPv6 at all, there are some limitations. On an IPv4/IPv6 dualstack host you unfortunately have to choose between running the agent either against an IPv4 or IPv6 address. Right now it is not possible to chose both at the same time. It would make more sense if this was possible along with specifying via the test which protocol to use. This way ThousandEyes doesn’t provide the capability to easily compare IPv4 vs. IPv6 traffic out of the box.   Instead I had to simulate such a setup by using two agents, one for IPv4 and one for IPv6. Adding more agents would break this model and makes things hard to read and understand.   Also it is currently not possible to install more than one agent on a Linux host requiring separate hosts for the IPv4 and IPv6 agents. But thanks to Docker I was able to deploy one agent against the IPv6 address and one agent inside a Docker container against an IPv4 address on the same host. The two agents that you therefore see in Figure 2 are actually both running on the same host. With this I could go on with my initially intended use case.   I’m confident that these minor shortcomings will be fixed over time as more and more Enterprise and SaaS provider customers adopt this tool.   Defining Tests   You can define four kinds of tests within ThousandEyes. Depending on which test you select, sub-tests are either automatically created or can be created on demand. The example in Figure 3 shows a “HTTP Server Only” test which probes the URL of this blog site with a given interval. This test will provide you with web site specific metrics around Availability, Response Time and Fetch Time. More advanced Web tests are possible, but I will not cover them here.   With the tick box “Enable network measurements” you activate a so-called network test against the same hostname and port that gives insight into end-to-end metrics like Latency, Packet Loss or Jitter. Also tests for providing information around Bandwidth and MTU - important for discovering problematic tunnels - can be added. A network test can also be configured manually.   A network test will also automatically create a BGP test against the IPv4 and/or IPv6 prefix that the hostname resolves to. More about this later on.   Unfortunately, it is currently not very intuitive which tests are created and what hierarchy exists between them while you create them. Your best indicator is to look at the Alert Rules field. For each kind of test that will be created, a separate Alert Rule is created. In this example you see the three rules corresponding to the three tests I just mentioned.                              Figure 3: Setting up tests against applications and websites     Dashboard   Once you’ve setup the agents and tests, you can move to the Dashboard and wait for some initial data to come in. The Dashboard provides you key metric information about the tests you defined. In the case of a “HTTP Server Only” test you will see the average Availability and Response time across all agents (See Figure 4). Clicking on one of the tests will then provide you additional deeper information.   The Dashboard will also show you alarms and the status of your agents.                              Figure 4: Keeping an eye on important metrics     Web-HTTP Server Test   At the Web-HTTP Server level for a test we can see additional information around Availability, Response Time and Fetch time for the configured URL. These are nicely visible across a select-able time axis as well as across the different select-able agents.   In this example we notice that the Connect Time for the same URL differs quite dramatically between a connection over IPv4 and IPv6. The Connect Time over IPv4 being 5 ms and the Connect Time over IPv6 being more than twice of that with 13 ms (See Figure 5).                              Figure 5: Website performing better over IPv4 than IPv6     Let’s try to use ThousandEyes and figure out why our users will get a worse experience of our sample website via IPv6 than they would get over IPv4.   For this we drill down into the next layer of information, into the End-to-End metrics via the “Jump to…” function (See Figure 6).                              Figure 6: Drill down via Jump to…     Network End-to-End Metrics   Within the Network End-to-End Metrics we will find information about Loss, Latency, Jitter and Bandwidth of the connection. Again, these are nicely visible across a select-able time axis as well as across the different select-able agents. With this we can quickly determine that our IPv4 agent has a lower latency towards the destination then the IPv6 agent. As both agents are basically the same machine, this should not be the case (See Figure 7). Let’s drill down even more into the next layer of information.                              Figure 7: Cause: Latency higher over IPv6 than IPv4     Network Path Visualization   The Network Path Visualization is a graphical Traceroute on steroids. Using TCP instead of ICMP - which might be filtered or take a different path - it shows all detected path between an agent and target along with valuable information. Hovering the mouse over the hops of the IPv6 connection will show information about each of the hops. The same applies for the link between two hops (See Figure 8).   This way we can quickly determine that the IPv6 agent reaches the webserver for the configured URL via the Vienna Internet Exchange in Vienna, Austria. The agent itself is located in Nuremberg, Germany about 300 miles / 480 km away from Vienna.                              Figure 8: IPv6 route goes from Nuremberg to Vienna     Looking at the IPv4 agent we can see that this in this case the webserver is located in Frankfurt, Germany about 100 miles / 160 km away (See Figure 9).                              Figure 9: IPv4 route goes from Nuremberg to Frankfurt     This discrepancy in path chosen for IPv4 and IPv6 traffic as well as the distance between the locations explain the performance difference.   I need to point out that the URL in this example is served by CloudFlare, a content delivery network and distributed domain name server service which uses Anycast for improving website performance and speed, and to protect websites from online threats. As CloudFlare has Points-of-Presence (POPs) in both Vienna and Frankfurt, traffic can be served by either of these locations, both in IPv4 and IPv6.   The problem in this case appears to be on the side of the provider in Nuremberg as they prefer a path towards Frankfurt for connecting to AS13335 (CloudFlare) via IPv4 while preferring a path towards Vienna for the same AS via IPv6. Here another nice feature of ThousandEyes comes into play: Share This Screen. This allows me to share the current screen with either live data or “canned” data around the time I have currently selected with someone who is not a customer of ThousandEyes. I can therefore easily share what I just discovered with the Service Provider in Nuremberg, allowing them to reproduce and better understand the issue. A great feature that saves a lot of time (See Figure 10).                              Figure 10: The Share This Screen feature allows others to see what I see     Network - BGP Route Visualization - IPv6   Let’s drill down even further into the BGP Route Visualization. We will start with IPv6. Here we see the BGP connectivity between ThousandEyes public agents and the target AS. Unfortunately the number of IPv6 capable public agents is very limited. Nevertheless we can see CloudFlare (AS13335) connecting to large transit providers such as Telia Sonera (AS1299) (See Figure 11).                              Figure 11: IPv6 Peering of CloudFlare (AS13335; blue-green circle) observed from agents (green circle)     Network - BGP Route Visualization - IPv4   Next we will look at the BGP routes for IPv4. Before we can do so, we will notice something interesting. ThousandEyes has discovered three applicable prefixes for the URL that was provided. There is a /21 and two more specific /24 prefixes (See Figure 12). Note that the URLs hostname actually resolves via DNS to two IPv4 addresses, one in each of the /24 prefixes.                              Figure 12: CloudFlare announces three IPv4 prefixes     An interesting feature of ThousandEyes is the ability to discover and show path changes for the case that a BGP peer was lost. Such path changes often result in brief moments of lost reachability while the path change propagates upstream. Or even worse it might cause route flapping, which can cause excessive activity in all the other directly connected routers.   Let’s look at the /21 prefix first. With a much larger number of IPv4 public agents we get a pretty nice picture of CloudFlares route graph (See Figure 13). We can see direct connections to Transit providers such as nLayer (aka GTT; AS4436) or Telia Sonera (AS1299), but also to smaller peering partners such as SoftLayer (AS36351).                              Figure 13: IPv4 Peering of CloudFlare (AS13335)     Now let’s see what happens when we look at one of the more specific /24 prefixes. Well, we are seeing a surprise (See Figure 14). The more specific /24 prefix is actually not visible from the majority of other Autonomous Systems. It appears to be only visible via especially smaller providers that peer directly with CloudFlare.                              Figure 14: More specific prefix is not visible in all peers, depicted as red border     CloudFlare uses a capability of BGP where a more specific route beats a less specific one, in order to perform clever traffic engineering. This approach allows CloudFlare to force end-user traffic via these direct peers to a more local POP, instead of preferring a transit provider and thus potentially terminating traffic farther away with a higher latency.   I wonder if CloudFlare has a tool as nice as ThousandEyes available to monitor and optimize their BGP traffic engineering.   ThousandEyes raises this anomaly as an alert, which you might have seen in Figure 4, even though it is not actually an issue but a desired behavior. Unfortunately there is no way to specify this as the desired behavior and disable the alarm for this.   Discovering Issues with your ISP: IPv6 vs IPv4   Let’s look at another interesting use case for ThousandEyes: IPv4 vs. IPv6 path performance. In this case we will have a look at an IPv4/IPv6 Dualstack target in the same physical location. Thus no Anycast this time.   One path leverages IPv6, the other one IPv4 (See Figure 15). Here we can clearly see issues with the IPv6 path - depicted in red - within the provider network that hosts the target.                              Figure 15: IPv4 vs IPv6 path with issues in IPv6 path     Looking at one of the problematic links (highlighted in yellow) via the IPv6 path, we can see a high delay and loss rate (See Figure 16). We also see that this link is part of an MPLS tunnel.                              Figure 16: High Delay and Loss via IPv6 on a segment within an MPLS tunnel     Looking at the same link via IPv4 (again highlighted in yellow), we don’t see any loss and a reasonable delay (See Figure 17). It appears that the depicted provider has performance problems with its IPv6 traffic.                              Figure 17: No issues on same segment via IPv4     It is no secret that older network equipment provides inferior performance for processing IPv6 traffic over IPv4 traffic. In extreme cases this can even mean that processing for IPv6 is done in software using the supervisor, while processing IPv4 is done in ASIC. Such an outdated network will obviously create massive issues as IPv6 vs. IPv4 traffic ratios pick up.   Summary   ThousandEyes is a very interesting tool for gaining insight into SaaS application performance and your overall network infrastructure. If you are an enterprise relying on SaaS applications such as Microsoft Office 365 or Google Apps, this is a great way to ensure that your employees get the performance they expect. It will help you identify issues and let you troubleshoot and resolve them quickly.   If you are a service provider offering a SaaS application, ThousandEyes is equally valuable as you are now not only able to monitor your service from various locations worldwide, but also drill down deep into any issues in the Internet that might degrade your customer’s experience. In the end the customer cares about the end-to-end experience, where a SaaS provider has limited direct control over the delivery chain.   Let’s hope that the existing rudimentary IPv6 support gets better over time as well.  ","categories": ["EdgeCloud"],
        "tags": ["IPv6","Management","Network"],
        "url": "https://www.edge-cloud.net/2014/06/02/monitoring-ipv6-websites-via-thousandeyes/",
        "teaser":null},{
        "title": "Protecting your website with DNS-Based Authentication of Named Entities (DANE)",
        "excerpt":"How can your users be sure that your HTTPS protected web-site is really what it seems to be and is actually your site? Today your web browser trusts a list of about 60-to-100 Certificate Authorities (CA) and you trust these CAs to only issue a certificate for a web site to the rightful owner of that site. As previous incidents, such as the March 11th Comodo security incident and the DigiNotar SSL Certificate security breach in the summer of 2011 have shown, this trust is not always justified. But how can your users verify that the X.509 certificate offered by your HTTPS server is indeed the certificate that should be used by this server and not a fake one used for a man-in-the middle attack against them?   This is where DNS-based Authentication of Named Entities (DANE) comes into the picture, adding another layer of security by tying the X.509 certificate of a website to the Domain Name System (DNS). This way you are adding a second independent channel which provides information about your X.509 certificate that a user can use to verify the rightfulness of this certificate.   Background   DNS-based Authentication of Named Entities (DANE, RFC6698) allow X.509 certificates, commonly used for Transport Layer Security (TLS), to be bound to DNS names using Domain Name System Security Extensions (DNSSEC). DNSSEC assures users that the information you obtain from DNS - in this case the fingerprint of the X.509 certificate came from the correct source, was complete and its integrity was not compromised during the transfer. All answers from DNSSEC protected zones are digitally signed. By checking the digital signature, a DNS resolver is able to check if the information is identical (i.e. unmodified and complete) to the information published by the zone owner and served on an authoritative DNS server (See Figure 1). Refer to How DNSSEC works from NIC.CZ for more information.                              Figure 1: DNSSEC concept (From NIC.CZ)     Today TLS encryption is based on certificates signed by certificate authorities (CAs), allowing users to trust such a signed certificate. Unfortunately many CA provider have suffered major security breaches in recent years, allowing the issuance of certificates for well-known domains to those who don’t own those domains. Users trusting such a compromised CA could thereby be fooled to trust rogue sites impersonating as such well-known domains.   DANE enables the administrator of a domain name to certify the certificates of the domain’s TLS servers by storing their fingerprint in the Domain Name System (DNS). In order to provide end-to-end security, DANE needs DNS records to be signed with DNSSEC to prevent DNS cache poisoning attacks. TLS servers that can be certified include e.g. HTTPS servers for secure web traffic but also SMTPS servers for secure mail exchange. Postfix e.g. introduced DANE support, allowing to further secure an SMTP connection.   Hands-On   In the rest of the article, I will show you how you can setup DANE for your own website and protect your users from Man-in-the-Middle attacks. For this the following items are needed as a pre-requisite:      A secured website using HTTPS: I will use a dummy website www.examples.com hosted on an NGINX webserver. Numerous instructions already exist on how to setup an HTTPS server with NGINX or Apache.   A Domain that supports DNSSEC: Today the majority of top level domains (TLD) support DNSSEC, but unfortunately not all do. A domain within such a TLD also has to be registered with a provider that supports placing DS records into the root zone. I will use a .com domain registered with GoDaddy as an example.   A DNS authoratative Server that supports DNSSEC and DANE: ISC BIND 9.9.1-P3 and newer supports TLSA records necessary for DANE and also DNSSEC. If you don’t want to go through the hassles of hosting your own DNS server, you can use a hosted DNS Service. I will use the hosted authoritative DNS Service Rage4. To my knowledge it is the only DNS Service that supports DNSSEC and DANE records at this time.   With the above elements in place let’s get started.   Registering a domain in Rage4   We will first start by registering a new domain in Rage4. Sign in or Register with Rage4 at https://secure.rage4.com. Create a new regular domain and fill out at least the Domain name and Administrator’s email. Confirm with a click on Save (See Figure 2).                              Figure 2: Create a new domain     Enter the Rage4 nameserver as the nameserver for your domain. You will have to do this with your registrar. In the case of GoDaddy you will find a row called Nameserver under the Domain Settings section of your domain. It will take between a few minutes and hours until the changes become visible in DNS.   Enabling DNSSEC   Next we will enable DNSSEC for this domain. Rage4 not only provides hosted authoritative name services with a global footprint and based on Anycast, but also hosted DNSSEC capabilities, taking care of the entire lifecycle for the digital certificates utilized in the hosted zone. You only have to manually place the DS record into the parent zone. For this we will later use GoDaddy. This makes dealing with DNSSEC super simple.   Click on Manage within the domains row to manage the newly created domain (See Figure 3).                              Figure 3: Manage the new domain     You will notice that by default DNSSEC is turned off for a newly created domain. Turn it on with a click on DNSSEC ON (See Figure 4).                              Figure 4: Notice that DNSSEC is turned off for the domain     Verify that DNSSEC has been turned on for the domain and click on DNSSEC INFO to retrieve DNSSEC information for the created domain (See Figure 5).                              Figure 5: DNSSEC is now enabled     Rage4 will display the content for the DS record that needs to be placed into the .com parent zone. We will need the value of the Key tag, Digest type 1, and Digest type 2 fields (See Figure 6). We will not need the actual DNSSEC key.                              Figure 6: DNSSEC information for the domain     Next head over to GoDaddy where you manage your domain. Within the Domain Settings area of your domain find the DS Records row. Click on Manage (See Figure 7).                              Figure 7: Manage DS record with Godaddy.com     You need to create 2 DS records. One of Digest Type 1 and one of Digest Type 2. Start with Digest Type 1.   Copy the Key tag value from the Rage4 DNSSEC Info page and paste it into the Key tag field. As Algorithm select 7 and as Digest type select 1. Last copy the Digest type 1 value from the Rage4 DNSSEC Info page and paste it into the Digest field (See Figure 8). Save the record.                              Figure 8: Create the first DS record (Digest type 1)     Next create the Digest Type 2 record. Copy the Key tag value from the Rage4 DNSSEC Info page and paste it into the Key tag field. As Algorithm select 7 and as Digest type select 2. Last copy the Digest type 2 value from the Rage4 DNSSEC Info page and paste it into the Digest field (See Figure 9). Save the record.                              Figure 9: Create the second DS record (Digest type 2)     It can take several minutes until the DS record has been updated in the .com zone. You can validate that your DNSSEC has been setup correctly via the Verisign Labs DNSSEC Analyzer. It will show you the trust chain from the ”.” zone, over the com zone to your own zone. If everything has been setup correctly you should see every single line with a green check mark (See Figure 10).                              Figure 10: Verify the DNSSEC status of the domain     See how easy it was to setup DNSSEC with Rage4?! With that one would hope that more domains were leveraging DNSSEC.   Create records for the HTTPS server   Now that DNSSEC is successfully working for the domain, it is time to create one or more A records for the HTTPS server, resolving e.g. https://www.examples.com and https://examples.com to an IPv4 address. If your HTTPS server supports IPv6, you can also create AAAA records.   Within the A records section click on NEW RECORD (See Figure 11).                              Figure 11: Create a new A record - Step 1     Enter the Record name and the Record value (IP address) for your HTTPS webserver. Save the entry with a click on Save (See Figure 12).                              Figure 12: Create a new A record - Step 2     Generate and save TLSA record   DANE uses a so-called TLSA record, which includes the fingerprint of the X.509 certificate that protects a host. First we will need to generate this TLSA record based on our webserver’s public certificate. Then this TLSA record will need to be added to our DNS zone.   For generating the TLSA record, we will use the TLSA Record generator from Shumon Huque.   Leave the Usage Field, Selector Field, and Matching-Type Field at the default settings. Next paste the public certificate of your website’s X.509 certificate in PEM format into the form. As the Port Number enter 443 and as the Transport Protocol enter tcp. As the domain name enter your domain, e.g. examples.com. Click on Generate to create your TLSA entry (See Figure 13).                              Figure 13: Generate the TLSA record     The generated TLSA record could be used directly with a modern version of ISC BIND. But as we are using Rage4, only the portion highlighted in yellow is of interest to us (See Figure 14).                              Figure 14: Retrieve the generated TLSA record     Unfortnately Rage4 doesn’t support adding TLSA records to a domain via the Web GUI yet. TLSA records are an experimental feature and are only supported via the API. We therefore need to use a REST client against Rage4’s API to complete this step. I will show you how to do this via the Advanced REST client for Chrome. But any other REST client should work as well in a similar way.   Note: In the meantime Rage4 has added support for TSLA records via their Web GUI. Instead of using the API you can therefore now create the entry via the Web GUI.   Before we can get started you need to lookup the zone ID for your domain. You can find this numeric 5 digit value as the last part of the URL while in the Manage view of your domain in the Rage4 Web GUI. In this example I’ll assume that this zone IS is 12345   With this the API endpoint is https://secure.rage4.com/rapi/createrecord/12345 and the method is GET.   You will need to pass the following Query parameters:      name: The value _443._tcp.www.examples.com or _443._tcp.examples.com with your domain instead of examples.com. You will want to create both entries in separate API calls.   content: The value from the TLSA generator highlighted in yellow (See Figure 14).   type: The number 17 for TLSA.   failover: The value false.   failovercontent: Leave the value field blank.   ttl: Use the default value of 3600.   geozone: Use the default value for global.   Also make sure to configure Basic Auth with your Rage4 username as the username and your API key as the password. Execute the API call (See Figure 15).                              Figure 15: Use a REST client to create the TLSA entry     Verify the success of your API call by querying the DNS system for the created entry. This will either be _443._tcp.www.examples.com or _443._tcp.examples.com. Use the command dig along with type52 for the TLSA record type:   root@srv01 ~ # dig type52 \\_443.\\_tcp.examples.com  ; &lt;&lt;&gt;&gt; DiG 9.9.5-3-Ubuntu &lt;&lt;&gt;&gt; type52 \\_443.\\_tcp.examples.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;   Congratulations! Your HTTPS web server is now protected via DANE. Don’t forgot to update the TLSA record in DNS in case you replace the X.509 certificate of your website.   Validating DANE protected sites   Protecting a HTTPS web site via DANE will be pointless unless the user verifies the data presented by the TLS connection with the data present in DNS. In the previous step you have already seen how to look up the certificate fingerprint stored in DNS via dig. In the above example the fingerprint is 3A7D64AD0D61F7EC2236261307744CCB7FE8A01AFE59377ADB04C8DE3DE3040A.   Using OpenSSL it is also possible to extract the fingerprint of the X.509 certificate use by a HTTPS server:   root@srv01 ~ # echo -n | openssl s_client -connect www.examples.com:443 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; /tmp/dane.txt depth=0 C = US, CN = www.examples.com, emailAddress = info@examples.com verify error:num=20:unable to get local issuer certificate DONE root@srv01 ~ # openssl x509 -noout -fingerprint -sha256 &lt; /tmp/dane.txt | tr -d : SHA256 Fingerprint=3A7D64AD0D61F7EC2236261307744CCB7FE8A01AFE59377ADB04C8DE3DE3040A   As you can see the fingerprint obtained by DNS and by the HTTPS connection match. This is excellent news as it shows us that we are actually talking to the correct server.   Of course it is unrealistic that users would perform this manual lookup every time before connecting to an HTTPS website. Instead we would expect that browsers do this lookup out of the box and warn the user in case of a mismatch. Unfortunately none of the modern browsers has this capability built-in. But luckily NIC.CZ provides this capability as a plugin for most modern browsers at https://www.dnssec-validator.cz/.   With the TSLA validator plugin installed you will now be able to identify whether a visited HTTPS website is protected via DANE and whether the validation of the TSLA record via DNSSEC succeeded or not (See Figure 16).                              Figure 16: Verify your HTTPS being protected by DANE     In this example the green closed lock symbol indicates that the X.509 certificate has been successfully validated via DANE.   Securing this blog with DANE   I would love to secure this blog with DANE in order to make the web a more secure place. Unfortunately I’m not able to do so as I use CloudFlare to deliver this blog. Doing so I’m forced to use CloudFlare as DNS service, which unfortunately neither supports DNSSEC nor TSLA records.   This is a shame as it would be super simple for CloudFlare to implement DNSSEC in a similar way to Rage4, thereby almost entirely Hands-Off for end-users. But as CloudFlare also manages the X.509 certificates for their customers, it would be possible to generate the TSLA records “automagically”. The result would be an extremely simple way to secure your HTTPS website with DANE via CloudFlare. Given CloudFlare’s footprint among major websites, this in return would hopefully put pressure on the Browser vendors to include DANE checking and lead to a much safer Internet.  ","categories": ["EdgeCloud"],
        "tags": ["Network","Security","Web"],
        "url": "https://www.edge-cloud.net/2014/06/16/practical-guide-dns-based-authentication-named-entities-dane/",
        "teaser":null},{
        "title": "Implementing IPv6: Six steps to success",
        "excerpt":"Implementing IPv6 in your network does not require tearing down your aging IPv4 network and replacing it with a new IPv6-enabled network. Instead it is possible - usually even wise - to run the IPv4 and IPv6 networks in parallel in what the industry calls a “dual-stack” network, thus adding IPv6 capabilities to your network’s existing IPv4 capabilities. While such an endeavor is certainly not trivial, it might be easier than your think.   The following article introduces a six step process for successfully implementing IPv6. It has served me well in past deployments and will hopefully give you some ideas and guidance.   Goal   Dual-stack as defined in RFC 4213 refers to side-by-side implementation of IPv4 and IPv6. In this case both protocols run on the same network infrastructure, and there’s no need to encapsulate IPv6 inside IPv4 (using tunneling) or vice-versa. This approach has to be considered the most desirable IPv6 transition mechanisms until IPv6 completely supplants IPv4. While it avoids many complexities and pitfalls of tunneling, it is not always possible to implement, since outdated network equipment may not support IPv6 at all.   The goal of the highlighted implementation steps in this article will focus on implementing dual-stack in an existing network. Such a network could be a data center network, a campus network, a Wide Area Network, or even wireless network. You should still have a look at other transition mechanism and decide what best suits your requirements.   Overview   After having determined the goal, let’s have a first look at the mentioned six steps for implementing IPv6, before going into details (See Figure 1).                              Figure 1: Six steps for a successful IPv6 implementation     The proposed steps include:      Training and Education: Enable technical staff (engineers, support, …) with IPv6 experience.   Network Audit: What can run IPv6 today, and what needs to be upgraded?   Network Optimization: Is the IPv4 network the best it can be?   Managing IPv6 address space: Acquire IPv6 address space and transit, draft your IPv6 address plan.   Deploy IPv6 in the network: Roll-Out IPv6 addressing and routing in the network.   Enable Network Services: This includes Active Directory/LDAP, DNS, NTP, …   Let’s look at each of these steps in more detail.   Step 1: Training and Education   While IPv6 is very similar to IPv4, it is still different enough to stumble at times. An example for this is the functionality of automatic address assignment via DHCPv6, which is quite different from DHCP in the IPv4 world.   Therefore it is very important to train and educate the involved technical stakeholders in an IPv6 implementation project. Ensuring that network architects, engineers and support staff not only know the theory of IPv6, but at least had some hands-on experience in a lab setup, is crucial to the overall success.   A good start for IPv6 related training is the 6deploy IPv6 e-learning package. Other valuable resources are the ARIN IPv6 wiki, the RIPE IPv6 Act Now page or APNIC’s Training page.   Step 2: Network Audit   Next you need to find out not only what equipment you have in your network, but also if it will support IPv6. The ugly truth is that products from almost all vendors have issues and bugs when it comes to IPv6. In many cases even though IPv6 functionality is available according to product specifications, these capabilities are either not tested at all or not to the breadth and depth of IPv4. Even if equipment meets requirements of the NIST USGv6 or the IPv6 Ready logo program, it doesn’t mean that it’s usable in your network for your use case.   With this it is unfortunately unrealistic to just “move” an Enterprise network to IPv6, as you can’t necessarily believe all the vendor specifications. Instead you will have to go beyond the pure cataloging of your equipment and actually need to test the required IPv6 functionality yourself.   For missing or broken IPv6 functionality, you will then have to work with the product’s vendor to acquire a fix, e.g. via a software upgrade or update. Often it is not possible to update the software and a hardware upgrade is required. In case the vendor cannot provide such a fix, or at least a roadmap with a firm timeline on anticipated fixes, it is highly recommended that you completely replace this vendor’s product.   As the outcome of this step, you should have information on what equipment you use, what can be made to support IPv6 via software changes, what needs a hardware swap and especially within which time frame you can realistically expect all these upgrades and swaps to happen.   Step 3: Network Optimization   The IPv6 implementation in a network is often a quite large endeavor. But it is also your perfect chance to clean up your existing network - which some might even call a “mess”. Most enterprise networks have organically grown over time into what they are today and include numerous artifacts from different implementation phases.   As such, you should have another look at your existing network and attempt to optimize it. Whatever you can optimize and especially simplify in your existing network today will make your life easier when adding IPv6.   While optimizing your network, you should use the following guidelines:      Simplify: Reduce the complexity of your network as much as possible, as you’ll end up adding complexity unintentionally over time again anyways.   Unify: Standardize on components within your network. More coherence leads to less headaches while managing components.   Amplify: As your previous network plans probably ended up being too small, this time plan big, really big!   Keep in mind: If you can get rid of a component altogether, there is no need to upgrade it to IPv6.   Step 4: Managing IPv6 address space   Nowadays it is very easy to acquire IPv6 address space as well as transit for it. If you already own IPv4 address space with your own Autonomous System (AS) you can request IPv6 address space from your Regional Internet registry (RIR). For transit of this IPv6 address space contact your existing  IPv4 peering partners who can usually provide you with IPv6 peering as well. In case they do not offer IPv6 peering it’s a good idea to look for another ISP, one that can.   If you do not own your address space, but use IPv4 addresses provided by your service provider, you can usually receive IPv6 addresses and associated transit from this service provider.   You will either receive a /32 or a /48 IPv6 address space. In IPv6 addressing, a /32 results in 65,536 subnets, each of which is the size of a /48. Each /48 contains 65,536 /64s, which is the minimum size of a subnet. Each /64 contains 18,446,744,073,709,551,616 IPv6 addresses. This means that each IPv6 /32 allocation contains 4.29 billion /64s. This is probably enough address space for a typical enterprise network. But on the other hand: What are we gonna do with all these addresses? That’s where a solid IPv6 address plan comes into the picture, allocating this address space to locations and/or functions. While you might not necessarily have such an address plan for your IPv4 network, it is crucial to have one for IPv6.   If you ask “Why?”, consider the following analogy: It might be possible to put together a puzzle with a few hundred or even thousand pieces without looking at the cover and seeing the picture of the final puzzle. But it’s pretty much impossible to do he same with a few million or billion puzzle pieces.   A very good resource on IPv6 address planning is RIPE’s document on Preparing an IPv6 address plan or the book IPv6 Address Planning from Infoblox’ IPv6 evangelist Tom Coffeen.   As highlighted in my previous article IPv6 deployment: Using link-local addresses as default gateway, I’m using the ULA address range fd53::/64 for DNS Anycast, where my DNS resolvers are fd53::11 and fd53::12 everywhere in the network. Also I use the Link Local address fe80::1 as the default gateway for static addressing. Following RIPE’s recommendation I reserve a /64 network for point-to-point links, while addressing them as a /127 for 2 member addresses or /126 for 4 member addresses (e.g. for VRPP/HSRP). Furthermore I only subnet on nibble boundaries (network mask which aligns on a 4-bit boundary), making it easier to perform the math around IPv6 addresses and subnets.   Last but not least: At this point often the question comes up whether to use ULA addresses throughout the entire company along with IPv6-to-IPv6 Network Prefix Translation (RFC 6296) or IPv6-to-IPv6 Network Address Translation (NAT66), or not. This question is especially posed based on the desire to simulate the usage of IPv4’s RFC 1918 address space along with NAT, coupled with the false believe that this provides security to your network. In short: It doesn’t.   From my experience of using both approaches, Global Address space as well as ULA with NPT, I could not find any benefit of the ULA+NPT approach. Instead it only created more hassles and work. With this approach you now have to managed twice the amount of address space. So instead be happy that NAT is (hopefully) finally dead with IPv6 and use Global Unicast addresses.   Step 5: Deploy IPv6 in the network   Once you made it up to here, it’s time to put your preparation into action. Surprisingly this step will be very easy, if you’ve done all your homework right. Configuring IPv6 addresses on network device interfaces is usually straight forward and configuring routing protocols - such as OSPFv3 - with IPv6 is also quite simple.   For Cisco IOS devices you can refer to the IPv6 Implementation Guide or the book Cisco Self-Study: Implementing Cisco IPv6 Networks. Also recent course material for the Cisco Certified Network Associate (CCNA) Routing and Switching certification includes details on implementing IPv6.   For address management in IPv6 you have different options as I described in the previous article IPv6 Address management of hosts. In real life you will usually either use manual assignment of IPv6 addresses to e.g. server systems, or Stateful DHCPv6, where O(ther) and M(anaged) flags are set, while the (A)utonomous flag is unset inside the Router Announcements (RA). While address assignment via Stateless Address Auto Configuration (SLAAC) is often touted as one of the benefits of IPv6, it still requires DHCPv6 to assign DNS resolver information due to the lack of RDNSS (RFC 6106) support in modern client OSes. This makes this approach as complicated and complex as using Stateful DHCPv6 right away.   Step 6: Enable Network Services   After configuring your network components, ranging from L3 switches, over firewalls and routers to load balancers or WAN accelerators, you’re almost ready to connect end-users via IPv6. What’s missing are network services such as Domain Name System (DNS), Network Time Protocol (NTP), Remote Authentication Dial In User Service (RADIUS) or Microsoft Active Directory.   While the network services available in your network might vary and differ from the list above, only by making services available via IPv6, are you actually adding value via IPv6. Otherwise your IPv6 implementation is like a highway without any on or off ramps.   Similar to the previous step, enabling IPv6 on devices or hosts for such network services is usually straight forward once you have verified that the product in its current version actually supports IPv6.   Summary   Hopefully you’ve seen that implementing IPv6 in addition to IPv4 in your network isn’t that hard. A successful IPv6 implementation often comes down to the right mindset of the involved stakeholders. The team needs to realize that IPv6 is not a bolt-on to IPv4. Instead it will replace IPv4, eventually. As such you should focus on designing based on this reality and ensure that IPv6 has at least the same status as IPv4. Even if you don’t change any existing equipment and services, but make sure that all new equipment and services are IPv6 capable, it’s just a matter of time until your network will support IPv6.   With that  IPv4 is the past, IPv6 is the future. You should know your history, but put your energy into the future.  ","categories": ["EdgeCloud"],
        "tags": ["IPv6","Network"],
        "url": "https://www.edge-cloud.net/2014/06/25/implementing-ipv6-six-steps-success/",
        "teaser":null},{
        "title": "Use a Cisco WLC based Wifi with the CloudTrax captive portal",
        "excerpt":"The Cisco Wireless LAN Controller (WLC) solution provides 802.11 wireless networking solutions for enterprises and service providers via a combination of a central wireless LAN controllers and associated lightweight access points controlled by the controller, all concurrently managed by any or all of the operating system user interfaces.   The solution simplifies deploying and managing large-scale wireless LANs by managing all data client, communications, and system administration functions, performing radio resource management (RRM) functions, managing system-wide mobility policies, and coordinating all security functions (See Figure 1).                              Figure 1: Simple Cisco WLC based network     Despite these outstanding capabilities, Cisco’s WLC brings rather limited and complicated capabilities to the table when it comes to creating a simple captive portal for Wifi users, where authentication of valid customers is performed via voucher codes.   Here Open-Mesh with it’s cloud-based CloudTrax controller offers a very simple and cost-effective solution to deploy a Wifi network - e.g. within a hotel, restaurant, college campus or other location - for users to authenticate via a Captive Portal for free, with a free or prepaid voucher or via PayPal payment (See Figure 2).                              Figure 2: Captive Portal provided by CloudTrax     But if you already made a large financial and time investment into your Cisco WLC based Wifi network you do not necessarily want to rip and replace this network with an Open-Mesh network. Instead you might want to use the best of both worlds: The rock solid Cisco hardware with the easy to use CloudTrax captive portal. In this article I’ll show you how to do exactly this.   Existing Cisco WLC network   With Cisco WLC a service set identifier (SSID) is mapped to a VLAN within a port. As shown in Figure 3, each controller port connection is an 802.1Q trunk and should be configured as such on the neighbor switch. As an alternative you can also use the untagged VLAN within the WLC, thus not requiring a 802.1Q on the neighbor switch side. But in this case only a single SSID can be configured. As we want to provide a single public SSID with a CloudTrax backed captive portal, this setup would be sufficient.                              Figure 3: Ports, Interfaces, and WLANs in a Cisco WLC     Open-Mesh physical setup   The Cisco WLC does not directly support interacting with the CloudTrax-based Captive Portal. This capability is restricted to Open-Mesh based devices. In order to make a Cisco WLC based WiFi network work with CloudTrax, the solution is to place an Open-Mesh device - e.g. the OM2P - between the Cisco WLC based WiFi network an the internet (See Figure 4).                              Figure 4: Physical Open-Mesh setup     Make sure to plug the internet uplink into the port closest to the power plug and the Cisco WLC to the other port. Also keep in mind that not all Open-Mesh devices come with a second ethernet port. The Open-Mesh OM2P does and is therefore the recommended device for this project.   From the perspective of the Cisco WLC, the OM2P just acts like an upstream switch and from the perspective of the OM2P the Cisco WLC just appears to be a wired device.   Keep in mind that the OM2P does not support an 802.1Q trunk towards the WLC. You therefore either have to use the native VLAN capability on the WLC when connecting it directly to the OM2P. Or you have to place a switch that does support an 802.1Q trunk between the WLC and the OM2P. In this case the VLAN from the 802.1Q trunk towards the WLC needs to be mapped to an access port towards the OM2P.   CloudTrax Online Dashboard Configuration   Please refer to the CloudTrax documentation at https://www.cloudtrax.com/ for the quick start guide and documentation on how to use the splash page. Below are the configuration items that need to be changed for CloudTrax to work with the WLC:   Public SSID settings tab   You don’t actually want to use the Open-Mesh device to provide any wireless capabilities. This should all be handled by the Cisco WLC-based network. Unfortunately it is not possible to turn off the wireless capabilities on an Open-Mesh device completely. Instead we will need to make some configuration changes to make the wireless network on the Open-Mesh device as inaccessible as possible. First, choose an SSID that does not correspond to the SSID that is configured in the Cisco WLC for the public accessible network and also hide this SSID (See Figure 5). Bottom-line: You don’t want clients to connect to the Open-Mesh device, but to the Cisco WLC network.                              Figure 5: CloudTrax Public SSID settings     Private SSID settings tab   Similar for the private SSID. You do not actually want to provide this network via the Open-Mesh device. Therefore turn it off completely. But also make sure that the “Wired Clients” tick box is not selected (See Figure 6). This way the Cisco WLC based network will be mapped to the public network in CloudTrax and can use the captive portal.                              Figure 6: CloudTrax Private SSID settings     Radio settings tab   Next reduce the transmit power of the Open-Mesh device to the minimum available (See Figure 7). This will reduce the likelihood that devices can connect to the Open-Mesh device over the wireless network. Instead they will connect over the Cisco WLC network.                              Figure 7: CloudTrax Radio settings     You can also remove the antenna from the Open-Mesh device to further reduce the likelihood of a device connecting over the wireless interface to that device. But doing so will burn out the wireless chip of the Open-Mesh device over time. This approach is therefore not advisable if you plan on using this Open-Mesh device as a WiFi unit in the future.   CloudTrax scripting   Unfortunately we are not done yet at this point. We need to add a custom script on the Open-Mesh device, so that it can handle the static IP address of the WLC device. We also need to configure the WLC with such a static address for the network issued by the Open-Mesh device.   First connect to the Open-Mesh device via SSH and lookup the IP address information used for the public SSID via the command “ifconfig br-pub”. The result should look like this:   root@N3:~# ifconfig br-pub br-pub    Link encap:Ethernet  HWaddr AC:86:12:34:56:78           inet addr:10.255.48.1  Bcast:10.255.51.255  Mask:255.255.252.0           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1           RX packets:2379186 errors:0 dropped:306 overruns:0 frame:0           TX packets:1812674 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:0           RX bytes:178505478 (170.2 MiB)  TX bytes:2171022384 (2.0 GiB)  root@N3:~#   Note that in this case the public SSID network operated by Open-Mesh is 10.255.48.0 with a netmask of 255.255.252.0 and the Open-Mesh device itself will operate at 10.255.48.1.   With these information we can now configure the Cisco WLC device to be at 10.255.48.2 (See Figure 8). Note that both the Gateway and the Primary DHCP Server are filled with the IP address of the Open-Mesh device.                              Figure 8: Cisco WLC Interface configuration     Next create the file /sbin/wlc inside the Open-Mesh device with the following content. Replace the IP address 10.255.48.2 with the IP address that corresponds to the WLC in your setup.   #!/bin/sh echo \"Checking for WLC\" ping -c3 10.255.48.2 &gt; /dev/null 2&gt;&amp;1 || {         ifconfig br-lan2 down         ifconfig br-pub down         brctl delif br-lan2 eth1         brctl addif br-pub eth1         ifconfig br-pub up         ifconfig br-lan2 up }   This script will test connectivity to the WLC and if this test fails, re-configure the network interfaces on the Open-Mesh device. This script should run regularly, as configuration changes pushed from CloudTrax can undo any changes. This can be accomplished by creating a symbolic link from /sbin/wlc to /etc/cron.5mins/wlc with the command ln -s /sbin/wlc /etc/cron.5mins/wlc.   Check your results with:   root@N3:~# ls -l /etc/cron.5mins/wlc lrwxrwxrwx    1 root     root             9 Dec 29 22:41 /etc/cron.5mins/wlc -&gt; /sbin/wlc root@N3:~#   Results   After a maximum of 5 minutes you should be able to connect to the Cisco WLC-provided SSID and receive an IP address on the above mentioned network. Upon opening a web browser and connecting to any web-page you will be displayed with the captive portal, where you can login via a free or prepaid voucher.   The client device will connect to the lightweight Cisco access point that is managed via the WLC. When asking for an IPv4 address via DHCP, the WLC acts as a DHCP proxy with the Open-Mesh device being the actual DHCP server.   Keep in mind that all Internet traffic from the Cisco WLC based WiFi network will now traverse through the Open-Mesh device. This means that a high number of parallel user sessions on the WiFi network can cause a high CPU utilization on the Open-Mesh device.  ","categories": ["EdgeCloud"],
        "tags": ["WiFi"],
        "url": "https://www.edge-cloud.net/2015/01/13/cisco-wlc-with-cloudtrax-captive-portal/",
        "teaser":null},{
        "title": "Software Defined Data Center (SDDC) Architecture - Introduction",
        "excerpt":"A Software Defined Data Center (SDDC) is a vision for a new IT infrastructure that applies virtualization concepts such as abstraction, pooling, and automation to all resources of a data center. It promises to improve the delivery, operation, and recovery of Business Applications through increased agility and performance, while reducing service delivery times.   Creating and executing a strategy to realize this vision is a journey that needs to include not only new technology, but also changed processes as well as people with new training and mindsets.   In a series of articles I want to focus on the architecture and some of its design elements for a SDDC. This first article will focus on the requirements of an SDDC, before attempting to break up the problem into manageable “chunks” and address them in a divide-and-conquer fashion in subsequent posts.   Although the presented problem statement, architecture and design could apply to a wide variety of products, I will mostly focus on products from VMware and its eco-system partners. Also while the presented architecture and design might not necessarily exist in its entire form at a customer site today, individual elements presented have certainly proven it’s success as part of numerous customer projects.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Throughout the architecture and design discussion I will attempt to provide traceability between the design decisions and these requirements. Therefore let’s look into each of these requirements in more detail:   Highly automated operation at Scale   Today’s IT departments are pressed to do more with less and provide IT services at a high quality and a lower cost. Doing so, IT departments often have to compete with outside services ranging from public clouds such as Amazon Web Services (AWS) for Infrastructure services, all the way to Office 365 for SaaS based offerings. And if IT departments are successful with their internal offerings, they need to ensure that they can scale up in a reasonable time-frame to meet the new demand.   To deliver on this requirement one will quickly discover that it is necessary to use a strong foundation of automation to provide a swift and reliable infrastructure that can easily scale up and provide offered services. Adding more headcount to accomplish this task is not an option as it would not only lead to increased cost, but also to largely unpredictable outcomes due to human errors in the scaled operations.   Last but not least, in order to compete with the abilities and the price of web-scale services such as AWS, IT departments need to leverage some of their design elements to achieve similar unhampered scale at a reasonable price point.   Hardware and Software efficiencies   The traditional approach to data centers was often a combination of one-size fits all - for simplifying operations - as well as best-is-just-good-enough - due to the requirement of running mission critical workloads. While the requirement for reliability doesn’t go away, new and old business needs (see next section) have more differentiated requirements for business continuity and disaster recovery. This offers the possibility to shift certain capabilities around availability from hardware to software or even give up on them altogether within the infrastructure. Let the application itself deal with failures.   One of the corner stones of a Software Defined Data Center is the introduction of virtualization for not only compute, but also networking - known as Software Defined Networking (SDN), and storage - known as Software Defined Storage (SDS). This allows the tear-down of resource silos, allow resource pooling and thereby the reduction of costs.   New and old business needs   An IaaS cloud such as AWS is geared towards a cloud application model, with cloud native applications. These applications implement mechanism to cope with environment-induced failures within the application itself instead of leveraging hardware or platform redundancy. While we want to also support these cloud native applications in an SDDC, the vast majority of enterprise applications are still traditional applications that are not optimized for such a cloud model. The SDDC shall therefore especially provide a home for these legacy applications, while at the same time offering some of the benefits of cloud computing, such as automation and self-service.   High-level Architecture for a Software Defined Data Center (SDDC)   Next we will break up the design of a Software Defined Data Center (SDDC) into manageable “chunks” and address them in a divide-and-conquer fashion in subsequent posts. To do so, the SDDC is split into three main layers, along with capabilities spanning all three layers (See Figure 1).                              Figure 1: Architecture Overview of a Software Defined Data Center     The main layers are:      Physical Layer: This includes the physical compute, network and storage components.   Virtual Infrastructure Layer: This layer includes the traditional virtualization platform with the hypervisor, resource pooling and virtualization control. VMware products falling in this category are vSphere and NSX.   Cloud Management Layer: This layer adds capabilities to the Virtual Infrastructure Layer, bringing capabilities known from IaaS clouds to the SDDC. These capabilities include service catalogs, self-service portals and an orchestration engine. VMware products in this layer are vRealize Automation&lt;/a&gt; (formerly vCloud Automation Center), vCloud Director or VMware Integrated OpenStack (VIO)(http://www.vmware.com/products/openstack.html), along with vRealize Orchestrator (formerly vCenter Orchestrator).   Additional capabilities that span across these main layers are:      Service Management: The ability to manage the entire SDDC via a single pane of glass, including operations management and portfolio management for offered services. VMware products in this layer are vRealize Operations (Formerly vCenter Operations Management Suite).   Business Continuity: The ability to provide business continuity for the SDDC itself, but especially the hosted workloads. This includes the fault tolerance of SDDC components, backup &amp; recovery of data and services as well as data replication. Products from the VMware eco-system are Veeam Backup &amp; Replication or Zerto Business Continuity &amp; Disaster Recovery.   Security: Provide security mechanism for governance, risk mitigation and compliance. Products from the VMware eco-system are HyTrust Cloudcontrol and Datacontrol   We will need to address these capabilities in each of the layers.   Summary   This article is the foundation for a series of further articles in which we will together embark the journey to let the vision of a software defined data center come true through an architecture with specific design elements. After outlining the requirements to be fulfilled by this architecture, the above outlined high-level SDDC architecture also provides an outline for future articles.   By reducing the complexity of the SDDC, we can also reduce the risk of the entire project and thereby increase the likelihood of achieving the desired return on investment.   Posts in this series   Within the Physical Layer we want to look at these design artifacts:      What is a validated design?: Explains what the VMware Validated Design is and why it is useful.   Basic Design Elements: Introduces the 5 basic design elements of the SDDC with Layered logical model, POD / Core concept, L3 Spine / Leaf network, Management Applications Network Container, and Service Level tiers.   Core and POD Design: POD (Point-of-Delivery) as an atomic building block of Data Center resources, connected to a CLOS network for increased scale, agility, flexibility and resilience.   Availability Zones and Regions: Provide continuous availability of the SDDC, minimize unavailability of services and improve SLAs via Availability Zones, while using Regions to improve locality of SDDC resources towards end-users.   Virtual PODs for Management applications: The Virtual POD network container is a very powerful, yet simple concept to provide the management applications of the SDDC with security, modularity, simplicity, improved BC/DR capabilities and IPv6 support. All this with a minimum of integration effort.   Mapping of Logical Components to Physical Location: Mapping of the logical components of the Software Defined Data Center to the underlying physical components. Mapping of vCenter Server to Platform Service Controllers.   Business Continuity with multiple regions: Providing Business Continuity and Disaster Recovery (BC/DR) capabilities to the SDDC itself across multiple regions.   Disaster Avoidance with multiple Availability Zones (AZs): Using multiple Availability Zones (AZs) to prevent downtime and exercise Disaster Avoidance.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/02/20/sddc-architecture-introduction/",
        "teaser":null},{
        "title": "SDDC Architecture – Core and POD design",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC as well as some of its design elements. In this post we want to look at the physical layer of our SDDC architecture (See Figure 1).                              Figure 1: Physical Layer in the SDDC Architecture     Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Physical Layer Design Artifacts   Within the Physical Layer we want to look at these design artifacts:      POD and Core Design: POD (Point-of-Delivery) as an atomic building block of Data Center resources, connected to a CLOS network for increased scale, agility, flexibility and resilience.   POD and Core Design   More and more enterprises are looking at this concept known from web-scale companies in order to streamline functions and cost, but especially to support future growth. For this they leverage a design concept called “POD and core”, where data centers are build out with small chunks of equipment dedicated to different types of workloads and different capabilities. This approach, which we will here discuss further is very well suited for deployments that start small, but then grow to large-scale over time, while sticking to the same overall architecture. As a SDDC is a journey with multiple implementation phases to address maturing requirements, capabilities and demand, this approach is very suited here and therefore growing in popularity.   With the POD and Core design, data center architects design a small collection of common building blocks to suit various application needs. These building blocks can include different combinations of servers, storage, network equipment, etc. and can each be designed with varying levels of hardware redundancy and quality of components to fulfill the needs of the specific requirements. Over time individual building blocks can evolve through new revisions, allowing to address lessons-learned without the need to rip-and-replace existing hardware (See Figure 2).   Moreover by breaking down the entirety of a data center into groups of PODs, you are applying a divide and conquer methodology, making it easier for architects to understand how their gear fits together as a whole and allowing them to focus on optimizing individual pieces.                              Figure 2: PODs of different type, QoS and version     The common building blocks are then connected to a network core, that distributes data between them. This kind of network core has to address different requirements than traditional data center networks. Traffic patterns are transforming more and more from client-server (North-South) to between servers (East-West). Scale increases and has to connect 10s of thousands to 100s of thousand endpoints. Agility should improve by allowing to add PODs within hours, not weeks or month and new logical networks should be spun up in seconds and not weeks. At the same time the POD design calls for a high flexibility of the network design by (re-)using the same infrastructure for very different building blocks. All this should happen while improving the resilience through fine grained failure domains.   A very elegant answer to these physical network requirements is a L3 based CLOS network, also called Spine/Leaf (See Figure 3).                              Figure 3: CLOS Network     One of the guiding principle for such deployments is that the network virtualization solution via Overlay Networks allows to do away with any spanning of VLANs beyond a single POD. Although this appears to be a simple change, it has widespread impact on how a physical network infrastructure can be built and on how it scales. We can now use proven L3 capability - e.g. via Internet-scale proven BGP - between PODs, while restricting the L2 domain to the POD itself. This greatly reduces the size of the failure domain.   POD types   For the SDDC we will use four kinds of pods (See Figure 4):                              Figure 4 : SDDC Pod Concept        Compute: Compute PODs make up the main part of the infrastructure where virtual machines for end-user workloads are hosted. Different types of compute PODs with varying levels or redundancy, built-quality and price can be mixed within a single SDDC. This approach provides separate compute pools for different types of SLAs. While it is possible for Compute PODs to include local storage - e.g. via vSAN, doing so creates silos as the usage of this storage is limited to within the POD. If using separate storage PODs, storage from these PODs can be used by multiple compute PODs and one compute POD can use storage from multiple Storage PODs. This increases flexibility, as it allows you to grow storage and compute independently.   Storage: Storage PODs provide network accessible storage via NFS or iSCSI. Similar to the other pods, different levels of SLA can be provided via different kind of storage pods, ranging from JBODs with SATA drives and with minimal to no redundancy to fully redundant enterprise class storage arrays, filled with SSDs.   Management: The Management Pod runs all virtual machines instances that are necessary to operate the management functionality of the SDDC. More specifically this includes all VMware management components, including vCenter Server, NSX Manager, Cloud Management Platforms (CMP) - such as vCloud Director, VMware vRealize Automation or VMware Integrated OpenStack - and other shared management components. This POD also provides external network connectivity to the Management network and as such does not include any tenant specific IPv4 or IPv6 addressing.   Edge: The SDDC spine-leaf network fabric itself does not provide external connectivity. Instead external connectivity is pooled into so-called Edge PODs in order to reduce cost and better scale changing demand for external connectivity. The Edge pods connect to the data center fabric as well as the Internet or Enterprise-internal Wide Area Networks (WANs). They therefore also provide the on-/offramp functionality between the overlay networks of the network virtualization solution and the external networks. This is accomplished by running VM-based edge services on general compute hardware. As such the main functions provided by an edge rack are:            Provide on-ramp and off-ramp connectivity to physical networks       Connect with VLANs in the physical world       Optionally host centralized physical services           Tenant-specific IPv4 and IPv6 addressing is exposed to the physical infrastructure in the edge rack. This is either done via L3 connectivity, using static or dynamic routing or via L2 connectivity, bridging VLANs from the Internet / WAN side into VXLANs on the SDDC network fabric side.&lt;/li&gt; &lt;/ul&gt;   In small to medium sized deployments it is recommended to combine Management and Edge POD into a single rack as compute requirements for these two types of PODs are rather limited. Also these POD types both require external network access, making them a prime candidate for combining.   In large deployment it is advisable to split Management and Edge POD. This allows to scale external connectivity of the SDDC by adding additional Edge PODs (See Figure 5).                              Figure 5 : SDDC Pod Concept for a large setup     PODs and Service Levels   As already mentioned it is possible to have different pods of the same type, providing different characteristics for varying requirements. As such one compute pod could e.g. be architected using full hardware redundancy for every single component (redundant power supplies through ECC memory chips) for increased availability, while at the same time, another compute pod in the same SDDC could use low-cost hardware without any hardware redundancy. With these kind of possible variations an architect is better suited to cater to the different requirements for the SDDC to cater to new and old business needs.   From a network perspective you also have multiple options for attaching servers to the Leaf network node (See Figure 6). Which option you choose depends on your requirements - including cost and available / approved hardware.                              Figure 6: Network Attach Options     The options in detail are:      Single Attach: Servers are connected via a single (usually 10 GigE) network connection to a single leaf switch. This attach option provides no network redundancy. But it is nevertheless frequently used by webscale companies, where failures of servers or entire PODs are easily mitigated by the infrastructure or application software layer.   Dual Attach via Port-channel: In this case the server is connected via two (usually 10 GigE) network connections to a pair of switches. These switches must support Multi-Chassis Link Aggregation Group (MLAG) capabilities to provide a port-channel (ideally via LACP) towards the servers. Various vendors offer data center switches that support this capability, e.g. by allowing to stack the Top-of-Rack (ToR) switches.   Dual Attach via separate subnets: The other option for Dual attaching servers is to use multi-homing. Here each server is also connected via two (usually 10 GigE) network connections to two upstream switches. But in this case, the upstream leaf switches are truly separate and each offer a separate subnet towards the server. As a result the server would e.g. have two separate subnet for management capabilities available.   Summary   This article highlighted the concept of the POD and Core design. It especially outlined how this concept fulfills our SDDC requirements:      Highly automated operation at Scale: The modularity of the POD design allows to quickly and easily scale up and down the SDDC based on actual demand. Due to the high uniformity of a POD it is possible to automizing parts of adding and removing a POD, while reducing physical labor to standard procedures.   Hardware and Software efficiencies: The modular PODs not only allow on-demand scaling for varying capacity needs, but especially promote resource pooling to drive increased utilization of resources and reduce cost. This is especially achieved by splitting compute and storage capacity in separate resources and thereby allowing independent pooling and scaling.   New and old business needs: Using different instance types for the various, it is possible to offer differentiated SLAs through varying levels of hardware cost. As a result it is possible to remove redundancy from hardware, thereby reducing cost, while allowing infrastructure or especially application software layers to mitigate this missing feature. As a single SDDC can include pods of varying SLA type, this allows the operation of legacy applications with traditional business continuity and disaster recovery needs right along new cloud-native applications.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/03/10/sddc-architecture-core-pod/",
        "teaser":null},{
        "title": "IPv6 in vSphere 6",
        "excerpt":"Note: Last updated on May 1st, 2015   With the release of vSphere 6, the IPv6 capabilities of vSphere have greatly improved. In this post I want to provide an overview of these new capabilities for vSphere as well as NSX for vSphere (NSX-v). Together these two products are now able to form a decent virtual infrastructure layer, supporting many capabilities in IPv6.   This IPv6-enabled virtual infrastructure in return can be the foundation for Horizon View, which also supports IPv6 since version 6.1.   Use Cases   The overview will be grouped by use case, product and then by function. The two use cases for IPv6 are:      Tenant Use Case: Ultimate Goal: Provide IPv4/IPv6 connectivity to tenant workloads   Management Use Case: Ultimate Goal: Manage entire vCloud backend via IPv6-only   For each use case this article lists what capabilities and features supported IPv6 in vSphere 5.1, vSphere 5.5, vSphere 6.0, vCNS 5.5, NSX-v 6.0 and NSX-v 6.1.   Note: Only NSX-v 6.1.3, supports vSphere 6.0.   Let’s have a look at these two use cases in more detail:   Tenant Use Case   Ultimate Goal: Provide IPv4/IPv6 connectivity to tenant workloads (See Figure 1).                              Figure 1: IPv6 Tenant Use Case     The tenant use case is geared towards providing full IPv4/IPv6 network connectivity to workloads running on top of a virtual infrastructure. This includes the tenant path of network infrastructure elements, but does not include the management of non-end-user exposed component.   This use case includes the following requirements:      Support for Workloads            Guest customization (Configure IP settings from outside VM)       Virtual Network Access (vNIC): IPv6 Offload, App Firewall           Support for Workload Management/Configuration            “Awareness” of IPv6: Ability to manage IPv6-capable configuration items (interfaces, pools, route tables, ACLs, …)       Support for network devices (switch, router, security device, load balancer). Applies to vSS, vDS and vShield/NSX Edge (Multiple profiles).           vSphere                  Product/Function       vSphere 5.1       vSphere 5.5       vSphere 6.0       Notes                       General Operation       Yes       Yes       Yes       Functionality has to be provided by Guest OS                 Guest OS                                                 TCP Segmentation Offload (TSO) over IPv6       Yes       Yes       Yes       Only supported by VMXNET3 vNIC. Not supported by E1000 vNIC.                 Guest Customization (Sysprep)       No       No       Yes       No support to join Active Directory via IPv6 in vSphere 6.0. Certain limitations apply.                 Virtual Switch (vSS/vDS)                                                 Multicast support       Yes       Yes       Yes       Snooping modes supported: IGMPv1, IGMPv2, IGMPv3 for IPv4, MLDv1 and MLDv2 for IPv6 supported.           vCNS / NSX-v                  Product/Function       vCNS 5.5       NSX-v 6.0/6.1       Notes                       Guest VM Addressing       Yes       Yes       VXLAN encap packets (VXLAN Encapsulated Inner Header) are capable of carrying IPv6 payload. If IP hashing is configured, ESXi can base decisions on IPv4 or IPv6 packets.                 vCNS / NSX Edge                                         Edge Interface IP Address       No       Partially       Only support for static IPv6. No support for DHCPv6 or SLAAC.                 DNS Resolver               Yes       IPv4 and IPv6 Listener. Supports AAAA records.                 Router Announcements (RA) for SLAAC       No       No                         DHCP server / relay       No       No       Supports IPv4 only                 Static Routing       No       Yes                         Dynamic Routing (OSPF, ISIS, BGP)               No       No dynamic routing with IPv6                 Firewall       No       Yes                         Load Balancer       No       Yes       IPv4 and IPv6 VIP. Transparent Mode: IPv4 VIP with IPv4 Pool, IPv4 VIP with IPv6 Pool.Proxy Mode: IPv4 VIP with IPv4 Pool, IPv6 VIP with IPv6 Pool, IPv6 VIP with IPv4 Pool (L7 only), IPv4 VIP with IPv6 Pool (L7 only)                 NAT       No       No       No support for NAT64 or NAT66                 IPSec       No       Yes       IPv4, IPv6 and Hybrid. IPv6 Peers with IPv6 Internal Subnet. IPv6 Peers with IPv4 internal Subnet.                 SSL VPN               Partially       IPv4, IPv6 Listener. IPv6 Listener Address. IPv6 Listener with IPv4 private subnet.                 L2 VPN               Partially       IPv4, IPv6 Listener. Outer packet can be IPv4 and IPv6. Inner packet can only be IPv4.                 NSX Virtual Distributed Router (VDR)                                         Static Routing               No                         Dynamic Routing (OSPF, ISIS, BGP)               No       No dynamic routing with IPv6.                 Distributed Firewall               Yes                   Managemenet Use Case   Ultimate Goal: Manage entire vCloud backend via IPv6-only (See Figure 2).               Figure 2: IPv6 Management Use Case       The management use case assumes that the operator of the virtual infrastructure has exhausted its IPv4 address space. The goal is therefore to move all management traffic (all traffic not directly visible to customers or tenants) to IPv6-only, potentially with the interim step of IPv4/IPv6 Dualstack. Therefore all non-tenant traffic of the virtual infrastructure with external systems, but also between components residing on different devices needs to be IPv6-only. This use case includes the following requirements:      Operating System (OS) support            Fulfill device requirements for hosts with the goal for an OS: Ability to offer IPv4/IPv6 transport services to applications           Application support            Server applications vs. client applications: Accept vs. initiate IPv6 communication.       Be able to communicate over IPv4-only and IPv6-only networks.       Network parameters in local or remote server settings, need to support configuration of IPv6 parameters       All features offered over IPv4 must be available over IPv6 without any noticeable difference (usability, performance), unless providing explicit benefit to the user.           vSphere                  Product/Function       vSphere 5.1       vSphere 5.5       vSphere 6.0       Notes                       vCenter Server                                                 vCenter on Windows       Yes       Yes       Yes       For deploying a single instance standalone. To configure vCenter Server with an external database, provide the FQDN of the target database server.                 vCenter Linux appliance (vCVA/CloudVM)       No support, but may work       No support, but may work       Yes       For deploying a single instance standalone. vCenter Server Appliance deployment wizard requires a DNS entry for the ESXi host and does not support a literal IPv6 address. Connection of the vCenter Server Appliance to Active Directory is not supported over IPv6. Use Active Directory over LDAP as an identity source in vCenter Single Sign-On instead.                 PSC / SSO, Inventory Services       No       No       Yes       For deploying vC across multiple hosts. Includes STS (secure token service), Lotus (directory service / key value store), IDM (identity manager), VMCA (certification service). To configure the Platform Services Controller with an external database, provide the FQDN of the target database server.                 Common Logging Infrastructure       No       No       Yes                         vCenter Converter       No       No       Yes                         VMotion       Yes       Yes       Yes       VMotion incorrectly prefers transport via IPv4 if both IPv4 and IPv6 transport (aka Dualstack) are available                 Auto Deploy / PXE boot / UEFI boot       No       No       Partially       There is currently no definition of IPv6 support in the PXE hardware agents. UEFI will allow remote boot via IPv6. Or PXE boot the ESXi host over IPv4 and configure the host for IPv6 by using Host Profiles.                 Host Profiles       No       No       Yes                         vAPI       No       No       Yes                         vCLI       No       No       Yes                         vSphere Management SDK       No       No       Yes       Includes: Web Services SDK, SSO Client SDK, EAM SDK, SMS SDK, and SPBM SDK.                 vCloud Suite SDK       No       No       Yes       Includes: SDK for Java, .NET, Python, Ruby, and Perl.                 vSphere Authentication Proxy       No       No       No       The vSphere Authentication Proxy service binds to an IPv4 address for communication with vCenter Server, and does not support IPv6.                 Other Management                                                 vSphere Management Assistant (vMA)       No       No       Yes                         vSphere Update Manager (VUM)       No       No       Yes                         vCenter Client                                                 Windows version       Yes       Yes       Partially       Use of the vSphere Client to enable IPv6 on vSphere features is not supported. Use the vSphere Web Client to enable IPv6 for vSphere features instead.                 Web Client       No support, but may work       No support, but may work       Yes                         ESXi                                                 Transport for Management Communication       Yes       Yes       Yes       Certified via USGv6 certification                 ESXi Firewall       Yes       Yes       Yes                         Netdump       No       No       Yes                         Syslog Agent       Yes       Yes       Yes       Syslog collector / server can be configured via IPv4 address, IPv6 address or DNS name.                 Using Active Directory to authenticate Users       No       No       No       The vSphere Authentication Proxy service binds to an IPv4 address for communication with vCenter Server, and does not support IPv6.                 Network Time Protocol (NTP)       Yes       Yes       Yes       You can use the name or IP address to specify the NTP server (IPv6 address valid for vSphere 4.0 and later)                 SNMP       Yes       Yes       Yes                         Virtual Distributed Switch (vDS)                                                 Link Layer Discovery Protocol (LLDP) / Cisco Discovery Protocol (CDP)       No       No       Yes                         Internet Protocol Flow Information Export (IPFIX)       No       No       Yes                         L3-SPAN       No       No       Yes                         Storage                                                 iSCSI (HW and SW initiator)       No support, but may work       No support, but may work       Yes       While SW iSCSI initiator work in vSphere 5.1 and 5.5, they are not supported. The vSphere 5.1 Storage Guide and vSphere 5.5 Storage Guide explicitly call iSCSI with IPv6 unsupported. Previous versions (e.g. 4.1 had “Experimental support”). vSphere 6.0 does support iSCSI.                 NFS (HW and SW adapters)       No support, but may work       No support, but may work       Partially       While NFS works in vSphere 5.1 and 5.5, they are not supported. The vSphere 5.1 Storage Guide and vSphere 5.5 Storage Guide explicitly call NFS over L3 with IPv6 unsupported, but also make no positive support statement for NFS over L2, therefore making it not supported. Previous versions (e.g. 4.1 had “Experimental support”). In vSphere 6.0 NFS 4.1 storage with Kerberos auth is not supported; use NFS 4.1 with AUTH_SYS instead.                 vStorage APIs for Array Integration (VAAI)       No support, but may work       No support, but may work       Yes       Could work in vSphere 5.1 and 5.5, if the vendor provides an IPv6-capable VAAI plugin. NetApp provides an IPv6 capable VAAI plugin. This combination would not be supported by VMware in vSphere 5.1 or vSphere 5.5. It is supported in vSphere 6.0 for both NFS and iSCSI.                 APIs for Storage Awareness (VASA) / Virtual Volumes (VVols)       No       No       No                         Virtual Storage Area Network (vSAN)                                                 Transport between storage nodes       No       No       No       Virtual SAN does not support IPv6 as transport mechanism between nodes.                 Availability                                                 Fault Tolerance (FT)       No       No       Yes                         High Availability (HA)       Yes       Yes       Yes       vSphere HA supports both IPv4 and IPv6. A cluster that mixes the use of both of these protocol versions, however, is more likely to result in a network partition.                 Symmetric multiprocessing fault tolerance (SMP-FT)                       Yes                         vStorage APIs for Data Protection (VADP)       No       No       Yes                         Dynamic Power Management (DPM)       No       No       Partially       DPM on IPv6 will only support Wake-on-LAN                 Orchestrator (vCO)                                                 Transport for All Communication       Yes       Yes       Yes       All vCenter Orchestrator components, including plug-ins built by VMware, have been tested and certified to run on IPv6 networks.                 Site Recovery Manager (SRM)                                                 SRM to vCenter communication       Yes       Yes       Yes       Site Recovery Manager supports IPv6 for all network links                 vSphere Replication       Yes       Yes       Yes       Site Recovery Manager supports IPv6 for all network links           vCNS / NSX-v                  Product/Function       vCNS 5.5       NSX-v 6.0/6.1       Notes                       VXLAN Transport (Outer Header of a VXLAN Encapsulated Packet)       No       No       The VXLAN specification includes IPv6 for the outer header since revision 2 from February 22, 2013. The VMware implementation does not address IPv6.                 vCNS / NSX Manager IP Address       No       Yes       Only support for static IPv6. No DHCPv6 / Autoconf.                 vCNS / NSX Edge Management IP Address       No       Yes       Only support for static IPv6. No DHCPv6 / Autoconf.                 vCNS / NSX Syslog       No       Yes       Export logs to both IPv4 and IPv6 Syslog Servers.           Related VMware products   The only other VMware product line that supports IPv6 is Horizon View with version 6.1.   Horizon View   With Horizon View the machine/OS can be configured for IPv4-only, IPv6-only or for IPv4/IPv6 dual stack. But Horizon components will not dynamically select IPv4 or IPv6 connections depending on the availability of those protocols on the entities that are involved in making the connection.                  Product/Function       Horizon 5.3       Horizon 6.0       Horizon 6.1       Notes                       VMware Horizon View Components                                                 View Connection Server       No       No       Yes       Support for PCoIP and RDP client connections.                 View Security Server       No       No       Yes       Support for PCoIP and RDP client connections.                 View Agent       No       No       Yes       Can communicate with other View components over IPv6.                 View Client       No       No       Partial       Only the Windows platforms is supported. Mac, iOS, and Android are not supported.                 View Administrator       No       No       Yes       Web Service usable over IPv6 connection. Can manage IPv6-enabled desktops and configure components for IPv6.                 View Composer       No       No       Yes       Allow View to rapidly deploy multiple linked-clone desktops from a single centralized base image, over IPv6.                 VMware Horizon View Integration Interfaces                                                 Event database       No       No       Yes       Interact with MS SQL or Oracle database over IPv6.                 View PowerCLI       No       No       No       Use of IPv6 in PowerCli scripts not supported. Interact with View from PowerCLI over IPv6 not supported.                 Lightweight Directory Access Protocol (LDAP)       No       No       Yes       Interact with LDAP server over IPv6.                 Microsoft System Center Operations Manager (SCOM)       No       No       No       Interact with SCOM server over IPv6 not supported.          ","categories": ["EdgeCloud"],
        "tags": ["IPv6","VMware"],
        "url": "https://www.edge-cloud.net/2015/03/18/ipv6-in-vsphere-6/",
        "teaser":null},{
        "title": "VMware vRealize Automation workloads with IPv6",
        "excerpt":"Unfortunately VMware’s primary Cloud Management Platform (CMP) for the Enterprise, VMware vRealize Automation (vRA) does not support IPv6-enabled workloads out of the box. This applies to IPv6-only workloads as well as IPv4/IPv6 Dualstack workloads.   The reason for this shortcoming can partially be found within vRA, which lacks the ability to manage the lifecycle of an IPv6 enabled VM as well as associated networks. But it is also due to the lacking support of automatic IPv6 address assignment towards VMs in VMware NSX, which provides advanced network capabilities in vRA.   With various workarounds it is nevertheless possible to utilize VMware vRealize Automation with IPv6-enabled workloads. In this post I want to present three different approaches of such workarounds. They will focus on the NSX Edge device, as one of the main inhibitors and can be described with the following high-level themes: a) Remove, b) Replace, c) Script on top&lt;/li&gt;   vRealize Automation Network profiles   VMware’s vRealize Automation provides four different kinds of “network profiles”, which become part of a blueprint and therefore determine how the Virtual Machines within such a blueprint are connected to the network.   Let’s have a brief look at each of them, in order to understand where to start with possible workarounds:   Private   Here the VMs of an instantiated blueprint would not have outbound connectivity, but instead all networking would be isolated (contained) within the instantiated blueprint. Such a network can include a virtual router but does not have to. For our IPv6 with VMware vRealize Automation (vRA) use case this profile is irrelevant (see Figure 1).                              Figure 1: vRA Network Profile: Private     NAT   Here the VMs of a blueprint connect to a virtual router – which becomes part of the blueprint. This virtual router connects to a (physical) upstream router. It connects the blueprint internal networks to the outside world via Network Address Translation (NAT).   Important to point out here, that the NAT Gateway will be part of the blueprint, while the upstream router is not.   This profile is also irrelevant for our IPv6 with VMware vRealize Automation (vRA) discussion as we do not want to (and realistically cannot) use NAT with IPv6 (See Figure 2).                              Figure 2: vRealize Automation Network Profile: NAT     Routed   This profile is very similar to the NAT profile, except that the virtual router does not provide NAT gateway capabilities, but instead routes the subnets (In the below figure it’s 3 of them: “Web”, “App”, and “Database”) to the upstream router via static, or better dynamic routing (See Figure 3).                              Figure 3: vRealize Automation Network Profile: Routed     When it comes to IPv6, we will have to work with the following limitations of VMware NSX in this scenario:      Static routing only: The Logical router above will only be able to use static routing, but no dynamic routing with IPv6.   Router address assignment: The logical router above will not be able to learn an IPv6 address via DHCPv6 or SLAAC for itself. Router interfaces here have to be configured manually.   VM address assignment: The virtual machines above will not be able to learn an IPv6 address via DHCPv6 or SLAAC from the Logical Router as the Logical Router cannot act as a DHCPv6 server or relay.   As a side note: With IPv4, vRA will maintain a pool of IPv4 subnets, assign them to an instantiated blueprint and configure the dynamic IPv4 routing. This is not possible today with IPv6.   External:   Here all VMs in an instantiated blueprint would connect to pre-deployed network segments (See Figure 4).                              Figure 4: vRealize Automation Network Profile: External     For this approach various options exist:      These network segments can either be physical network (VLANs) or virtual networks (VXLANs). What they have in common: They need to be pre-deployed outside of vRA.   All VMs in a Blueprint can be attached to the same network or to different networks (see the three network example above).   Irrespective of how many networks the VMs are connected to, VMware NSX’ logical firewall can be used to create an experience similar to AWS security groups, where VMs are placed into a security group that enforces a coherent set of security rules.&lt;/ul&gt;   Possible workarounds for IPv6   Now that we have set the stage with the above network profiles, we can look into possible ways to make this work. The following possibilities should be considered in this specific order due to the associated complexity and gained benefits:   External profile with external address assignment  In this option network segments (either VLANs or VXLANs) would need to be setup outside of vRA. These network segments would provide the ability to handout IPv6 addresses via DHCPv6, SLAAC, or a combination of both from a physical router.                              Figure 5: Address assignment via DHCPv6 from upstream router     This approach has various levels of possibilities that could be used:      Flat network with single L2 segment vs. multiple function-based network segments (see 3 tier example above).   Use of NSX Distributed Firewall for Security groups or not. This model would be very similar to the traditional AWS EC2 Networking (the network design that was in place before AWS VPC was introduced).   Pro: Address Management is completely outside vRA and can be handled by traditional hardware routers and IPAM solutions (e.g. Infoblox)   Con: Limited possibilities for network design as part of a blueprint / architecture.   External profile with 3rd party software router   In this option you would try to emulate the “Routed” profile with a 3rd party software router (basically a small Linux VM that is pre-configured for IPv6 and/or can accept configuration changes via a self-made API).   In this case you would especially want automate the IPv6 address assignment for the VMs living on the network segments within the blueprint as much as possible (See the example with the 3 segments in the figure below). A very elegant way to do this would be to look at the way how major Service Providers (e.g. AT&amp;T or Comcast) assign IPv6 addresses to their customer gateways (CPE). This address assignment can be done via DHCPv6-PD (Prefix Delegation) (See Figure 6).                              Figure 6: Logical Router acting as DHCPv6-PD CPE device     Looking at the example above, here is what would happen in this scenario:      The 3rd party virtual router would request via DHCPv6-PD a /56 prefix for the entire blueprint.   Each blueprint internal network segment (“Web”, “App”, and “Database” in the example above), would receive a /64 segment out of the assigned /56 segment.   Address assignment to VMs on the blueprint internal network segments could happen via DHPCv6, SLAAC, or a combination of both.   Dynamic routing would not be necessary. Each internal network segment uses the 3rd party virtual router as Def. gw. The 3rd party virtual router uses the upstream router as the Def. GW.   The physical upstream router has a static route for the /56 network pointing to the 3rd party virtual router (this is done automatically by the route as part of DHCPv6-PD.   If desired it is possible to run additional services (e.g. load balancer, firewall via IPtables) on this 3rd party virtual router and make configuration accessible from vRA workflows via a simple custom API. As an alternative it would be possible to use a specialized Linux router distribution such as OpenWRT and allow configuration of the router from within the Blueprint via a Web GUI.   Pro: Regain advanced possibilities for network design within a blueprint / architecture, while allowing address assignment to all VMs via DHCPv6 and/or SLAAC. Leverage SP-proven network concepts to treat applications like customer networks.   Con: Need to engineer and maintain a 3rd party virtual router based on a standard Linux distribution.   Route profile with vRA workflow based address assignment   In this option address assignment to the end-user VMs, but also the virtual router would not happen via DHCPv6 or SLAAC, but statically. In the case of the NSX Edge based virtual router this would need to happen via API calls to the NSX Manager and in the case of the end-user VMs this would happen via Guest Customization through the VMware Tools. As none of these assignment capabilities, as well as the capability to manage pools of IPv6 prefixes and sub-prefixes exist in vRA today, this functionality would have to be written via vRA workflows.   On the other hand, this approach would allow to use all of the NSX Edge based network services (load balancer, firewall) for IPv6, although configuration of these would need to happen via vRA workflows as management capabilities of these services doesn’t come out of the box via vRA.   Pro: Ability to use the NSX based Edge device as router and for network services   Con: Requirement to manage IPv6 addresses statically and maintain IPv6 prefix pools manually. Need to custom develop missing software capabilities via custom workflows in vRA.   One important thing to keep in mind: Any of the L2 network transport capabilities (vDS, vSS, VXLAN) in vSphere will transport IPv6 traffic. This is different from e.g. AWS, where network segments that look like L2 (e.g. VPC subnet) are not actually L2 and will filter out any kind of IPv6 traffic. The focus of the workaround is therefore almost exclusively on the L3 element.   Summary   This post hopefully showcased that albeit not trivial, it is possible to leverage VMware vRealize Automation with IPv6-enabled workloads, through various workarounds.  ","categories": ["EdgeCloud"],
        "tags": ["IPv6","NSX","VMware"],
        "url": "https://www.edge-cloud.net/2015/05/26/vmware-vrealize-automation-workloads-with-ipv6/",
        "teaser":null},{
        "title": "TYPO3 with Cloudflare",
        "excerpt":"Cloudflare provides a content delivery network and distributed domain name server services to help secure and accelerate websites. This cloud-based service sits between the visitor and the CloudFlare user’s hosting provider, acting as a reverse proxy for the website. While the majority of web content management systems have no problem with such an approach out of the box, TYPO3 is different. There are some minor settings that need to be changed for this combination to work. This article will show you how to accomplish this.                              Figure 1: Speed-up and protect your website with CloudFlare     TYPO3 is a free and open source web content management system written in PHP, which is more widespread in Europe than in other regions. The biggest market share can be found in German-speaking countries.   Support for HTTPS   Out of the box Cloudflare provides free SSL support for every website, allowing user to provide HTTPS for their website. This offering is called UniversalSSL and it can be used without the origin web server even supporting SSL.   As a result you could access a website like example.com under http://www.example.com as well as https://www.example.com.   Most content management system allow a website to be accessed both via HTTP and HTTPS by using relative links to include content, such as style sheets or images. TYPO3 unfortunately traditionally uses absolute URLs, which leads to a broken website when visiting a TYPO3 instance that was setup for HTTP via HTTPS or vice versa.   While this has been fixed in newer version of TYPO3, this legacy behavior can still be found in older versions or after upgrades where the configuration wasn’t adapted accordingly.   It is very easy and straight forward to change this behavior though. Doing so will instruct TYPO3 to change the Base URL depending on how the content was accessed via Cloudflare, acting as a Reverse Proxy. This means that if an end-user connects to a website via Cloudflare and HTTPS, the Base URL in TYPO3 will include https://, even though the origin server was contacted via HTTP by Cloudflare.   Use the following Typoscript with Condition inside your main template.       config.baseURL = http://www.example.com/     [globalString = ENV:HTTP_X_FORWARDED_PROTO=https, ENV:HTTPS=on]     config.baseURL = https://www.example.com/     [global]   Don’t forget to change the sample URL www.example.com with your actual domain.   The better alternative would be to use the more current config.absRefPrefix capability instead of the legacy config.baseURL. This configuration item would instruct TYPO3 to use relative URLs instead of absolute URLs.   Ideally you should configure your origin web server with an SSL certificate to enable HTTPS between Cloudflare and your server. For this it is sufficient to use a self-signed certificate.   Also after enabling TYPO3 for UniversalSSL, you can force usage of HTTPS on all your pages.   Support for the TYPO3 backend   The TYPO3 backend is another troublemaker in combination with a reverse proxy such as Cloudflare. In older versions the backend would only display a blank page after successful login. While this error has been corrected in recent versions, another challenge arises when using the backend via HTTPS. As mentioned earlier, with Cloudflare you can access your backend via HTTPS over Cloudflare as a reverse proxy, without requiring your origin server to support HTTPS. Unfortunately in this case TYPO3 will always attempt to switch back to HTTP as the backend was contacted by Cloudflare via HTTP.   But there is also an easy fix for this behavior. TYPO3 allows the configuration of an SSL Proxy, which is what Cloudflare basically behaves as here.   Just add the following entries to your typo3conf/localconf.php file.       $GLOBALS['TYPO3_CONF_VARS']['SYS']['reverseProxyIP'] = '*';     $GLOBALS['TYPO3_CONF_VARS']['SYS']['reverseProxyHeaderMultiValue'] = 'first';     $GLOBALS['TYPO3_CONF_VARS']['SYS']['reverseProxySSL'] = '*';     $GLOBALS['TYPO3_CONF_VARS']['SYS']['trustedHostsPattern'] = '(www.)?example.com';   Don’t forget to change the sample URL www.example.com with your actual domain.   CloudFlare extension for TYPO3   While not mandatory to operate CloudFlare with a TYPO3 based website, it is highly recommended to use the CloudFlare extension for TYPO3. This excellent extension allows you to flush the CloudFlare cache, restores the origin IP address of the end-user towards TYPO3 and takes care of the above mentioned SSL setup transparently.   Summary   Cloudflare is an ideal and free addition for anyone running a website. And while especially older TYPO3 installs might not work smoothly with Cloudflare out of the box, it is trivial to change this with a few small changes.  ","categories": ["EdgeCloud"],
        "tags": ["Cloudflare"],
        "url": "https://www.edge-cloud.net/2015/07/20/typo3-with-cloudflare/",
        "teaser":null},{
        "title": "SDDC Architecture – Regions and Availability Zones (AZs)",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC as well as some of its design elements. In this post we want to look at the physical layer of our SDDC architecture (See Figure 1).                              Figure 1: Physical Layer in the SDDC Architecture     Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Physical Layer Design Artifacts   Within the Physical Layer we want to look at these design artifacts in more detail:      Availability Zones and Regions: Provide continuous availability of the SDDC, minimize unavailability of services and improve SLAs via Availability Zones, while using Regions to improve locality of SDDC resources towards end-users.   Introduction   Availability Zones are a concept to provide continuous availability for the SDDC, minimize unavailability of services and improve service levels. Multiple Availability Zones form a single region.   The core concept behind availability zones is that the likelihood of external factors (power, cooling, physical integrity) affecting an outage in one zone, also leading to an outage in the other zone would be extremely rare (major disasters only) or de-factor impossible. As such deploying workloads (especially management capabilities) across two zones would yield much higher availability and result in improved service levels.   Differentiation   The differentiation between Availability Zones and Regions is very much driven by the physical distance and available bandwidth between two sites. The short distance and high bandwidth between Availability Zones allows the use of synchronous replication between storage arrays deployed across the Zones. The result is the ability to operate workloads across multiple Availability Zones (within the same Region) as if they were part of a single site. This enables designs with very high availability that are suited to host mission critical applications.   Once the distance between two sites becomes too large, these site can no longer function as two Availability Zones within the same Region and instead need to be treated as separate Regions (See Figure 2).                              Figure 2: SDDC Availability Zone concept     Availability Zones   Each availability zone (AZ) runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Each zone should have independent power, cooling, network and security. Common points of failures within a physical data center, like generators and cooling equipment, should not be shared across Availability Zones. Additionally, these zones should be physically separate; such that even extremely uncommon disasters such as fires, tornados or flooding would only affect a single Availability Zone. As such Availability Zones are usually either two distinct data centers within metro distance (latency in the single digit range) or two safety/fire sectors (aka data halls) within the same large scale data center.   Multiple Availability Zones (usually two) belong to a single Region, where the physical distance between Availability Zones is below 50 km or 30 mi, therefore offering low single digit latency between Availability Zones, along with large bandwidth - e.g. via dark fiber - between the Zones. This allows the SDDC equipment across the Availability to operate in an active/active manner as a single “Virtual Data Center” or region (See Figure 3).                              Figure 3: SDDC Availability Zone design     Regions   The distance between Regions is usually rather large, as having multiple regions caters to a different use case. With multiple regions you can place workloads closer to your customers - e.g. by operating one region on the US East coast along with one region on the US West coast, or operating a Region in Europe and another region is North America. This reduces latency and improves user experience. Regions are also suited to deploy Disaster Recovery (RD) solutions with one Region being the primary site and another Region being the recovery or bunker site.   Last but not least multiple regions can be used to address data privacy laws and restrictions in certain countries and regions, by ensuring that tenant data is kept within a region inside the same country.   Multiple Regions are not suited to be operated as a single virtual data center and rather need to be treated as separate SDDC instances.   Summary   The concept of Availability Zones and Regions is highlighted, which provides a toolset for improving continuous availability of the SDDC, thereby especially catering to the requirement of operating legacy applications with traditional business continuity and disaster recovery needs.   In a nutshell: An availability zone is an “islands” of infrastructure that are isolated enough from each other to stop the propagation of failure or outage across their boundaries. A region brings workloads closer to end-users and serves the purpose of disaster recovery for business continuity.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/07/31/sddc-architecture-regions-and-availability-zones-azs/",
        "teaser":null},{
        "title": "SDDC Architecture – Basic Design Elements",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC as well as some of its design elements.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Introduction   The SDDC architecture is based on five basic design elements, which we will cover in this post in more detail (See Figure 1).                              Figure 1: Basic SDDC Design Elements     These design elements allow us achieve the previously stated design goals for an SDDC and are crucial to its success. They allow us to create a simple, yet powerful design.   Layered logical model   Basing the SDDC on a layered logical model enables a very high level of modularity (See Figure 2).                              Figure 2: Layered Architecture of a Software Defined Data Center     With this approach it is possible to replace the Cloud Management System (CMS) or even to run multiple CMS at the same time, on top of the same Virtual Infrastructure.   This means that VMware Integrated OpenStack (VIO) could easily replace VMware vRealize Automation as the CMS. As this change solely happens in the Cloud Management Layer, no changes are necessary to the underlying Virtual Infrastructure Layer or any other layer.   Or in another scenario it would be possible to run both VMware vCloud Director (vCD) and VMware Integrated OpenStack in parallel within the Cloud Management Layer on top of the same Virtual Infrastructure Layer.   It’s important to point out that in this case workloads would be exclusively managed by one of the two CMS.   POD / Core concept   The POD and core concept leverages individually designed point-of-delivery (POD), which connect to a common routed core layer. The routed core spans multiple PODs of different type and generation, treating them as an atomic building block and providing fast and simple interconnect.   This approach of scaling out in concrete chunks matches the incremental demand of modern data centers: PODs are designed, engineered, installed and retired as a distinct unit. As such an SDDC can comprise multiple generations of a pod, sitting next to each other, attached to the same shared core and allow an iterative approach, where each pod generation improves on the previous one.   Further splitting PODs by functional capability into Storage POD, Management POD, Compute POD and Network Edge POD, you also gain the ability to easily scale your SDDC capacity and capability based on demand, leading to even higher flexibility (See Figure 3).                              Figure 3: POD / Core concept with L3 Spine / Leaf network     L3 Spine / Leaf network   For many years the predominant data center network design was the three-tier approach of a Core (Layer 3), Aggregation (Layer 2 / Layer 3), Access (Layer 2) tiers. This design approach has been very successful over the last 20 years, allowing network architects to design highly reliable and scalable networks, recent industry developments are breaking the underlying assumptions:   The three-tier network assumes a significant price difference between network device capable of Layer 3 routing vs. Layer 2 Switching, which is no longer the case. It further assumes that traffic is primarily exchanged between servers and the outside world (north-south traffic), while today server-to-server traffic (east-west traffic) is more common. Last but least it assumes that the interface speed at the Core and Aggregation tier is significantly higher than in the access layer, which also isn’t true anymore thanks to the prevalence of 10 Gigabit Ethernet equipped servers.   At the same time the traditional three-tier design fails to address innovation in modern data centers through its lack of modularity and rigidness. This hampers fast iterations and experimentation, while also preventing to keep up with price/performance improvements in the industry.   Consequently the time has come for a new approach to data center network design, based on hyper- or webscale data center center design. Despite their massive scale, hyper- or webscale data centers leverage a network design that starts small and innovates quickly. As such the key differentiator is that a Spine / Leaf network supports you to grow organically in incremental blocks of capacity, starting small each time, while at the same time allowing you to scale almost beyond imagination. Such an approach works very well hand-in-hand with the above mentioned POD / Core concept.   Management Applications Network Container   With the network containers for management applications we place each management application in a dedicated network container with an NSX Edge acting as firewall, load balancer, and gateway of that container. With the previously presented management applications this means that we have one container for VMware vRealize Automation, another one for vRealize Operations and a third one for vRealize LogInsight.   These network container then connect to a “business network” on which end-user facing services - such as the vRealize Automation Web interface - are presented, as well as a “management network” via which infrastructure admins connect. Nevertheless components within each network container are protected against these two networks, which present two different trust zones, besides the network container as a third trust zone.                              Figure 4: Object-oriented design: Object     To draw an analogy of what this means: Let’s look at the Object Oriented (OO) design approach, well known from software engineering (See Figure 4).   With the SDDC network container, we want to enforce access to services through the front door, via the Load Balancer. In OO this is equivalent to accessing an object via methods.   With the network container we want to prevent access through the “backdoor” or direct node access for end-users. In OO this is equivalent to accessing attributes directly, which would violate the OO concept.   As a result network container provide us with a few benefits:      Security: Granular yet simple control who can access what kind of service through the “front door” (load balancer).   Modularity: Replace or upgrade an application without breaking the entire SDDC as dependencies are established via well-know interfaces.        Simplicity: Reduce the number of required IP addresses on the business network, keeping the integration effort low and enabling deployment on the Internet.       In other words: Keep it simple. No dynamic routing towards the corporate network is necessary. As a result we have less components and less configuration.       BC/DR: Simplifies the Business Continuity/Disaster Recovery story with VMware Site Recovery Manager.   Service Level tiers   Especially with the ambition to address new and old business needs via the support of legacy applications with traditional business continuity and disaster recovery needs, in addition to new cloud-native applications, a one-size fits all approach doesn’t work anymore. Such an approach would either become prohibitively expensive by providing all workloads with the same level of high end treatment or would fall short on the provided business value through capabilities.   Yet at the same time you also want to limit customization in order to be able to maintain self-service capabilities, as each customization adds complexity and cost.   The solution here is to use a small set of distinct service level tiers. Each service tier implements a certain capability along the spectrum of capabilities. As such it would e.g. be possible to workloads different data stores with varying IOPS.   Another example would be the ability to offer three levels of backup capabilities: The lowest tier, usually called bronze, would offer no backup whatsoever. The next tier, usually called silver, would offer backups within the same region. The most advanced tier, usually called gold, would offer backups within and across regions.   Summary   The ultimate goal of the SDDC is to provide business value through a simple, stable and understandable architecture.   The above presented basic design elements of the SDDC not only allow us to clearly communicate the elements and their benefits of the SDDC, but are crucial for leveraging these benefits in a real-life SDDC installation.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/08/04/sddc-sddc-architecture-basic-design-elements/",
        "teaser":null},{
        "title": "Building a Cumulus Networks VX cloud lab with Ravello Systems",
        "excerpt":"In a previous post I’ve already shown how to build a virtual network lab with Arista vEOS and VMware ESX. In this post we want to take this concept even further to the next level. Instead of Arista vEOS we will use the newly released Cumulus Networks Virtual Experience (VX) edition. And instead of VMware ESX we will use Ravello Systems, allowing us to use AWS or Google Compute Engine.                              Elements used   Cumulus Networks provides a standard Linux based operating system for data center switches, thus simplifying dramatically the data center operations. If you are familiar with running and operating Linux on a regular server, managing a Cumulus Network based switch shouldn’t be a big challenge for you. Also you can now finally leverage the hundreds of existing management, automation and monitoring tools that you know and love for Linux. Cumulus Networks Virtual Experience (VX) is a community-supported virtual appliance that enables cloud admins and network engineers to preview and test Cumulus Networks technology at zero cost inside a virtual machine or standard X86 server.   Ravello Systems provides a Smart Labs solution on top of AWS and Google Compute Engine (GCE) based on nested virtualization. This allows you to run self-contained lab “capsules” with almost any kind of VMware or KVM based virtual machine in them. This approach - proven for years with the VMWorld labs - now allows you to use modern public clouds for test, training and demo purposes at a minimal cost. Specific for networking lab cases Ravello Systems provides virtual L2 networking on top of both AWS and GCE, allowing you to simulate very complex network scenarios.   Architecture   For this exercise we want to build a very simple two-leaf/two-spine virtual network architecture (See Figure 1).                              Figure 1: Architecture for Cumullus Networks VX lab     The architecture will include two spine switches (spine1 and spine2) as well as two leaf switches (leaf1 and leaf2), cross connected in a typical CLOS network architecture. For the four point-to-point connections between the devices we will use the depicted RFC1918 IP ranges, while making the management interface of the switches available over the internet (not depicted).   Getting Started   Before you can get started you need an account for Ravello Systems. You can conveniently sign-up for a free trial account. This account will allow you to use Ravello for free for 14 days. You do not need a credit card or any existing cloud credentials with AWS or GCE.   Next we need to download the Cumulus Networks VX images in QCOW2 format. This image will later be imported into Ravello Systems.   If you want to save time you can also use my pre-built Ravello Systems blueprint for Cumulus Networks VX. See further down for details.   Uploading the Cumulus Networks VX image into Ravello Systems   Once you have received your login credentials for your Ravello Systems account, you can start your cloud lab experience by uploading the Cumulus Networks VX images - in QCOW2 format - into Ravello Systems (See Figure 2).                              Figure 2: Uploading the QCOW2 Cumulus Networks VX image     You can leave most of the settings as default, when importing the VM. Please note that you will need to add two network interfaces, resulting in a total of three network interfaces. It is recommended to call the first interface “eth0”, the second interface “swp1”, and the third interface “swp2”.   Creating an application   Next create an “application” within Ravello Systems that maps to your desired network architecture. Place four instance of the previously imported Cumulus Networks VX image into this application (See Figure 3). It is recommend to position them in a similar fashion to the network diagram in Figure 1. This will provide you a better overview and simplify the configuration.                              Figure 3: Basic application with 4 Cumulus Networks VX devices     Next choose the first VM in your Ravello Systems application and give it a meaningful name (See Figure 4).                              Figure 4: Naming a Cumulus Networks VX instance     Also configure the interfaces “swp1” and “swp2” according to the network diagram in Figure 1. For this you have to change the IP configuration for each of the two interfaces from DHCP to static and enter the static IP address as well as network mask (See Figure 5).                              Figure 5: Network Configuration for Cumulus Networks devices     Entering these IP information will not actually configure the interfaces correspondingly inside the Cumulus Networks VX device. Instead it will instruct Ravello Systems to place the interfaces on separate network segments (See Figure 6).   You should be able to see that by default all network interfaces would reside on the same network segment as it is the case for the so far unconfigured devices “Cumulus VX1”, “Cumulus VX2”, and “Cumulus VX3”.   For the configured device “Cumulus VX - Spine 1”, the interfaces “swp1” and “swp2” have already been moved to separate network segments.                              Figure 6: Mix of configured and unconfigured Cumulus Networks VX devices     The default network segment 10.0.0.0/16 should remain connected to eth0 as it will provide us inbound and outbound Internet connectivity and therefor allow us to connect via SSH later.   Repeat the above steps on all Cumulus Network VX devices and provide each instance with a unique name and the network interface configuration corresponding to the network map in Figure 1.   The resulting network map within Ravello Systems should look like depicted in Figure 7.                              Figure 7: Complete network map for Cumulus Networks VX devices     You should see the four point-to-point network segments with two connections per segment.   The fifth network segment, connecting to all devices provides external connectivity via the router symbol depicted at the bottom. This network segment connects to the management port “eth0” on the Cumulus Networks VX device.   Providing access via SSH   If you select a single Cumulus Networks VX instance within your Ravello Systems application, you can see that by default this VM only has outbound network connectivity via the eth0 interface (See Figure 8).                              Figure 8: Cumulus Networks VX without inbound access     Once deployed you will be able to access each of the Cumulus Networks VX devices via the build-in Ravello Systems console. While the Ravello Systems console is a great way to interact with virtual machines, we also want to be able to use SSH access for accessing each of the Cumulus Networks VX devices. This will simplify the configuration in subsequent steps and feel more natural.   For doing so, you have to add a “Service” for each of the VMs within your Ravello Systems application and configure it for SSH with port TCP/22 against the interface DHCP. The interface DHCP corresponds to eth0 (See Figure 9).                              Figure 9: Configure an inbound SSH service     Returning to the Canvas view within Ravello Systems, you will see the four Cumulus Networks VMs with the SSH service configured (See Figure 10).                              Figure 10: Application with four Cumulus Networks VX devices     Deploy the network testbed   Next we want to deploy - called “Publish” in Ravello Systems terms - the network testbed to either AWS or GCE, so that we can use it. To do so, just hit “Publish” and confirm the default settings to publish the testbed in the most cost effective location (See Figure 11).                              Figure 11: Deploy the Ravello Systems application     By default the tesbed will only be deployed for 2 hours. After this it will be shut down automatically. If you need your testbed longer, adapt the time accordingly.   Accessing the network testbed   After a few minutes your testbed should be published to either AWS or GCE and each VM should be up and running. At this point you can connect via SSH to each of the Cumulus Networks VX nodes.   Use the hostname or IP address shown for each VM within the Ravello Systems Console along with the username “cumulus” and the password “CumulusLinux!” (See Figure 12).                              Figure 12: Accessing Cumulus Networks VX nodes via SSH     Keep in mind, that the Cumulus Network VX nodes are still unconfigured at this point. The IPv4 address configuration for the VMs in Ravello Systems was only for generating the different networks segments within Ravello Systems and does not apply to the network configuration within the Cumulus Networks VX node.   Basic network testbed configuration   In this step we want to perform basic network configuration and at least enable the Cumulus Networks VX nodes to communicate with each other. For this we will configure all “swp” interfaces on all devices as a routed port with the IPv4 address depicted in Figure 1.   We will start by connecting to Spine1 via SSH. Next configure the IPv4 addresses via editing the file “/etc/network/interfaces”. You can do so using “sudo” and your favorite browser:   cumulus@cumulus$ sudo vi /etc/network/interfaces   Add the following lines to the end of the file:   auto swp1 iface swp1         address 10.1.0.1  auto swp2 iface swp2         address 10.2.0.1   Next restart the Cumulus Networks VX networking for the configuration to be applied:   cumulus@cumulus$ sudo service networking restart   Repeat the above steps for the other three devices. Remember to adapt the IPv4 addresses in the file “/etc/network/interfaces” for the interfaces “swp1” and “swp2” according to the network diagram in Figure 1.   Testing your network setup   Test the functionality of your basic network lab. First look at the available interfaces and their IP addresses via standard Linux commands:   cumulus@spine1$ ifconfig eth0      Link encap:Ethernet  HWaddr 2c:c2:60:46:1e:68           inet addr:10.0.0.5  Bcast:10.0.255.255  Mask:255.255.0.0           inet6 addr: fe80::2ec2:60ff:fe46:1e68/64 Scope:Link           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1           RX packets:181 errors:0 dropped:0 overruns:0 frame:0           TX packets:120 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:1000           RX bytes:22332 (21.8 KiB)  TX bytes:15935 (15.5 KiB)  lo        Link encap:Local Loopback           inet addr:127.0.0.1  Mask:255.0.0.0           inet6 addr: ::1/128 Scope:Host           UP LOOPBACK RUNNING  MTU:16436  Metric:1           RX packets:1 errors:0 dropped:0 overruns:0 frame:0           TX packets:1 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:0           RX bytes:28 (28.0 B)  TX bytes:28 (28.0 B)  swp1      Link encap:Ethernet  HWaddr 2c:c2:60:46:43:f7           inet addr:10.1.0.1  Bcast:0.0.0.0  Mask:255.255.255.0           inet6 addr: fe80::2ec2:60ff:fe46:43f7/64 Scope:Link           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1           RX packets:7 errors:0 dropped:0 overruns:0 frame:0           TX packets:6 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:1000           RX bytes:541 (541.0 B)  TX bytes:652 (652.0 B)  swp2      Link encap:Ethernet  HWaddr 2c:c2:60:32:7b:88           inet addr:10.2.0.1  Bcast:0.0.0.0  Mask:255.255.255.0           inet6 addr: fe80::2ec2:60ff:fe32:7b88/64 Scope:Link           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1           RX packets:8 errors:0 dropped:0 overruns:0 frame:0           TX packets:6 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:1000           RX bytes:662 (662.0 B)  TX bytes:652 (652.0 B)  cumulus@spine1$   For each device within your testbed you should see the following four interfaces:      eth0: IPv4 address on the 10.0.0.0/16 network, providing you inbound and outbound network connectivity   lo: Loopback interface with IPv4 address 127.0.0.1/8 and IPv6 address ::1/128   swp1: Switchport interface with IPv4 address, connected to another spine or leaf switch   swp2: Switchport interface with IPv4 address, connected to another spine or leaf switch   Next verify if you can ping the adjacent interface of another connected switch. From the device “Spine1” we can try to ping the interface “swp1” with the IPv4 address 10.1.0.2 on the switch “Leaf1”:   cumulus@spine1$ ping -c 3 10.1.0.2 PING 10.1.0.2 (10.1.0.2) 56(84) bytes of data. 64 bytes from 10.1.0.2: icmp_req=1 ttl=64 time=2.67 ms 64 bytes from 10.1.0.2: icmp_req=2 ttl=64 time=0.892 ms 64 bytes from 10.1.0.2: icmp_req=3 ttl=64 time=0.769 ms  --- 10.1.0.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 0.769/1.445/2.676/0.872 ms cumulus@spine1$   The pings should succeed.   Save time, use a Ravello Systems Blueprint   Instead of going through the above steps you can also use my pre-built Ravello Systems blueprint for Cumulus Networks VX. Just add this public accessible blueprint to your own Ravello Systems library and deploy the above described network lab in a few minutes.   More capabilities   Cumulus Networks VX provides a lot more capabilities than what I was able to showcase in this blog post. With the above described simple testbed you are now empowered to explore these capabilities.   Have a look at the Cumulus Network Technical Documentation to get inspired on what you can do.  ","categories": ["EdgeCloud"],
        "tags": ["Cumulus-Networks","Network","Ravello-Systems"],
        "url": "https://www.edge-cloud.net/2015/08/21/building-a-cumulus-networks-vx-cloud-lab-with-ravello-systems/",
        "teaser":null},{
        "title": "SDDC Architecture - What is a VMware validated design?",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC via VMware Validated Designs.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   What is a VMware Validated Design?   A VMware Validated Design is a comprehensive design for a Software Designed Data Center (SDDC). It includes everything needed for a fully functional Software Defined Datacenter covering a set of use cases – yet remains hardware-agnostic. Through automated testing its functionality is continuously validated with every new build or any component.   VMware Validated Designs are designed by a team of VMware expert, continuously improved based on feedback from real deployments.   Have a look at this VMware video that explains what a VMware Validated Design is, some common components across all VMware Validated Designs, and how the VMware Validated Design process makes deploying a Software-Defined Datacenter a streamlined experience.   Why do you need a VMware Validated Design approach?   After having seen what a VMWare Validated Design offers, you might ask yourself why you would need this and what benefit it would bring.   For this let me use an analogy to describe how building an SDDC based data center with VMware products looks like today:   As an IT organization you are confronted with a whole lot of different pieces, while dealing with VMware data center products. While these pieces are not car pieces, the picture in Figure 1 will certainly raise some memories and associations. But not only do IT organizations end up with a lot of different pieces. It’s also very challenging to put them together. They might not even fit.   And to take this analogy a bit further: You often end up with the puzzle pieces, but without the picture on the box that shows you how the final result should look like.                              Figure 1: The challenge - SDDCs come in lots of pieces     To be clear: The Validated Design is not just the picture on the puzzle box and is also not just a step by step guide on how to fit the puzzle pieces together. It is more than that. Much more.   Let me again use an analogy and return to the car example. Looking at how the Volkswagen Group, which besides Volkswagen also includes Audi as well as brands that are less known in the US, one will notice that they came up with something quite remarkable: A validated base design for a car, where the engine would be mounted sideways (See Figure 2).                              Figure 2: Volkswagen’s MQB car platform     This approach enables engineers to innovate faster, much faster. Instead of literally re-inventing the wheel for every new car model, teams can leverage this base design and cut short on the undifferentiated heavy lifting. As an alternative engineering teams can then focus on the differentiation and build a concrete instance on top of this base design (See Figure 3).   This means that the MQB is actually the foundation of quite different vehicles, ranging from the VW Passat as a sedan to the tiny VW UP!, which is not known in the US.                              Figure 3: Volkswagen’s MQB as platform for innovation     The VMware Validated Design has a very similar goal: Provide a validated design as the underpinning of the Software Defined Data Center (SDDC), that is on one side uniform enough to provide sufficient guard rails around implementation guidance and underlying validation. On the other hand the VMware Validated Design provides sufficient flexibility to be the common underpinning of many different SDDC instances and as such be the common underpinning of various VMware IT Outcomes initiatives.   As such an SDDC based on a VMware Validated Design enables you as an IT organization to become a strategic partner and innovator to the business.   Keep it simple!   An SDDC based on the VMware Validated Design should serve as a simple and stable underpinning, providing clear business value. You as an IT organization should be empowered to clearly articulate these benefits to the business but also achieve the promises in real life. This is a key driver behind keeping the VMware Validated Design simple and understandable (See Figure 4).                              Figure 4: Simplicity as the basis for a SDDC     Keep in mind: Just because something can be done, doesn’t mean it should be done. The quality of a design does not increase with the number of elements. Instead with an increase of elements the complexity increases, which usually leads to a poor design.   Summary   With the VMware Validated Designs you can get the undifferentiated heavy lifting out of the way, while building SDDC based data centers and focus on innovation and adding value through differentiation.   Want more?   Attend the VMWorld 2015 sessions SDDC5440 - VMware Validated Designs - A Reference Architecture for the SDDC and SDDC5609 - VMware Validated Designs for a Software Defined Data Center (SDDC) to learn more about the VMware Validated Designs.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/08/27/sddc-architecture-what-is-a-vmware-validated-design/",
        "teaser":null},{
        "title": "SDDC Architecture - Virtual PODs for Management applications",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC via VMware Validated Designs.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   The motivation   The concept of the Management Stack Virtual POD can be considered as part of the secret sauce for the VMware Validated Designs. This is a concept only found in the management stack and aims at providing SDDC management application with low integration efforts, while treating them as sub-systems and providing advanced security.   One of the main ideas behind this concept is depicted in Figure 1:                              Figure 1: Virtual POD for SDDC Management Applications     The SDDC based on a VMware Validated Design use two main network segments:      Business Network: This network must be connected to the corporate network of the organization where the SDDC is being deployed. Otherwise end-users and tenant admins of the SDDC have no ability to access components such as VMware vRealize Automation. As a result the SDDC would not be able to deliver any business value.   Management Network: Only infrastructure-admins should have access to this network. In fact end-users and tenant-admins should not have access to this network at all. The VMWare Validated Design supports various options for connecting to this network, where the best choice depends on the use cases for the SDDC within the organization where it is deployed:            Directly connect to the corporate network via static or dynamic routing with a firewall in between       Connect via MPLS from off-site for a managed services offering. Turns out that a VMware Integration Partner, delivering VMware SDDC today, follows exactly this path.       Provide a Jump-Host for more secure access from the corporate network       Many other ways to connect admins to admin network           SDDC management workloads are placed on the management network and fronted with a load balancer. This is a typical concept for web application and shouldn’t be new or surprising. With VMware products this concept already used widely by VMware Integrated OpenStack (VIO), but also VMware vCloud Director and VMware vRealize Automation. &lt;/li&gt; &lt;/ul&gt;   The container concept   The above concept depicted in Figure 1 can only be considered as an interim step. We will certainly keep the concept of the business network vs. the management network. But in addition we place each management application in a dedicated Virtual POD with an VMware NSX Edge acting as router, firewall, and load balancer at the boundary of this container. As such we end up with one container for VMware vRealize Automation, another one for VMware vRealize Operations and the third one for VMware vRealize LogInsight (See Figure 2).                              Figure 2: Virtual POD for SDDC Management Applications     This approach provides the following benefits:      Security: Granular yet simple control who can access what services within a Virtual POD.   Modularity: Replace or upgrade an application without breaking the entire SDDC as dependencies go via well-know interfaces.   Simplicity: Reduces the number of required IP addresses on the corporate network, but also keeps the integration effort low, by not requiring dynamic routing towards the corporate network.   Business Continuity / Disaster Recovery (BC/DR): Simplifies the Business Continuity / Disaster Recovery (BC/DR) story with VMware Site Recovery Manager (SRM). See below for more.   IPv6: Ability to provide the management applications over IPv6 by solely enabling IPv6 on the Virtual IP (VIP) of the NSX Edge load balancer.   Virtual PODs for cross-region failover via SRM   Let’s have another look at the Virtual POD concept and see how it performs within a multi-region setup, using VMware Site Recovery Manager (SRM) for Business Continuity / Disaster Recovery (BC/DR) purposes (See Figure 3). The key benefit of this approach is the ability to use a purely DNS-based service failover mechanism, instead of having to re-announce IPv4 routes from the recovery location.                              Figure 3: Virtual POD with Site Recovery Manager     One of the fundamental concepts of the Virtual PODs is that for a given management application, the same IPv4 subnet is used within each region’s VPOD. In the example, depicted in Figure 3 this means that the Virtual POD in bother regions uses the IPv4 subnet 192.168.11.0/24.   As a result, in combination with VMware SRM, there is no need to change the IP addresses and therefore also SSL certificates of the service nodes for a recovered application like VMware vRealize Automation. This is a major benefit as an application like VMware vRealize Automation requires multiple cumbersome steps - including manipulation of the database entries - to change the IP addresses of a once installed instance.   The NSX Edge devices in both regions have equivalent settings for load balancer rules and firewall rules, except for the IP addresses on the Business and Management network. As a result the virtual IPs (VIP) between these two different load balancers are obviously different, but the underlying load balancer configuration is the same.   The external facing service, while the Management application resides in Region A, is reachable under the VIP of that regions NSX Edge device. After a failover via SRM of the Management Application to Region B, the service is alive under that regions NSX Edge device.   Therefore in order to make the failover complete for end-users of the service, a change to the DNS name of the service is necessary. With that change the DNS entry is re-pointed from the VIP on the NSX Edge in Region A to the VIP on the NSX Edge in Region B.   Performing such a DNS update can be easily done using corresponding Dynamic DNS services on the Internet, building your own equivalent or using a simple script to update you Microsoft DNS server.   Again, no dynamic routing updates are necessary after a SRM failover towards the business network. In fact, you don’t even need to run dynamic routing towards the business network unless you want to.   Virtual PODs and SRM failover testing   An interesting, but also very important feature of VMware Site Recovery Manager (SRM) is the ability to test a failover. In this case the application remains operational and untouched in the primary location. In the recovery location a second instance of the application is created based on a snapshot of the most recent replicated data. While this failover-test application is running, replication between sites is unaffected. Also after the test completes the failover-test application along with the snapshot data can be discarded.   With the Virtual PODs you can test the functionality of the failover-test application in the actual environment that the application would be restored to. No special test network constructs are necessary. Instead you deploy the failover-test application into the second regions vPOD where you can access this application under the corresponding Virtual IP. You could even place a test DNS entry on this VIP. As long as you don’t touch the DNS entry for the primary application it will not be affected.   Summary   The Virtual POD network container is a very powerful, yet simple concept to provide the management applications of the SDDC with security, modularity, simplicity, improved BC/DR capabilities and IPv6 support. All this with a minimum of integration effort. I can therefore be seen as a major enabler for an SDDC based on the VMware Validated Design.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/08/31/sddc-architecture-vpods-for-management-applications/",
        "teaser":null},{
        "title": "SDDC Architecture - Mapping of Logical Components to Physical Location",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC via VMware Validated Designs.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Logical Space   In this post we will map the physical space of the SDDC architecture to the logical space (See Figure 1).                              Figure 1: Mapping of logical to physical components within the SDDC     vSphere Clusters   For this we will start with the physical space that includes a Management POD, an Edge POD, and one or more Compute PODs. Each of these PODs maps to one cluster.   As a result we will end up with a single Management Cluster, one Edge cluster, and one or more Compute Clusters.   Here it is important to point out, that one Compute POD could house more than one Compute Cluster. This purely depends on the use case for this specific SDDC instance. In order to keep things simple and uniform it is recommended that in this case, the cluster size should be 1/4, 1/2, or 1/1 of the total available server count within a POD. This allows you to create multiple clusters in a uniform manner, without any waste. It is also important to point out, that a compute cluster cannot span more than one compute POD, as L2 host network traffic is not carried across PODs.   vSphere vCenter domains   The vSphere cluster above are distributed across two or more vCenters, which we will call stack. Each stack is therefore managed by one or more different vCenter and includes one or more vSphere cluster.   With that the Compute Stack includes:      Compute cluster: Used to host workloads (also called payloads). These are tenant VMs that an operator of an SDDC could effectively charge for.   Edge cluster: This vSphere cluster hosts the three NSX Controller VMs and NSX Provider Edge devices for the Compute Stack. NSX Provider Edges provide network connectivity between a physical VLAN based network and a VXLAN based overlay network.   The Management Stack includes:      Management cluster: The management stack houses two different set of virtual machines. Both sets are necessary for the operation of the SDDC itself.            Virtual Infrastructure Management components: vSphere vCenter server, Platform Services Controller, NSX Managers, NSX Controller and NSX Edges for the Management stack. These components are part of the Virtual Infrastructure itself and are necessary in each SDDC Region.       Cloud Management and Service Management applications: vRealize Automation, vRealize Operations, vRealize Log Insight. These components are encapsulated within Virtual PODs.           Rationale   The rationale behind using two or more vCenters is based on the following items:      Safety: Ensure that excessive operations of a cloud management platform on one of the the Compute vCenter Servers has no negative impact on the Management vCenter Server and thereby the management stack. It is therefore possible to still manage the SDDC itself in such a situation.   Security: The Cloud Management Platform only needs access to the Compute vCenter Servers and never needs to access the Management vCenter Server. This improves security as even when messing up the Role-Based Access Control (RBAC) within vCenter, the Cloud Management Platform can never accidentally or on purpose harm the applications within the management stack.   Scale out: Using separate vCenter Server for Compute and Management stack allows us to add additional vCenter Servers within the Compute stack in order to cope with increased payload churn rates. This refers to the case where the number of VM lifecycle operations per unit of time exceeds the capacity of a single vCenter. In these cases the task queue would increase without vCenter ever having a chance to work through them in a reasonable time. To get around this scenario you would add Compute vCenter Server(s) and let the Cloud Management Platform distribute the load across them.   With this an SDDC will usually have a single vCenter Server for the Management Stack and one or more vCenter Server for the Compute Stack within a region.   Virtual Distributed Switches   This design uses three Virtual Distributed Switches (vDS), one for each type of POD. The rationale behind this is that each type of POD has a different kind of network connectivity and therefore needs a separate vDS.   In detail these vDS are:      Compute vDS: This vDS stretches across all Compute PODs and therefore all Compute clusters.   Edge vDS: This vDS stretches across the Edge POD and therefore all Edge clusters.   Management vDS: This vDS stretches across the Management POD and includes the Management cluster.   NSX Transport Zones   Each stack would use a dedicated NSX Transport Zone. The reason for this is that today a 1:1 mapping between NSX Manager and vCenter Server exist. Therefore having 2 separate vCenter Server domains, means that you also end up with two NSX Manager domains. Within each of these domains it is then sufficient to create a single NSX Transport Zone.   vCenter Server to Platform Service Controller Mapping   In this design we use vSphere 6.0, which introduces the Platform Services Controller (PSC). The PSC provides a set of common infrastructure services encompassing Single Sign-On (SSO), Licensing, and Certificate Authority. As a result an administrator could log in to any one of the vCenter Servers attached to PSC within a single SSO domain and manage all resources within these attached vCenters. The result is a single pane of glass from a management perspective.   While an HA deployment scenario does exist for the Platform Services Controller, this scenario is not necessary and not recommended in this design. The High Availability (HA) topology would add additional components with the load balancer and would require additional configuration steps. While this would improve the availability of the PSC itself dramatically, the consumer of the PSC - the vCenter Server - are only able to leverage vSphere HA. As such PSC itself could also use vSphere HA, simplifying the topology.   The resulting design (See Figure 2) would also prepare the overall SDDC design to upcoming capabilities around high availability of the PSC.                              Figure 2: vCenter Server to Platform Services Controller mapping     The recommended design is to deploy a pair of Platform Services Controllers per Region and Availability Zones (AZ) and join all PSC in a single Single Sign On (SSO) domain. A maximum of 8 PSC can be placed into a single SSO domain. With this restriction it would be possible to span a single SDDC across up to 2 regions with 2 AZs each.   Within each AZ, the deployed vCenter Server will be split across the available PSC. In the minimum deployment size where you have only one vCenter Server for Management and one vCenter Server for Compute, this would result in a one-to-one mapping.   In the case of a PSC failure, due to an underlying ESXi host failure, the PSC would be restarted via vSphere HA on another ESXi hosts. This would result in a downtime of multiple minutes for the attached vCenter Servers.   In case of a prolonged downtime of a PSC - e.g. due to VM corruption or alike - the vCenter Servers mapped to this PSC (Red arrows in Figure 2) would not be able to leverage this PSC anymore. In this situation the vCenter Servers should be re-pointed to the remaining active PSC within a region (Blue arrows in Figure 2). At this point the SDDC is operational again and the defective PSC can be re-build, for the SDDC to return to a redundant operational state.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","Cloud","SDDC"],
        "url": "https://www.edge-cloud.net/2015/09/09/sddc-architecture-mapping-of-logical-components-to-physical-location/",
        "teaser":null},{
        "title": "SDDC Architecture - Business Continuity with multiple regions",
        "excerpt":"This article is part of a series of articles, focusing on the architecture of an SDDC via VMware Validated Designs.   Requirements   A Software Defined Data Center promises to be the new underpinning or platform for delivering today’s and tomorrow’s IT services. As such this next generation infrastructure needs to address some shortcomings of today’s infrastructure in order to be successful:      Highly automated operation at Scale: Leaner organization that scales sub-linearly with an operating model build around automation. Leverage modular web-scale designs for unhampered scalability.   Hardware and Software efficiencies: Support on-demand scaling for varying capacity needs. Improved resource pooling to drive increased utilization of resources and reduce cost.   New and old business needs: Support legacy applications with traditional business continuity and disaster recovery, besides new cloud-native applications.   Conceptual Design   This solution assumes that two regions exist. Under normal circumstances each region consists of an Software Defined Data Center (SDDC) installation, where components of the virtual infrastructure layer exist independently in both regions for the Management and Compute stack.   The management applications VMware vRealize Automation together with VMware vRealize Orchestrator and VMware vRealize Operations only exist in the primary region, while they manage and monitor resources in both regions. In a case of a failure these applications will be failed over to the secondary location, using VMware Site Recovery Manager (SRM).   For all other management applications a dedicated instance needs to exist per region. This includes vRealize Log Insight, of which a dedicated instance exists in both regions (See Figure 1).                              Figure 1: Disaster Recovery Conceptual Design     This underlying design limits the management components that need to be moved between the failed primary region and the secondary region in case of a failure of the primary region. At the same time it ensures that under normal circumstance both regions can provide sufficient services in an active/active manner. The design also ensure that the excess capacity that needs to be available for accepting a failed-over workload is kept to a minimum.   After a failure of either region, the overall SDDC management capabilities are still available. Solely the workload capacity has been reduced by whatever percentage of capacity the failed region makes up of the total capacity.   We will look at BC/DR capabilities for the workloads of the SDDC separately.   Disaster Recovery Design Example   Within this example, the SDDC includes two locations: A protected “Region A” in San Francisco, CA and a “Region B” for recovery purposes in Los Angeles. VMware’s Site Recovery Manager (SRM) provides a solution for automating the creation and execution of a disaster recovery plan or workflows between these two regions for the above described management applications.   Region A initially hosts the management application virtual machine workloads, that are being protected. As such this region is referred to as the “protected region”.   Logical Design   Dedicated network connectivity must exist between Region A and Region B, so that data from Region A can be replicated to Region B using VMware vSphere Replication, but also so that VMware Site Recovery Manager can coordinate the failover.   Region A has a management cluster of ESXi hosts with management application virtual machines that must be protected. Region B has a management cluster of ESXi hosts with sufficient free capacity to host the management applications from Region A. Each region has an instance of vCenter Server that manages the ESXi hosts within the region. Each region also has a Site Recovery Manager server and a Site Recovery Manager database. vSphere replication provides replication between the storage arrays and/or VSAN between Region A and Region B (See Figure 2).   The vCenter Server design includes a total of two virtual vCenter Server systems for the Management stacks. One Management Stack vCenter Server is located in Region A and one Management Stack vCenter Server is located in Region B. These are deployed within the same four-node ESXi management cluster within each region. Each vCenter Server provides specific functions as follows:      VMware vCenter Server Management / Region A: Located within the Region A data center to provide management of the primary management cluster and integration with Site Recovery Manager.   VMware vCenter Server Management / Region B: Located within the Region B data center to provide management of the recovery management cluster and integration with Site Recovery Manager.                              Figure 2: Site Recovery Manager Logical Design     Network Design   Physically moving a service from one region to another represents a networking challenge. Additional complexities can be introduced if applications have hard-coded IP addresses. Network addressing space and IP address assignment design considerations require that you choose to use either the same IP address or different IP address within the recovery region.   While protecting typical 3 tier web applications, this problem can be simplified by leveraging a load balancer to separate between a public reachable network segment, and a private network segment. On the public network segment, the web application is accessible via one or more virtual IP (VIP) addresses, while the inner working of the application are “hidden” on the isolated private network segment. Following this approach it is possible to treat the internal private network segment as a VLAN or VXLAN island without the requirement to change the IPv4 subnet between regions during a failover. Solely the external IPv4 address of the load balancer VIP changes between regions.   After a failover the recovered service is available under a different IPv4 address (VIP), which requires DNS entries to be changed. This can easily be accomplished in an automated manner (See Figure 3).                              Figure 3: Logical SDDC Network Design for cross region deployment with Management application network container     The vSphere Management networks (Figure 3, grey network) between SDDC regions have to be interconnected via VPN or MPLS. Various options exist for accomplishing such a cross-connect, ranging from VMware NSX Edge devices with IPSec VPN to various hardware based network products.   The IPv4 subnets within the VLAN “islands” (Figure 3, yellow network) are routed within the vSphere management network (Figure 3, grey network) of a region. Nodes within these “islands” are therefore reachable from within the SDDC (including Jump-Hosts, SSLVPN connections or alike). As these IPv4 subnets overlap across a region, care must be taken that these IPv4 subnet are not propagated beyond a region.   The public facing Ext-Management network (Figure 3, blue network) of both regions is assumed to be reachable by users of the SDDC and is also assumed to both connect to external resources, such as Active Directory or DNS.   The load balancers - here NSX Edge devices - across the two regions must be configured with the same settings (while taking into account the differing external IP addresses) for a given management application and it’s SRM shadow segment. This configuration sync needs to happen either manually or can be accomplished via scripting.   It is assumed that Active Directory and DNS services are running at both the primary and secondary location. It is advisable to use Anycast to make DNS Resolvers available under the same IPv4 address at different location, as well as using Global Traffic Management to make local Active Directory Domain Controllers available under a common global domain name.   Furthermore it is recommended to use the NSX DNS server functionality within a vPOD to provide DNS server capabilities to the nodes within the vPOD. This way each node leverages the NSX Edge of the vPOD as DNS resolver. This NSX Edge in return leverages a local DNS server as resolver.   Summary   Using the here described BC/DR strategy for the Software Defined Data Center (SDDC), not only simplifies the setup of the resource protection itself, but also simplifies the operation of the actual failover. Especially the concept of the previously introduced network container helps a lot in this scenario.  ","categories": ["EdgeCloud"],
        "tags": ["Architecture","SDDC"],
        "url": "https://www.edge-cloud.net/2015/09/11/sddc-architecture-business-continuity-with-multiple-regions/",
        "teaser":null},{
        "title": "Track your CloudFlare PoPs usage in Google Analytics",
        "excerpt":"Cloudflare provides a content delivery network and distributed domain name server services to help secure and accelerate websites. This cloud-based service sits between the visitor and the CloudFlare user’s hosting provider, acting as a reverse proxy for the website.   This article will help you visualize the global presence of your website, when using CloudFlare, with the well-known tool Google Analytics. You need to have your website running through CloudFlare for this to work.   Signing up with CloudFlare is easy and free and usually only takes 5 minutes.   About CloudFlare’s Points-of-Presence   CloudFlare currently uses 63 data centers worldwide as points-of-presence to deliver fast and secure website traffic (See Figure 1).                              Figure 1: Current CloudFlare Network Map     This means that as a CloudFlare customer your website is delivered to your audience from all these locations worldwide, reducing the network hops and lowering latency. As a result your website gains a global presence on an affordable budget, while improving performance.   Wouldn’t it be great to get insight into how much these worldwide locations help you with your website? In this post you will learn how to get started by gaining insight into which CloudFlare location delivers your website traffic.   Setup   Create a custom dimension in Google Analytics   First, set up a custom dimensions for the location of the CloudFlare PoP that serves a request in Google Analytics:      Sign in to Google Analytics.   Select the Admin tab and navigate to the property to which you want to add custom dimensions.   In the Property column, click Custom Definitions, then click Custom Dimensions.   Click New Custom Dimension.   Add a Name.   This can be any string, but use something unique so it’s not confused with any other dimension or metric in your reports. Only you will see this name in the Google Analytics page.   Select the Scope.   Choose to track at the Hit, Session, User, or Product level. For this scenario I recommend to choose Hit or rather Session.   Check the Active box to start collecting data and see the dimension in your reports right away. To create the dimension but have it remain inactive, uncheck the box.   Click Create.   Note down the dimension ID from the displayed example codes. In the example for this blog post the dimension ID is “1” (See Figure 2).                              Figure 2: Create a Google Analytics Custom Dimension     Embed the Google Analytics Tracking Code   Next we need to embed the Google Analytics tracking code within the website, in order to fill the newly created custom dimension with data. This tracking code has to be placed between the code for creating the Google Analytics tracker, which looks like this: __gaTracker('create','UA-12345678-1','auto');, and the code to submit the tracker, which looks like this __gaTracker('send','pageview');.   If you are using WordPress the easiest way to include the custom tracking code is by using the “Google Analytics by Yoast” plugin. This plugin allows you under Advanced &gt; Custom Code to embed the below code right away and without any coding requirements.   But first we have to actually determine the CloudFlare PoP location that serves a request. For this we can use the HTTP response header “cf-ray”, which is added by CLoudFlare for troubleshooting purposes. It includes a numeric value, as well as the location code of the CloudFlare PoP.   The below JavaScript code will read the “cf-ray” response header, extract the location ID and push it into the Google Analytics custom dimension variable. Ensure that the numeric ID of this custom dimension variable matches what you have created in above steps.  function loc(){   var req = new XMLHttpRequest();   req.open('HEAD', document.location, false);   req.send(null);   return (req.getResponseHeader(\"cf-ray\").split(\"-\"))[1]; } __gaTracker('set','dimension1',loc());&lt;/pre&gt;  Embedded in your website along with the standard Google Analytics tracking code, this custom JavaScript code will determine the CloudFlare PoP over which the corresponding site was served and push it into Google Analytics.   A few hours after embedding the code you should see your first custom dimension data in Google Analytics.   Usage   Create a custom report in Google Analytics   You can now create custom reports with the custom dimension in Google Analytics. A simple example would be to determine which CloudFlare PoP serves how many of your audience’s session.      Make sure you are still signed in to Google Analytics.   Select the Customization tab and click on New Custom Report.   Name your Custom Report here.   Select a Metric for which you want to see your Custom Dimensions. I recommend the metric “Sessions” within the “Users” Metric Group.   Next select the custom dimension that you created as the “Dimension Drilldown” (See Figure 3).   Click on the Save button.                              Figure 3: Create a Custom Report in Google Analytics     The resulting Custom Report will show you how many session - in total numbers, but also in percent - were served by which CloudFlare PoP. Using the “Percantage” button you can quickly generate a pie chart with the data (See Figure 4). In the example below you can see that the three most used CloudFlare PoPs for this site are San Jose, Chicago and Frankfurt (Germany).                              Figure 4: Your CloudFlare PoP utilization     Combining the new custom dimension with Google Analytics Data   Now that we have the CloudFlare Point-of-Presence that served a web site in Google Analytics, we can leverage this data for many more interesting reports.   One such report could be to map the location from where users connect to the CloudFlare PoPs. Figure 5 shows all 9 CloudFlare PoPs in the United States and the cities from which end-users connect to them, while accessing this blog.                              Figure 5: Geographic Reach of CloudFlare PoP for the US     In this report we can see various users not being served by the closest Point of Presence, but one farther away. This is mostly caused by the way that the Internet works and especially by some   ISPs not peering directly with content or CDN networks. Instead these ISP use Tier 1 network provider, which can cause these inefficiencies.   Summary   Google Analytics Custom dimension provide a simple way to visualize the great benefits of the global presence of your website, thanks to CloudFlare. Leverage it to see the benefits to your website, while using CloudFlare.  ","categories": ["EdgeCloud"],
        "tags": ["Cloudflare","Google-Analytics"],
        "url": "https://www.edge-cloud.net/2015/10/04/cloudflare-pops-in-google-analytics/",
        "teaser":null},{
        "title": "CloudFlare: Redirect to Social Media on origin failure",
        "excerpt":"It happens to the best: Sometimes a web server just goes down. Ideally you would have redundancy in place for this case, with a second (or even more) web server(s) mirroring your page. But redundancy doesn’t come for free: It’s costly and might end up being complicated. Therefore you sometimes have no other choice, but to rely on a single origin web server for your web site. Especially when using a web hosting company you rely on whatever redundancy you are provide or more than often not provided with.   Nevertheless you do not want to display just nothing or a nondescript error page to your website users, when the hopefully rare moment hits and your web server does go down. One interesting approach is Dyn’s Social Failover capability, which reroutes your web traffic to your Twitter feed, Facebook page, or another URL of your choice, in case your web server goes down.   Using CloudFlare   In this post I want to show you how you can accomplish the same with any of the paid CloudFlare plan levels. For these plan types CloudFlare offers the capability to customize error messages with your own HTML code. One of these error messages would be displayed in the case where CloudFlare cannot contact your web server, e.g. because it is down.   We can make use of this capability and instead of displaying a custom error page, just display an HTML redirect to a site of your choice. This way your visitors would be redirected to this page in the case of an origin failure (See Figure 1).                              Figure 1: Redirect to Social Media Page on Origin Failure     CloudFlare custom error pages as redirect   Forwarding a web browser to another website via an HTML page is quite simple and straight forward. It basically requires a single line of HTML code. For a custom error page, CloudFlare requires certain content to be show within the page for troubleshooting. In our case we don’t really care about this information as we will anyways redirect the user. Nevertheless the special CloudFlare Error box has to be included, otherwise you are not able to later utilize the HTML page. A small trick is to change the background color of our redirect page to use a black background. This way end-users won’t be confused by any error message being displayed briefly.   Preparing the custom error page   Let’s start by creating the custom error page. You can create and host this page on any place that is accessible from the Internet. This can be a temporary spot and can even be on the site that you want to use the redirect with. This page is only used briefly while CloudFlare downloads the custom error page. It will not be used during an actual outage.   In this example I place the below custom error page at https://www.edge-cloud.net/redirect.html.   &lt;html&gt;   &lt;head&gt;     &lt;meta http-equiv=\"refresh\" content=\"0; url=https://twitter.com/ChristianElsen\" /&gt;     &lt;meta name=\"robots\" content=\"noindex, nofollow\" /&gt;   &lt;/head&gt;   &lt;body bgcolor=\"#000000\"&gt;     &lt;center&gt;::CLOUDFLARE_ERROR_500S_BOX::&lt;/center&gt;   &lt;/body&gt; &lt;/html&gt;   You can see in the third row the actual redirect, which in this case points to my Twitter home page. We also don’t want Robots to index this temporary redirect page and certainly not to follow the redirect link. That’s what the next line is for.   You can also see the CloudFlare error box ::CLOUDFLARE_ERROR_500S_BOX::, which must be included. More on this later.   Uploading the custom error page   Within your CloudFlare account navigate to the Customize app (See Figure 2).                              Figure 2: Customize application within the CloudFlare menu     Next find the box for “500 Class Errors”. This is the kind of error that would be displayed when your origin cannot be contacted by CloudFlare and your website cannot be served. Within this box click on the “Customize” button (See Figure 3).                              Figure 3: Customize 500 Class errors     Now provide the URL to your custom error page into the “Address of customized page” field. In this example it would be https://www.edge-cloud.net/redirect.html.   Notice the warning that the CloudFlare error box ::CLOUDFLARE_ERROR_500S_BOX:: has to be included in the custom page (See Figure 4). Lucky for us, we already did this earlier.                              Figure 4: URL with Redirect to other site     Now you can click on the “Preview” button: A new tab will open in which you will briefly see a black page. Within a few moments this black page should redirect you to the site you selected. Once everything is working for you, click on the “Publish” button within the CloudFlare window. That’s it. You are all set.   Summary   In this article I have outlined how you can use any CloudFlare paid plan to redirect your website to your Social Networking page or any other page in case your origin server fails.  ","categories": ["EdgeCloud"],
        "tags": ["Cloudflare"],
        "url": "https://www.edge-cloud.net/2016/01/15/cf-redirect-to-social-media/",
        "teaser":null},{
        "title": "RUM \"Light\" with CloudFlare and Google Analytics",
        "excerpt":"Would you like to know the ratio of visitors accessing your website over IPv4 vs. IPv6? Or curious about how many visitors you serve over HTTP/2 vs. SPDY or HTTP 1.1?   Using Google Analytics, CloudFlare and some JavaScript magic we can easily get answers to these questions. For this we will use the principle of Real User Monitoring (RUM).   In a previous post I’ve already shown how we can track the usage of CloudFlare data centers for the delivery of our website in Google Analytics. In this blog post I want to show you how to refine this method even further and also include information about other interesting metrics, such as IPv6 and HTTP/2 usage .   Possible custom dimensions   In the previous post I used response header information, presented by the CloudFlare edge servers to extract information about the served traffic.   This time we will instead use a special debugging URL that is available for all CloudFlare powered sites. It is available under https://www.example.com/cdn-cgi/trace, where https://www.example.com/ corresponds to the CloudFlare powered domain.   The result will look like this:   fl=4f96 h=www.cloudflare.com ip=2606:4700:1000:8200:7847:642:dc8e:b176 ts=1454615047.969 visit_scheme=https uag=Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.103 Safari/537.36 colo=SJC spdy=h2 loc=US   Let’s have a closer look at the fields that are interesting to us and what they mean:      ip: This lists the IP address that was used to contact CloudFlare for this request. We can use this value to determine whether the request was made over IPv4 or IPv6.   visit_scheme: This will tell you whether the request was made over HTTP or HTTPS. In case you serve traffic only over HTTPS, this is rather not interesting. Otherwise you could use it to determine the traffic split between visitor using an encrypted vs. un-encrypted connection.   colo: The CloudFlare Points-of-Presence (PoP), which served this traffic. In the previous post you have already seen how to use this.   spdy: The HTTP protocol version that was used to serve this content. Even though the key is called “spdy”, the value can also indicate HTTP/2 via “h2” (as depicted above). A value of “3.1” will indicate SPDY/3.1 and “off” will indicate HTTP 1.x. It would make more sense to have the field “http” available with ”http=h2” for HTTP/2, ”http=spdy/3.1” for SPDY/3.1, and ”http=http/1.x” for HTTP/1.x. But for now the “spdy” field is good enough.   loc: The country that CloudFlare located the visitor in. This information is already available in Google Analytics, we therefore do not need to extract it here.   How does Real User Monitoring fit into the picture?   Now that we know about all this great data at /cdn-cgi/trace for our website, how can we make this accessible in Google Analytics? Especially while keeping in mind, that this data is only available for the current connection and differs from user to user.   The trick: We make the user download the data, extract the values and push the results into Google Analytics.   This approach of leveraging a real user to measure or monitor something is called Real User Monitoring or RUM for short.                              Figure 1: Interaction between browser, CloudFlare and Google Analytics     The workflow that will be execute on every page load is as follows (See Figure 1):      Step 1: A user requests a page via his or her browser.   Step 2: The content is returned and includes a JavaScript to be executed by the browser.   Step 3: The JavaScript requests the content from /cdn-cgi/trace.   Step 4: The different values for that particular visitor and browsing session are returned to the JavaScript.   Step 5: The JavaScript pushes the values as custom dimension into Google Analytics.   The additional request of the browser to /cdn-cgi/trace is similar to the browser loading stylesheets or images for rendering a page. This added round-trip to the closest CloudFlare edge for one more element does not impact the performance of the site.   Create custom dimensiosn in Google Analytics   For each value from /cdn-cgi/trace that we want to track, we need to create a custom dimension in Google Analytics. Here is an example mapping for the values that we want to track in this blog post:      dimension1: CloudFlare PoP location   dimension2: Access Scheme (HTTPS vs. HTTP)   dimension3: IP Transport Method (IPv4 vs. IPv6)   dimension4: HTTP version (HTTP 1.1 vs. SPDY vs. HTTP/2)   First, set up the custom dimensions in Google Analytics:      Sign in to Google Analytics.   Select the Admin tab and navigate to the property to which you want to add custom dimensions.   In the Property column, click Custom Definitions, then click Custom Dimensions.   Click New Custom Dimension.   Add a Name. This can be any string, but use something unique so it’s not confused with any other dimension or metric in your reports. Only you will see this name in the Google Analytics page.   Select the Scope. Choose to track at the Hit, Session, User, or Product level. For this scenario I recommend to choose Hit or rather Session.   Check the Active box to start collecting data and see the dimension in your reports right away. To create the dimension but have it remain inactive, uncheck the box.   Click Create.   Note down the dimension ID from the displayed example codes. In the example for this blog post the dimension IDs are listed above for the three monitored values.                              Figure 2: Create a Google Analytics Custom Dimension     Embed the Google Analytics Tracking Code   Next we need to embed the Google Analytics tracking code within the website, in order to fill the newly created custom dimensions with data. This tracking code has to be placed between the code for creating the Google Analytics tracker, which looks like this: __gaTracker('create','UA-12345678-1','auto');, and the code to submit the tracker, which looks like this __gaTracker('send','pageview');.   If you are using WordPress the easiest way to include the custom tracking code is by using the “Google Analytics by Yoast” plugin. This plugin allows you under Advanced &gt; Custom Code to embed the below code right away and without any coding requirements.   The below JavaScript code will read the values from /cdn-cgi/trace, extract the information we are interested it and push it into the Google Analytics custom dimension variables. Ensure that the numeric IDs of these custom dimension variables matches what you have created in above steps.      function processData(x) {       var y = {};       for (var i = 0; i &lt; x.length-1; i++) {         var split = x[i].split('=');         y[split[0].trim()] = split[1].trim();       }       return y;     }      function objData(x) {       return obj[x];     }      function isIPv6() {       ipv6 = (objData('ip').indexOf(\":\") &gt; -1);       switch (ipv6){         case true:           return \"IPv6\";           break;         default:           return \"IPv4\";       }     }      var data;     var obj;     var client = new XMLHttpRequest();     client.open(\"GET\", \"/cdn-cgi/trace\", false);     client.onreadystatechange =             function () {                     if(client.readyState === 4){                             if(client.status === 200 || client.status == 0){                                     data = client.responseText.split(\"\\n\");                             }                     }             };     client.send(null);     obj= processData(data);      __gaTracker('set','dimension1',objData('colo'));     __gaTracker('set','dimension2',isIPv6());     __gaTracker('set','dimension3',objData('spdy'));  Embedded in your website along with the standard Google Analytics tracking code, this custom JavaScript code will be executed by a visitor, collect metrics data from /cdn-cgi/trace and push it into Google Analytics.   A few hours after embedding the code you should see your first custom dimension data in Google Analytics.   Create custom reports in Google Analytics   You can now create custom reports with the custom dimensions in Google Analytics. A simple example would be to determine the IPv4 vs. IPv6 traffic ratio.      Make sure you are still signed in to Google Analytics.   Select the Customization tab and click on New Custom Report.   Name your Custom Report here.   Select a Metric for which you want to see your Custom Dimensions. I recommend the metric “Sessions” within the “Users” Metric Group.   Next select the custom dimension for the IP Transport Method (IPv4 vs. IPv6), that you created as the “Dimension Drilldown” (See Figure 3).   Click on the Save button.                              Figure 3: Create a Custom Report in Google Analytics     The resulting Custom Report will show you how many session - in total numbers, but also in percent - were served by which transport method.                              Figure 4: Traffic served over IPv4 vs. IPv6     You can generate similar graphs for the other custom dimensions. E.g. in order to find out how much of your web-sites traffic was served over HTTP/2 (See Figure 5).                              Figure 5: Traffic served over HTTP/2 vs. SPDY vs. HTTP 1.x     By combining data from the custom dimension with data collected by Google Analytcis natively you can answer many interesting questions for your website, such as: Is IPv6 traffic really mostly driven by mobile traffic? Where are all these users with HTTP/2 capable browsers located?   Summary   This article has shown you, that you can easily built your own Real User Monitoring system with Google Analytics Custom dimension and some JavaScript code. It allows you to extract many interesting metrics out of your CloudFlare usage and make it available in Google Analytics for further data analysis.   Not only has this site been using the above described method since November 2015, but also another HTTP/2 adoption, which has provided interesting insights into the HTTP/2 adoption.  ","categories": ["EdgeCloud"],
        "tags": ["Cloudflare","Google-Analytics","HTTP/2","IPv6"],
        "url": "https://www.edge-cloud.net/2016/02/05/rum-light-with-cloudflare/",
        "teaser":null},{
        "title": "CloudFlare Railgun on AWS",
        "excerpt":"This tutorial will show you how to configure CloudFlare Railgun within Amazon Web Services (AWS). We will leverage Amazon ElastiCache together with AWS Elastic Load Balancing, AWS Auto Scaling and Amazon EC2 to quickly and simply achieve this goal (See Figure 1).                              Figure 1: CloudFlare Railgun Setup with Amazon Web Services (AWS)     This setup should especially be appealing to CloudFlare customers, operating their origin servers in one or more Amazon Web Services (AWS) regions, and who want to leverage the power of AWS to compensate automatically against failures of the CloudFlare Railgun Listener application.   About CloudFlare Railgun   CloudFlare Railgun accelerates the connection between each CloudFlare point-of-presence (PoP) and a customer origin server so that requests that cannot be served from the CloudFlare cache are nevertheless served very fast. By leveraging techniques similar to those used in the compression of high-quality video, Railgun compresses previously uncacheable web objects up to 99.6%.   Railgun consists of two software components: the Listener and Sender. The Railgun Listener is installed on a server close to the customer origin server, or in some cases on the origin server itself. It’s a small piece of software that runs on a standard server and services requests from CloudFlare using the encrypted, binary Railgun protocol. Railgun Listener is a single executable whose only dependency is a running Memcache instance. It runs on 64-bit Linux and BSD systems as a daemon.   The Listener requires a single port (TCP/2408) open from the Internet for the Railgun protocol so that CloudFlare PoPs can contact it. And it requires access to the website to be accelerated via HTTP and HTTPS. The website still needs to be accessible over ports TCP/80 and TCP/443 by the CloudFlare PoPs, as a fallback path in case of Railgun experiencing any issues.   Ideally, the Listener would be placed on a server with fast access to the Internet and low latency. Installation is simply a matter of installing via an RPM or .deb file.   Railgun is available for customers with a CloudFlare Business or Enterprise plan or customers hosted with an Optimized Hosting Partner.   Setup Overview   As outlined in Figure 1, the setup will consist of the following elements:      Amazon Virtual Private Cloud (VPC): Provide network isolation with a public subnet to operate the Railgun nodes and a private subnet to operate the Railgun to Memcached connection.   Amazon ElastiCache: A managed Memcached cluster to fulfill the Railgun requirements. The cluster can contain one or more nodes.   AWS Elastic Load Balancing: A managed load balancer to distribute traffic across multiple Railgun instances within the same AWS region.   AWS Auto Scaling: Automatically replace failed Railgun instances.   Amazon EC2: Run a 64-bit Linux and automatically install the latest version of CloudFlare Railgun.   The following sections walk you step-by-step through the setup. In this example we will use a Memcached cluster with 2 nodes, spread across two AWS Availability Zones, as well as 2 Railgun nodes, spread across two AWS Availability Zones. While this setup will provide ideal redundancy, it might not suit your direct business needs. Therefore please adapt the setup accordingly.   VPC Setup   We will use Amazon Virtual Private Cloud (VPC) to provide network isolation. The Railgun nodes will be placed on two public network segments, so that they are reachable from the public Internet and therefore from the CloudFlare PoPs. Two private network segments will be used for traffic between the Railgun nodes and the ElastiCache based Memcached instances.   Two security groups, one for Railgun and one for ElastiCache, will be used to ensure that only valid traffic can reach the nodes.   Let’s get started by creating a VPC of type VPC with Public and Private Subnets (See Figure 2).                              Figure 2: VPC with Public and Private Subnets - Step 1     Choose your preferred IP CIDR block and provide a meaningful VPC name, such as Railgun-HA. Consciously pick an Availability Zone for both subnets and include the AZ identifier in the subnet name. This will later help you identify which subnet is used for what purpose and resides in what AZ. For the NAT instance information chose Use a NAT gateway instead (see Figure 3).                              Figure 3: VPC with Public and Private Subnets - Step 2     The VPC template for VPC with Public and Private Subnets will only create a single public and private subnet. But we will want to leverage two public and two private subnets, across two AZ in order to build a true highly available solution. Therefore we now need to create another public and private subnet within the VPC.   Let’s start by creating another public subnet. Pick a different AZ to the one that was selected when creating the VPC. Also try to come up with a pattern for assigning the CIDR blocks. Here I use even numbers for the third octet in case it is a public subnet and uneven numbers in case it is a private subnet (See Figure 4).                              Figure 4: Add another Public subnet     In order to make the newly created subnet a public subnet, you have to change the routing policy, so that the destination 0.0.0.0/0 points to an internet gateway, which is indicated via the target name prefix of igw (See Figure 5).                              Figure 5: Configure the Internet Gateway for the Public Subnet     Next create another private subnet. Here also pick a different AZ to the one that was selected for the first private subnet when creating the VPC (See Figure 6).                              Figure 6: Add another Private subnet     After successfully creating all the necessary subnets, it’s time to create the security groups. One will be used to allow Railgun traffic and another one will be used to allow Memcached traffic.   Let’s start by creating the Security Group for Memcached traffic (See Figure 7).                              Figure 7: Create a Security Group for Memcached     For the newly created Memcached security group, add an Inbound Rule. Select “Custom TCP Rule” with the protocol TCP and the port range 11211. As the Source you can select the IP range of your newly created VPC or limit it even further to the public subnets (See Figure 8).                              Figure 8: Configure the Inbound Rules for the Memcached Security Group     Next, create a Security Group for Railgun traffic (See Figure 9).                              Figure 9: Create a Security Group for Railgun     For the newly created Railgun security group, add an Inbound Rule. Select “Custom TCP Rule” with the protocol TCP and the port range 2408. As the Source you can select 0.0.0.0/0 to open up access to all of the public Internet. As an alternative you could limit access even further to the CloudFlare IP ranges only (See Figure 8).                              Figure 10: Configure the Inbound Rules for the Railgun Security Group     This completes the setup steps for the VPC and the Security Groups.   ElastiCache Setup   One of the pre-requisites for running CloudFlare Railgun is access to Memcached. ElastiCache provides us a managed Memcached cluster. While a Memcached cluster does not provide full protection against the failure of a cluster node, the impact of such a failure would at least be compensated. In case of a two node cluster, only approximately half of the keys would be lost in case one node fails.   In this tutorial we will create a two node ElastiCache cluster with the engine Memcached. Before we can do this, we need to create a Cache Subnet Group. A Cache Subnet Group attaches the nodes of an ElastiCache cluster to a certain VPC subnet. Here we want to attach our ElastiCache cluster to the private subnet, that we created in the previous step.   Lookup the subnet IDs for the two private subnets that you created and start the creation of the Cache Subnet Group. Pick the VPC that you created, and for each of the two Availability Zones, specify the two private subnets (See Figure 11).                              Figure 11: Create a Cache Subnet Group for Memcached     Next start the creation of a new ElastiCache cluster and chose the engine type “Memcached” (See Figure 12).                              Figure 12: Create an ElastiCache cluster with the engine Memcached - Step 1     Chose the node type depending on your performance needs and select the number of nodes that you want in your cluster. In this example we will use 2 nodes, in order to achieve a certain level of redundancy (See Figure 13).                              Figure 13: Create an ElastiCache cluster with the engine Memcached - Step 2     Select the Cache Subnet Group that you created at the beginning of this section. Use the Availability Zone policy of “Spread Nodes Across Zones”. This will ensure that in case of a Availability Zone failure, you still have one ElastiCache node left. For the VPC Security Group select the “Memcached” security group that you created (See Figure 14).                              Figure 14: Create an ElastiCache cluster with the engine Memcached - Step 3     Wait for the ElastiCache cluster to be created and note down the configuration endpoint address (See Figure 15). You will need this address in a later step.                              Figure 15: Lookup the Memcached endpoint address     This completes the setup steps for the ElastiCache Memcached cluster.   Elastic Load Balancer (ELB) Setup   The Elastic Load Balancer (ELB) serves the purpose of distributing incoming traffic from the CloudFlare PoPs among the active Railgun nodes. As the ELB and its hostname serve as the termination point for CloudFlare Railgun traffic, it is very simple to replace a failed Railgun node behind the ELB, without making any configuration changes to the Railgun setup.   This configuration also allows to scale out the number of Railgun nodes or scale up the size of the Railgun nodes, without impacting production traffic.   Create a new ELB inside the VPC that you are using for the Railgun setup. For the Listener Configuration chose TCP as the Load Balancer as well as Instance protocol. Enter “2408” as the Load Balancer and Instance port. For the subnets, select the two public subnets that you created earlier (See Figure 16).                              Figure 16: Define an Elastic Load Balancer - Step 1     Next assign the security group Railgun, which you have created in an earlier step. This will ensure that only Railgun traffic is allowed through the ELB (See Figure 17).                              Figure 17: Define an Elastic Load Balancer - Step 2     Next, configure the Health Check. This will determine how the ELB will probe the Railgun nodes and determine whether they are healthy. As the ping protocol select TCP with the ping port of 2408. This is necessary as the protocol between Railgun Listener and Railgun Sender is a binary protocol and not HTTP or HTTPS. Under Advanced Details you can reduce the timeout, interval and thresholds to the minimum values for a fast failover reaction (See Figure 18).                              Figure 18: Define an Elastic Load Balancer - Step 3     In the next step you would add the various instances to the ELB. As we have not yet created any Railgun instances, we will not add any instance here.   Instead we will solely ensure that the box for “Enable Cross-Zone Load Balancing” has been ticked (See Figure 19).                              Figure 19: Define an Elastic Load Balancer - Step 4     You can tag the ELB with a name (See Figure 20).                              Figure 20: Define an Elastic Load Balancer - Step 5     Before completing the creation of the ELB, you can validate that all settings have been configured correctly (See Figure 21).                              Figure 21: Define an Elastic Load Balancer - Step 6     Once the ELB has been created successfully, lookup the DNS name and note it down (See Figure 22). You will need it in a subsequent step.                              Figure 22: Lookup the ELB DNS name     This completes the setup steps for the Elastic Load Balancer (ELB).   Auto Scaling Setup   In this setup we will use Auto Scaling primarily to automatically replace failed Railgun instances. To do so, we configure an auto scaling group with a minimum of 2 instances.   Part of the Auto Scaling configuration is a launch group, where instead of using a pre-built Railgun AMI, we will built a Linux OS with the latest Railgun version on the fly. This will dramatically reduce the overhead for managing and updating custom OS images.   Start by creating a Launch Configuration, where you select the latest Ubuntu Server LTS version as your prefered AMI (See Figure 23).                              Figure 23: Create a Launch Configuration - Step 1     As mentioned we will instruct the launch configuration to install and configure the latest version of CloudFlare Railgun on the fly. This can be done via the EC2 Cloud Init method, where we pass a shell script into the newly created Linux OS.   Below is the script that will be executed upon boot. It will install and configure CloudFlare Railgun. You will have to replace three values within the script:      Railgun Activation token: You can find this token within the CloudFlare Web UI.   Railgun Host: The hostname of the ELB. You should have noted this down in an earlier step.        Memcached Server: The hostname of the ElastiCache Memcached cluster. You should have noted this down in an earlier step.       #!/bin/bash   echo ‘deb http://pkg.cloudflare.com/ ‘lsb_release -c -s’ main’ | tee /etc/apt/sources.list.d/cloudflare-main.list   curl -C - https://pkg.cloudflare.com/pubkey.gpg | sudo apt-key add -   apt-get update   apt-get install -y railgun-stable   sed -i “s/memcached.servers = 127.0.0.1:11211/memcached.servers = railgun-ha.zazcvl.cfg.euc1.cache.amazonaws.com:11211/” /etc/railgun/railgun.conf   sed -i “s/activation.token = YOUR_TOKEN_HERE/activation.token = 123a4567bc123d4e567abcd89012345a/” /etc/railgun/railgun.conf   sed -i “s/activation.railgun_host = YOUR_PUBLIC_IP_OR_HOSTNAME/activation.railgun_host = Railgun-HA-1142856350.eu-central-1.elb.amazonaws.com/” /etc/railgun/railgun.conf   service railgun start       Give your Launch Configuration a name and paste the above script into User Data field (See Figure 24).                              Figure 24: Create a Launch Configuration - Step 2     Within the Security Group configuration step, select the existing security group “Railgun” (See Figure 25). You have configured this security group in a previous step.                              Figure 25: Create a Launch Configuration - Step 3     Next we have to create the Auto Scaling Group, which ties the Launch Configuration, Elastic Load Balancer and VPCs together to automatically launch the Railgun nodes.   Under Network chose the VPC that you are using for this Railgun setup and under subnet select the two public subnets of the VPC.   Select the ELB that you configured under Load Balancing and pick ELB as the Health Check Type (See Figure 26). This will ensure that Railgun nodes that do not accept connections over TCP/2408 anymore are considered unhealthy, will be terminated and replaced.                              Figure 26: Create an Auto Scaling Group - Step 1     As we are solely using the Auto Scaling capability to replace failed Railgun nodes, pick “Keep this group at its initial size” for the scaling policy (See Figure 27).                              Figure 27: Create an Auto Scaling Group - Step 2     Next provide what tags you want to apply to all EC2 instances that are created as part of the Auto Scaling group. It is highly recommended to at least define the tag “Name” (See Figure 28).                              Figure 28: Create an Auto Scaling Group - Step 3     This not only concludes the configuration of the Auto Scaling setup, but of your entire setup. AWS should now automatically spin up EC2 images, automatically install and configure CloudFlare Railgun inside of them and add them to the ELB load balancer. Have a look at the next section for learning how to test your setup.   Testing the setup   Now, with the setup in place it’s time to wait for the Railgun nodes to boot up and configure correctly. You can head over to the ELB setup and look at the instances under the configured load balancer. While the nodes are still initializing you should see the status “OutOfService” displayed (See Figure 29).                              Figure 29: Monitor the Load Balancer Instances     After a few minutes the status of the instances should change to “InService”, indicating that the Railgun Listener nodes are up and running and accept traffic over port TCP/2408 (See Figure 30).                              Figure 30: ELB instances in status InService     Now you can login to the CloudFlare Dashboard and test the Railgun setup. If everything is working you should see a “compression_ratio”, as well as “origin_response_time” displayed (See Figure 31).                              Figure 31: Validate the functionality of Railgun     Summary   This tutorial showed you how to use Amazon Web Services (AWS), with the services Amazon ElastiCache together with AWS Elastic Load Balancing, AWS Auto Scaling and Amazon EC2 to quickly and easily setup a highly available CloudFlare Railgun Listener setup.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Cloudflare"],
        "url": "https://www.edge-cloud.net/2016/02/17/cloudflare-railgun-on-aws/",
        "teaser":null},{
        "title": "Processing Flowroute text messages with AWS Lambda",
        "excerpt":"Flowroute is a great SIP communications provider that I’ve been using for a few years now. Especially because they support T.38 for sending and receiving fax messages.   Earlier this year Flowroute added the capability for inbound and outbound text messages (aka. SMS) to their service.   In this post I want to show you how to integrate Flowroute’s inbound text messaging capabilities with AWS Lambda - the serverless compute offering from AWS. Instead of keeping servers up and running, but mostly idle, Lambda offers you the ability to only pay for execution of a software function whenever a text message arrives. Such a solution is able to handle various loads ranging from a handful of text messages per month to millions of messages.   In this example incoming text messages are forwarded via E-Mail to a configurable address (See Figure 1). But you could as well implement other functionality, such as text message based voting or alike with this approach.                              Figure 1: Architecture for processing Flowroute text messages     Flowroute text message API documentation   You can find more details on the Flowroute API documentation for receiving inbound text messages at https://developer.flowroute.com/.   Note that you need:     A Flowroute phone number added to the Direct Inward Dialing page, which will receive the text message.   A web application with a public IP or URL. This article shows you how to use AWS Lambda for creating and hosting this web application.   AWS Setup   AWS Pre-Requisites   This example uses Amazon Simple Email Service (Amazon SES) to deliver the received text message. You must at least have configured SES with a verified E-Mail address or domain in order to use it for sending E-Mails via the script below.   AWS Identity and Access Management role   AWS Identity and Access Management roles allow you to delegate access to services. The AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don’t have to share long-term credentials or define permissions for each entity that requires access to a resource.   This approach allows us to grant AWS Lambda permission to access Amazon SES, without the need to embed credentials within the code executed by Lambda. We can therefore even store the code in a public Github repository without the risk of exposing access keys.   To create an AWS IAM profile navigate to the Identity &amp; Access Management service within the Security &amp; Identity section in the AWS Console (See Figure 2).                              Figure 2: Navigate to Identity &amp; Access Management     Create a new role and give it a meaningful name, such as Lambda_SES (See Figure 3). We will later select the role under this name.                              Figure 3: Create a new role - Step 1     As the role type select AWS Lambda (See Figure 4). This type allows AWS Lambda functions to access other AWS services.                              Figure 4: Create a new role - Step 2     Enter SES into the Filter to only show policies that affect the Amazon Simple Email Service. Select the policy AmazonSESFullAccess and click on Next Step to proceed (See Figure 5).                              Figure 5: Create a new role - Step 3     Review the settings for the role to be created and confirm them by clicking on Create Role (Figure 6).                              Figure 6: Create a new role - Step 4     This completes creating the AWS IAM role for AWS Lambda. If you decide to use a different AWS service instead of SES, e.g. DynamoDB for storing the text messages, you will need to adapt your IAM role’s policy.   Create an Amazon API Gateway endpoint   We need to expose the AWS Lambda function via a REST API endpoint, so that Flowroute can invoke the function. Flowroute will send a POST message to the endpoint. Therefore we will solely allow POST messages to our AWS Lambda function.   Within the AWS Console, head over to the Amazon API Gateway section and create a new API. Give it a meaningful name and description (See Figure 7).                              Figure 7: Create a new API Gateway     Node.js function in AWS Lambda   Next we will create a Node.js function in AWS Lambda that will perform the actual job of re-formatting the incoming text message and sending it out as E-Mail.   For this navigate to the Lambda service within the Compute section in your console. Create a new function and skip the blueprint selection.   Under Configure Triggers select the API Gateway API name that you created in the previous section. Select prod for the Deployment Stage and Open for Security (See Figure 8). Flowroute will later send a POST message to this endpoint and trigger the Lambda function.                              Figure 8: AWS Lambda - Configure triggers     Next, configure the new function, by providing a meaning-full name, e.g. inboundTextMessage, entering a description and choosing Node.js 4.3 as the Runtime (See Figure 9)                              Figure 9: Create a Lambda function     Enter the code below into the inline code window. You can also find the code on Github.   The first section sets up the AWS SDK for interfacing with Amazon SES. The next section extracts text message values out of the posted data from Flowroute. The next section formats the outgoing E-Mail message. The last section sends the E-Mail message via Amazon SES.   exports.handler = function(event, context, callback) {     var AWS = require('aws-sdk');     AWS.config.region = 'us-west-2';     var SES = new AWS.SES();      var obj = JSON.parse(event.body);     var to = (obj.to === undefined ? 'Undefined (to)' : obj.to);     var body = (obj.body === undefined ? 'Undefined (body)' : obj.body);     var from = (obj.from === undefined ? 'Undefined (from)' : obj.from);     var id = (obj.id === undefined ? 'Undefined (id)' : obj.id);      var emailParams = {         Destination: {             ToAddresses: [\"recipient@edge-cloud.net\"]         },         Message: {             Body: {                 Html: {                     Data: \"From: \" + from + \"&lt;br&gt;To: \" + to + \"&lt;br&gt;Message: \" + body + \"&lt;p&gt;ID: \" + id                 },                 Text: {                     Data: \"From: \" + from + \"\\nTo: \" + to + \"\\nMessage: \" + body + \"\\n\\n\" + id                 }             },             Subject: {                 Data: \"Text message to \" + to             }         },         Source: from + \"&lt;sms@edge-cloud.net&gt;\"     };      var email = SES.sendEmail(emailParams, function(err, data){         if(err) {             var responseBody = {                 message: \"Message failed\"             };              var response = {                 statusCode: '500',                 body: JSON.stringify(responseBody)             };             console.log(\"response: \" + JSON.stringify(response))             context.succeed(response);         } else {             var responseBody = {                 message: \"Message suceeded\"             };              var response = {                 statusCode: '200',                 body: JSON.stringify(responseBody)             };             console.log(\"response: \" + JSON.stringify(response))             context.succeed(response);             }     }); };   Change the E-Mail addresses to whatever E-Mail addresses you would like to use. You might also want to adapt the AWS region in line 3, in case you use Amazon Simple Email Service in a different region than US-West-2.   Finish the configuration step by selecting the previously created Lambda_SES role and increasing the timeout to 15 seconds (See Figure 10).                              Figure 10: Create a Lambda function - Advanced Settings     Review the settings for the function to be created and confirm them by clicking on Create function (Figure 11).                              Figure 11: Create a Lambda function - Confirm     This completes setting up the AWS Lambda function for processing text messages from Flowroute.   Testing the AWS Lambda function   Next we want to test the functionality of the newly created AWS Lambda function, before proceeding any further.   Click on the Test button in the upper left corner (See Figure 12).                              Figure 12: Complete Lambda function     Remove the current data in the Input test event window and replace it with this test event data.   {   \"body\": \"{\\\"to\\\": \\\"12066418000\\\", \\\"body\\\": \\\"Hello there from Flowroute!\\\", \\\"from\\\": \\\"18553569768\\\", \\\"id\\\": \\\"mdr1-febb118b9b034338adfc662a8c02fd88\\\" }\",   \"resource\": \"/{proxy+}\",   \"httpMethod\": \"POST\" }   This test event data is derived from the Flowroute Messaging API documentation and is an example for what Lambda will receive from AWS API Gateway.   Execute the test by selecting Save and test (See Figure 13).                              Figure 13: Input test event     If everything was setup correctly, you should see a successful test execution with the status Message delivered (See Figure 14). Also you should receive an E-Mail message.                              Figure 14: Successful test execution     If this test failed, you need to fix the issue before proceeding. Most likely the issue was caused by a mis-configuration of Amazon Simple Email Service.   Looking up the API endpoint for the AWS Lambda function   Note down the API endpoint URL from the Lambda function’s trigger tab for further usage (See Figure 15). As Flowroute doesn’t support any of the authentication mechanism provided by Amazon API gateway yet, you need to treat the entire URL as a secret. Everyone with knowledge of the URL could invoke your Lambda function and pretend to send you a text message.                              Figure 15: Retrieve API endpoint URL     The AWS Lambda API endpoint URL will be used with Flowroute in a subsequent step.   Configuring Flowroute   As a last step you need to configure Flowroute to use the new web service that we just created. For this head over to the Flowroute web console and open the tab Preferences -&gt; API Control.   Enter the URL of your Lambda API endpoint into the SMS Callback field and save the configuration via a click on + Enable SMS (See Figure 16).                              Figure 16: Configure Flowroute SMS Callback     Congratulations! You completed your setup. Now send yourself a text message to your Flowroute DID and see you AWS Lambda turns it into an E-Mail message.   Summary   Flowroute provides a great API for sending and receiving text messages. This blog post showed you how to use AWS Lambda to process these messages and forward them as E-Mail using AWS SES.  ","categories": ["EdgeCloud"],
        "tags": ["AWS"],
        "url": "https://www.edge-cloud.net/2016/05/16/processing-flowroute-text-messages-aws-lambda/",
        "teaser":null},{
        "title": "Amazon Alexa Skill for San Francisco Muni",
        "excerpt":"Alexa is the voice service that powers the Amazon Echo and provides capabilities, or skills, that enable users to interact with the device by using voice. Examples of built-in skills include the ability to play music, answer general questions, set an alarm or timer, and more.   But the real power of Alexa is the Alexa Skills Kit, which is a collection of self-service APIs, tools, documentation and code samples that make it fast and easy for you to add skills to Alexa.   This post will show you how to use the NextBus API in an Alexa Skill to provide you information about SF Muni’s public transit options in San Francisco.   You will be able to use this Alexa skill from a standard Amazon Echo and Echo Dot device, but also from a Fire TV device to ask for up to date transit options around you.   Once everything is up and running you will be able to ask your Echo device “Alexa, ask Muni for the next train” for which it will then e.g. reply with “The next metros are N in 6 minutes, KT in 9 minutes, and N in 16 minutes.”   Pre-Requisites   Before you get started, here is what you need:      An Amazon Echo, Echo Dot, or Fire TV device to use your custom Alexa skill.   An Amazon AWS account, where you will use AWS Lambda to run the Alexa skill.   An Amazon Developer account for creating your Alexa skill.   It is also highly recommended that you have used Amazon AWS before and that you have basic coding skills.   Source code and customization   You can find all necessary source code for this custom skill on GitHub.   The “src” directory includes all necessary files for the AWS Lambda function, while the files in the directory “speechAssets” will be used for the Alexa Skill kit.   Keep in mind that the locations of the Muni stops for which this Alexa Skill provides updates is hard-coded. Before getting started you might therefore want to adapt the location of the Muni information to your specific location as my example uses the San Francisco Caltrain station at 4th &amp; King as the location.   To do so, have a look at the file index.js first, which includes the function “findMuniStop” at the bottom:   function findMuniStop(type,line,direction) { \tswitch(line){ \t\tcase \"N\": \t\t\tvar stopId = \"&amp;stops=N|5240\"; \t\t\tvar typeName = \"outbound N metros\"; \t\t\tbreak; \t\tcase \"KT\": \t\tcase \"K\": \t\t\tswitch(direction){ \t\t\t\tcase \"outbound\": \t\t\t\t\tvar stopId = \"&amp;stops=KT|7166\"; \t\t\t\t\tvar typeName = \"outbound KT metros\"; \t\t\t\t\tbreak; \t\t\t\tdefault: \t\t\t\t\tvar stopId = \"&amp;stops=KT|7397\"; \t\t\t\t\tvar typeName = \"inbound KT metros\"; \t\t\t} \t\t\tbreak; \t\tcase \"30\": \t\t\tvar stopId = \"&amp;stops=30|7235\"; \t\t\tvar typeName = \"outbound 30 buses\"; \t\t\tbreak; \t\tcase \"45\": \t\t\tvar stopId = \"&amp;stops=45|7235\"; \t\t\tvar typeName = \"outbound 45 buses\"; \t\t\tbreak; \t\tcase \"10\": \t\t\tvar stopId = \"&amp;stops=10|6695\"; \t\t\tvar typeName = \"inbound 10 buses\"; \t\t\tbreak; \t\tcase \"82\": \t\tcase \"82X\": \t\tcase \"82 express\": \t\t\tvar stopId = \"&amp;stops=82X|3164\"; \t\t\tvar typeName = \"inbound 82X buses\"; \t\t\tbreak; \t\tdefault: \t\t\tswitch(type){ \t\t\t\tcase \"bus\": \t\t\t\tcase \"buses\": \t\t\t\t\tvar stopId = \"&amp;stops=30|7235&amp;stops=45|7235\"; \t\t\t\t\tvar typeName = \"buses\"; \t\t\t\tbreak; \t\t\t\tdefault: \t\t\t\t\tvar stopId = \"&amp;stops=N|5240&amp;stops=KT|7166\"; \t\t\t\t\tvar typeName = \"metros\";\t\t\t\t\t\t \t\t\t} \t}  \treturn [stopId, typeName]; }   In my case I’m interested in the metro lines N and KT, as well as the buses 30, 45, 10, and 82X. Also I have a set of stops set as default value, when not specifying a mode of transportation or line. Change the lines and stops according to your needs.   You can lookup the SF Muni stop IDs via the NextBus API, e.g. for the K/T line.   Also, you will also need to update the custom slot types for TRANSIT_LINE in the “speechAssets” directory with your Muni lines.   Creating the AWS Lambda Function for a Custom Skill   First you need to create the AWS Lambda Function for your custom skill in Amazon AWS. Refer to the Alexa documentation Creating an AWS Lambda Function for a Custom Skill for a detailed walk through of this process.   Below you can find a brief walk through for the necessary steps   Go to the AWS Console and click on the Lambda service link. Ensure you are in US-East (N. Virgina) as Alexa can only use Lambda in this US AWS region.   Click on the Create a Lambda Function or Get Started Now button. When presented with sample blueprints, choose Skip.   Under Configure Triggers select the Alexa Skills Kit and click on Next (See Figure 1).                              Figure 1: Alexa Skills Kit as AWS Lambda trigger     Combine the content of the src folder into a ZIP file. Make sure that the ZIP file does not contain the “src” directory itself, but instead the files “AlexaSkill.js” and “index.js” need to be at the root level of the ZIP archive.   Name the Lambda Function “Alexa_MuniMetro_Skill” and provide a description if wanted. Keep the Runtime as “Node.js 4.3” and upload the previously created ZIP file.   Keep the Handler as “index.handler”, which refers to the main js file in the ZIP archive (See Figure 2).                              Figure 2: Create an AWS Lambda function     Create via a “custom role” a basic execution role for Lambda, which will have the following policy by default:   {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": [         \"logs:CreateLogGroup\",         \"logs:CreateLogStream\",         \"logs:PutLogEvents\"       ],       \"Resource\": \"arn:aws:logs:*:*:*\"     }   ] }   Leave all other settings as default, click Next and afterwards Create function (See Figure 3).                              Figure 3: AWS Lambda function for Alexa Skill     After the Lambda function has been created successfully, note down the ARN, which is later needed in the Alexa skill setup (See Figure 4).                              Figure 4: ARN of the AWS Lambda function     This complete the creation of the AWS Lambda function.   Setting up the custom Alexa Skill   Next we will need to setup a custom Alexa Skill that interacts with the AWS Lambda function above. For this login to your Amazon Developer Account and navigate to the Alexa Skills Kit section.   Add a New Skill. Leave the Language and Skill Type at the default value. Set Next Muni Bus or Metro as the skill name and muni as the invocation name (See Figure 5).   The Invocation Name is what is used to activate this particular skill. For example you would say: “Alexa, ask Muni when the next bus is coming.”   Click Next to go the next screen.                              Figure 5: Create a new Alexa Skill     Next you have to provide information for the voice interaction model. You can find all these information within the speechAssets.   First create the three custom slot types TRANSIT_DIRECTION, TRANSIT_LINE, and TRANSIT_TYPE and fill them with the information from the customSlotTypes folder.   Next, copy the content of IntentSchema.json into the Intent Schema and the content of SampleUtterances.txt into the Sample Utterances field.   Click Next to go to the next screen (See Figure 6).                              Figure 6: Voice interaction model     Chose Lambda as the Service Endpoint Type and specify the ARN to be in North America. Next copy the ARN for the Lambda function that you previously created into the field (See Figure 7).   Leave the Account Linking as is and click *Next.                              Figure 7: AWS Lambda as service endpoint     Make sure that your skill is enabled for testing on your account (See Figure 8). This will allow you to use this skill in your account without having to publish it.   Before invoking your Echo or Fire TV device, you can also use the Service Simulator to test your skill: In the Enter Utterence field enter for the next train and click on Ask Next Muni Bus or Metro.   You will see the request that is send off to Lambda, as well as the response received from Lambda. If everything is working correctly you should an answer like “The next metros are N in 6 minutes, KT in 9 minutes, and N in 16 minutes.” (See Figure 8).                              Figure 8: Test your Alexa Skill     Don’t fill out the sections Publishing Information and Privacy &amp; Compliance, as you cannot submit this skill for Certification. Instead only you can use it from your account.   You are now able to start using the skill on your device! You can also go to your Echo companion webpage http://echo.amazon.com/#skills and see the skill enabled.   Using the Alexa Skill   There are two main capabilities that this Alexa Skill will provide you:      Ask for the next bus or metro. The Alexa Skill will tell you the next three connections from a pre-configured stop ID. If there is a system notification, it will warn you about it as well.   Ask if there is a system notification. The Alexa Skill will reply with the system messages that are of priority High. These usually refer to service outages or impactes and don’t include “Elevator outages”.   Here are some examples on what you can ask the Alexa Skill about:   Q: “Alexa, ask Muni when the next metros are coming.”;  A: “The next metros are N in 1 minute, KT in 6 minutes, and N in 9 minutes.”;   Q: “Alexa, ask Muni for the next 45.”  A: “The next outbound 45 buses are 45 in 5 minutes, 45 in 17 minutes, and 45 in 32 minutes.”   Q: “Alexa, ask Muni for the next inbound 10.”  A: “The next inbound 10 buses are 10 in 3 minutes, 10 in 21 minutes, and 10 in 34 minutes.”   Q: “Alexa, ask Muni if there is a problem with the 45 bus.”  A: “There is currently no service message for the outbound 45 buses.”   Limitations   Please keep in mind that the prediction data for the next buses and metros, but also the service messages come from NextBus via their API. The service messages are provided by SF Muni to NextBus.   Unfortunately providing these service messages to NextBus is a manual step for Muni, which means that it is very frequently forgotten. Therefore the Alexa Skill might happily tell you “There is currently no service message for the metros.”, while reality looks quite different (See Figure 9).                              Figure 9: SF Muni Metro service interruption     Also predictions for buses and metros at the beginning of a line - which is the case of the N, 30 and 45 in my case - are rather less reliable.   Summary   This post showed you how to create your own custom Amazon Alexa Skill to get next bus and next metro information for San Francisco’s Muni. You can use this custom Alexa Skill with an Amazon Echo or Fire TV device.  ","categories": ["EdgeCloud"],
        "tags": ["Alexa","Echo"],
        "url": "https://www.edge-cloud.net/2016/10/10/amazon-alexa-skill-san-francisco-muni/",
        "teaser":null},{
        "title": "AWS CloudFront meta data in Google Analytics",
        "excerpt":"AWS CloudFront recently enabled support for the latest HTTP protocol version with HTTP/2 and for the latest Internet Protocol Version with IPv6. Website owners using AWS CloudFront and having enabled HTTP/2 and/or IPv6 on their distribution might now wonder how many guests use either technology. Or you might want to know where these IPv6 users are coming from. Are they really mostly coming from Mobile networks?   This post will show you how to use some Javascript code embedded in your web-page together with a small AWS Lambda script to push information about the IP version, HTTP version and CloudFront edge location into Google Analytics for each visitor.   All necessary code can be found on GitHub.   In a previous post I’ve already shown how to achieve a similar outcome while using CloudFlare.   Google Analytics Custom Dimensions   In Google Analytics custom dimensions are like the default dimensions, except that you have to fill them with data.   In this case we will need to create three custom dimensions. Each will store different values for each visitor of your website.      IP-Version: This dimension will store the values “IPv4” or “IPv6”.   HTTP-Version: This dimension will store the values “2.0”, “1.1”, or “1.0”.   Edge-Location: This dimension will store the three letter IATA airport code of the CloudFront edge location, e.g. “SFO” for San Francisco.   Setup   Create custom dimensions in Google Analytics   First, set up the custom dimensions in Google Analytics:      Sign in to Google Analytics.   Select the Admin tab and navigate to the property to which you want to add custom dimensions.   In the Property column, click Custom Definitions, then click Custom Dimensions.   Click New Custom Dimension.   Add a Name.   This can be any string, but use something unique so it’s not confused with any other dimension or metric in your reports. Only you will see this name in the Google Analytics page.   Select the Scope.   Choose to track at the Hit, Session, User, or Product level. For this scenario I recommend to choose Hit or rather Session.   Check the Active box to start collecting data and see the dimension in your reports right away. To create the dimension but have it remain inactive, uncheck the box.   Click Create.   Note down the dimension ID from the displayed example codes. (See Figure 1)                              Figure 1: Create Google Analytics Custom Dimensions     AWS Lambda Setup   We will use a small AWS Lambda script to extract the information about CloudFront edge location, IP version, and HTTP version from incoming requests.   The script can be found on GitHub.   Create a new AWS Lambda Function named “CloudFrontMetaData” and using the runtime Node.js 4.3. Use the content of the “CloudFrontMetaData.nodejs” file as the source code for Lambda and keep the Handler as “index.handler”. Create a basic execution role and increase the timeout to 10 seconds (See Figure 2).                              Figure 2: AWS Lambda function to extract meta data from CloudFront     Within the Script the IP-Version is inferred by looking for the character “:” in the source IP address, the IP address that CloudFront sees. If that address does include the character “:”, it is an IPv6 address, otherwise it is an IPv4 address.   The HTTP-Version information can be extracted from the “Via” header, which is used by a proxy like Cloudfront to indicate the intermediate protocols.   The edge location can be determined with a reverse DNS lookup, based on the source IP address that Amazon API Gateway sees from CloudFront.   API Gateway Setup   Next we need to make the above AWS Lambda function accessible via a URL. This can easily be done using Amazon API Gateway. To simplify the setup you can use a prepared swagger formatted file, which can be found on GitHub.   Within the file “CloudWatchMetaData-swagger.json” change the ARN on line 50 to the ARN of your Lambda function.   Create a new Amazon API Gateway using “Import from Swagger” and pasting the content of the above file.   This will automatically create the API Gateway with all it’s settings for you (See Figure 3).                              Figure 3: Amazon API Gateway created from Swagger definition     Deploy the newly created API into the “Prod” stage and lookup the “Invoke URL” (See Figure 4).                              Figure 4: Invoke URL of the deployed API     Cloudfront Setup   Next you need to place CloudFront in front of the API Gateway URL as the API Gateway neither supports HTTP/2 nor IPv6 at this point. Therefore we need to rely on CloudFront for this task.   The easiest approach is to create a custome path within your existing CloudFront distribution. But you could also create a separate distribution.   In both cases make sure both HTTP/2 and IPv6 are enabled for this distribution.   Within your existing CloudFront distribution create an additional origin with the API gateway Invoke URL, using “HTTPS only” for the “Origin Protocol Policy” (See Figure 5).                              Figure 5: Additional CloudFront origin     Under “Behavior” of the distribution create a path pattern for a path that your are not using, e.g. /cdn-cgi/edge-info, while specifying the API Gateway origin.   Disable caching on this path by setting Minimum TTL, Maximum TTL, and Default TTL to 0 (See Figure 6).                              Figure 6: CloudFront behavior for custom path pattern     After the update to the CloudFront distribution has been completed you should find the following text information under the /cdn-cgi/edge-info URL:   ipver=IPv6 httpver=2.0 edgeloc=sfo   Embed the Google Analytics Tracking Code   Next we need to update the Google Analytics tracking code within the website, in order to fill the newly created custom dimensions with data. This tracking code has to be placed between the code for creating the Google Analytics tracker, which looks like this: gaTracker('create','UA-12345678-1','auto');, and the code to submit the tracker, which looks like this gaTracker('send','pageview');.   If you are using WordPress the easiest way to include the custom tracking code is by using the Google Analytics by Yoast plugin. This plugin allows you under Advanced &gt; Custom Code to embed the below code right away and without any coding requirements.   The below JavaScript code will read the values from /cdn-cgi/edge-info, extract the information we are interested it and push it into the Google Analytics custom dimension variables. Ensure that the numeric IDs of these custom dimension variables matches what you have created in above steps.   Embedded the code from here into your website along with the standard Google Analytics tracking code. This custom JavaScript code will be executed by a visitor, collect metrics data from /cdn-cgi/edge-info and push it into Google Analytics.   A few hours after embedding the code you should see your first custom dimension data in Google Analytics.   Use the custom dimension in Google Analytics   Now you can use the newly created custom in Google Analytics to find out interesting information about your visitors:   You can look at the sessions by IP-Version to figure out the percentage of IPv6 user on your website (See Figure 7).                              Figure 7: Sessions by IP-version     Or you could see how many of your visitors can benefit of the improved performance of HTTP/2 over HTTP 1.1 (See Figure 8).                              Figure 8: Sessions by HTTP version     You could look at the top CloudFront edge locations serving your side (See Figure 9).                              Figure 9: Hits and Sessions by Edge-Location     While having a look at which cities your IPv6 enabled visitors are coming from, you will notice that AWS CloudFront has not yet completed the turn up of all ASNs (See Figure 10).                              Figure 10: Location of IPv6 visitors     Summary   This article has shown you, that you can easily built your own Real User Monitoring system with Google Analytics Custom dimensions, some simple JavaScript code and AWS Lambda. It allows you to extract many interesting metrics out of your CloudFront usage and make it available in Google Analytics for further data analysis.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","CloudFront","Google-Analytics","HTTP/2","IPv6"],
        "url": "https://www.edge-cloud.net/2016/10/24/cloudfront-metadata-google-analytics/",
        "teaser":null},{
        "title": "IPv6 with Amazon Web Services VPC and EC2",
        "excerpt":"On December 1st, 2016 AWS released IPv6 Support for EC2 Instances in Virtual Private Clouds (VPCs), currently limited to the US-East-2 (Ohio) region.   This post will show you how to enable your existing Default VPC and Default Subnets for IPv6.   Background   Amazon Virtual Private Cloud (Amazon VPC) enables you to launch Amazon Web Services (AWS) resources into a virtual network that you’ve defined. This virtual network closely resembles a traditional network that you’d operate in your own data center, with the benefits of using the scalable infrastructure of AWS. A virtual private cloud (VPC) is logically isolated from other virtual networks in the AWS Cloud and allows you to launch your AWS resources, such as Amazon EC2 instances, isolated from other VPCs within your own or other accounts.   A default VPC allows you to jump-start your AWS EC2 experience, without having to worry about VPCs and subnets. It is ready for you to use — you can immediately start launching instances into your default VPC without having to perform any additional configuration steps.   Unfortunately with the introduction of IPv6 within VPC, you do need to once perform some manual configuration steps in order to enable your VPC for IPv6. This blog post will show you the minimal required steps to do so.   Pre-Requisites   Please note that as of this writing IPv6 within VPC is only in the US-East-2 (Ohio) region. You therefore need to perform the below configurations in this region. Within your AWS Console, navigate to VPC section within the US-East-2 (Ohio) region.   Enable Default VPC and Default Subnets for IPv6   Edit VPC CIDR for IPv6   Each default VPC comes with a default IPv4 CIDR block of 172.31.0.0/16, but no default IPv6 CIDR block (See Figure 1). Therefore the first step is to allocated a /56 IPv6 block from the Amazon provided IPv6 block to your VPC.   First, right-click on your Default VPC and select Edit CIDRS (See Figure 1).                              Figure 1: Edit VPC CIDRs.     Within the VPC IPv6 CIDRS section click on Add IPv6 CIDR to allocate a random /56 CIDR from the Amazon IPv6 address pool to your Default VPC (See Figure 2).                              Figure 2: Edit VPC IPv6 CIDRs     All your IPv6 subnets will come from this /56 IPv6 CIDR block.   Edit Subnet CIDRs   Your default VPC will come with one subnet per Availability Zone (AZ). As US-East-2 (Ohio) has three availability zones, you will have the subnets 172.31.0.0/20 for the AZ us-east-2a, 172.31.16.0/20 for the AZ us-east-2b, and 172.31.32.0/20 for the AZ us-east-2c. In this step we will want to assign each of these subnets a /64 IPv6 CIDR block.   Navigate to the Subnets. Start with a right-click on the first subnet and select Edit IPv6 CIDRs (See Figure 3).                              Figure 3: Edit a Subnet IPv6 CIDRs     Within the VPC IPv6 CIDRs section click on Add IPv6 CIDR. Confirm your selection by clicking on the check mark to the right of the line (See Figure 4).                              Figure 4: Subnet CIDR blocks     Close the window and repeat the above steps for the remaining two subnets. After you’re done, each subnet should have a distinct /64 CIDR allocated to it (See Figure 5).                              Figure 5: Default Subnets with IPv6 CIDRs     Enable auto-assign of IPv6 addresses   By default each EC2 instance launched into the default VPC will be assigned a public IPv4 address. The instance can therefore not only reach other hosts on the Internet, but is also reachable from them Internet. The same is not true for IPv6. Instead your default subnets have to be enabled to automatically assign an IPv6 to each newly created EC2 instance within the default VPC. With the configuration in the last step of this blog post, this will ensure that each new EC2 instance within your default VPC will have outbound connectivity over IPv6, but will also be reachable from the Internet over IPv6.   Perform a right-click on your first default subnet and select Modify auto-assign IP settings (See Figure 6).                              Figure 6: Modify auto-assign IP settings     Next, enable the tick box for Enable auto-assign IPv6 address (See Figure 7).                              Figure 7: Enable auto-assign IPv6 address     Close the window and repeat the above steps for the remaining two subnets.   Update the route table   As a last step we need to update the route table for the default VPC with a default route to the Internet Gateway. This will allow our EC2 instances to be able to reach hosts on the Internet via IPv6, but also to be reachable from the Internet.   In case you only want your EC2 instances to reach the outside world but not be reachable via the Internet over IPv6, you need to first create an “Egress Only Internet Gateway”. This concept is new to VPC and only available for IPv6. It allows you to block incoming traffic while still allowing outbound traffic (think of it as an Internet Gateway mated to a Security Group).   Navigate to the “Route Tables”, select the route table for your default VPC and click Edit. Add a route with the Destination of “::/” and the target of your Internet Gateway (See Figure 8).                              Figure 8: Default route for IPv6     Launching EC2 instances   You can now head over to EC2 within the AWS Console and launch your first IPv6-enabled EC2 instance.   AWS uses stateful IPv6 address assignment. Therefore IPv6 addresses are assigned via DHCPv6 and not Stateless Autoconfiguration (SLAAC). Most operating systems (OS) are not prepared to acquire an IPv6 address via DHCPv6 out of the box and AWS has unfortunately only updated the AMI for Amazon Linux to do so. The common AMI for Ubuntu 16.04 will fail to acquire an IPv6 address out of the box and the Windows 2016 AMI should be updated to disable Teredo.   Also older versions of Ubuntu Linux and Windows 2016 might have issues picking up an IPv6 address at all. Here it is recommended to use the latest AMI version.   Ubuntu 16.04   Under Ubuntu 16.04 you will need to update the new file “/etc/network/interfaces.d/60-eth0-with-ipv6.cfg” and add the following content:   iface eth0 inet6 dhcp   Afterwards restart the network stack with sudo service networking restart.   Using the command ip -6 address you will see the IPv6 address assigned to eth0 via DHCPv6:   ubuntu@ip-172-31-8-35:~$ ip -6 address 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 state UNKNOWN qlen 1     inet6 ::1/128 scope host        valid_lft forever preferred_lft forever 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 state UP qlen 1000     inet6 2600:1f16:dc1:f800:4e68:4a28:f5b6:22bd/128 scope global        valid_lft forever preferred_lft forever     inet6 fe80::6d:ceff:fe42:4535/64 scope link        valid_lft forever preferred_lft forever   You can also pass the following script via User Data, when creating a new Ubuntu instance:   #!/bin/bash echo \"iface eth0 inet6 dhcp\" &gt;&gt; /etc/network/interfaces.d/60-eth0-with-ipv6.cfg dhclient -6 -N   This will setup DHCPv6 persistently for subsequent boots and acquire an IPv6 during the initial boot.   Ubuntu 14.04   The above instructions for Ubuntu 16.04 do not work under Ubuntu 14.04 and actually render an EC2 host inaccessible.   Instead you will need to update the file /etc/network/interfaces.d/eth0.cfg to look like this:   auto eth0 iface eth0 inet dhcp     up dhclient -6 $IFACE   Here, you can also pass the following script via User Data, when creating a new Ubuntu instance:   #!/bin/bash echo \"    up dhclient -6 $IFACE\" &gt;&gt; /etc/network/interfaces.d/eth0.cfg dhclient -6 -N   This will setup DHCPv6 persistently for subsequent boots and acquire an IPv6 during the initial boot.   RHEL 7 / CentOS 7   If you’re using Redhat Enterprise Linux 7 or CentOS 7 you will need to update two files:   First update /etc/sysconfig/network to look like this:   NETWORKING=yes NETWORKING_IPV6=yes   Next update /etc/sysconfig/network-scripts/ifcfg-eth0 to this content:   DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" IPV6INIT=yes DHCPV6C=yes NM_CONTROLLED=no ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\"   Same as with Ubuntu you can also pass the following script via User Data, when creating a new RHEL7/CentOS7 instance:   #!/bin/bash echo \"NETWORKING_IPV6=yes\" &gt;&gt; /etc/sysconfig/network echo -e \"BOOTPROTO=\\\"dhcp\\\"\\nIPV6INIT=\\\"yes\\\"\\nDHCPV6C=\\\"yes\\\"\\nNM_CONTROLLED=\\\"no\\\"\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 service network restart   This will setup DHCPv6 persistently for subsequent boots and acquire an IPv6 during the initial boot.   RHEL 6 / CentOS 6   While RHEL 6 and CentOS should follow the same process as RHEL 7 and CentOS 7, it includes a bug which prevents DHCPv6 from being usable and has only been fixed in very recent versions. Therefore before you can use the above configuration you need to change the file /etc/sysconfig/network-scripts/ifup-eth and change line 327 from if /sbin/dhclient “$DHCLIENTARGS”; then to if /sbin/dhclient $DHCLIENTARGS; then. Yes, that’s correct: Just drop the double quotes.   Windows Server 2016   The AMI for Windows Server 2016 configured to create an IPv6 over IPv4 tunnel via Teredo. The security implications for using IPv6 over IPv4 tunnels are described in RFC7123, while RFC6169 clearly spells out the security concerns with IPv6 tunnels.   Instead we want to solely use a native IPv6 address and disable any IPv6 over IPv4 tunnels.   Open a command prompt in your Windows Server 2016 instance. First remove the Teredo and Isatap tunnel support:   netsh int ipv6 isatap set state disabled netsh interface teredo set state disable   Your Windows host will now solely have IPv6 connectivity via the Ethernet interface.   But you might notice that your Windows instance will prefer IPv4 connections over IPv6. You can easily test this by opening http://test-ipv6.com/ from a browser on the Windows instance.   This non-recommended behavior can be fixed by downloading and running the Microsoft easy fix wizard for “Prefer IPv6 over IPv4 in prefix policies” from the Windows Support Guidance for configuring IPv6 in Windows for advanced users article. It’s the second item in the second row.   Summary   This blog post showed you how to retrofit your AWS Default VPC and subnets for IPv6. It also showed you how to enable Ubuntu 16.04 and Windows Server 2016 to acquire an IPv6 address via DHCPv6.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","IPv6"],
        "url": "https://www.edge-cloud.net/2016/12/12/ipv6-aws-ec2/",
        "teaser":null},{
        "title": "How AT&T broke my IPv6 Internet connection",
        "excerpt":"A few days ago I discovered that my AT&amp;T Internet connection at Home wasn’t working as expected. Websites were loading sluggish and access to Ubuntu’s repo via APT flat-out failed. My first suspicion that there must be an issue with the IPv6 connectivity was confirmed after heading to Test-IPv6.com.   It was time to start the troubleshooting and take a closer look:   First look at RIPE Atlas  I have had a RIPE Atlas probe directly connected to my AT&amp;T Home-gateway for the last 5 years. RIPE Atlas is a global network of hardware devices, called probes and anchors, that actively measure Internet connectivity. Anyone can access this data via Internet traffic maps, streaming data visualisations, and an API. RIPE Atlas users can also perform customised measurements to gain valuable data about their own networks. The data collected by this probe allows me to gain interesting insights into my own Internet connectivity at home, but also the global Internet connectivity.   In this case the status page for my RIPE Atlas probe showed me in plain English that “IPv6 Doesn’t Work Properly” (See Figure 1).                              Figure 1: RIPE Atlas showing that IPv6 isn’t working properly     Measurement targets   The RIPE Atlas error message talks about “stable and cooperating targets (i.e. targets known to respond to pings)”. Looking at the built-in measurements, we can see that these targets include some of the root name server. The root name servers are a authoritative name server for the root zone (“.”) of the Domain Name System (DNS) of the Internet. Due to the important role of these name servers to the overall DNS architecture, these servers are implemented in a highly reliable way, using e.g. Anycast, making them stable anchor points throughout different international locations.   And as all but the “G” root server respond to ICMP echo requests (aka. “Ping”) over IPv4 and IPv6, the root name servers are excellent targets for availability tests. In the case of my RIPE Atlas probe, we can see that all Pings to root name servers over IPv6 are failing, while Pings over IPv4 are successful (See Figure 2).                              Figure 2: Tests to Root DNS server over IPv6 failing while IPv4 is working     This shows that the actual Internet connection is up and at least working as expected for IPv4 traffic, while IPv6 traffic is facing issues. Disabling IPv6 while AT&amp;T fixes the issue should provide a temporary workaround, which isn’t great.   When did things break?   Next, let’s have a look at the question when things broke. Drilling down on the measurements for the Ping test over IPv6 to the “A” root server, allows us to look at the moment when the RIPE Atlas probe was last able to successfully ping this name server. Here we can see that the last successful IPv6 contact to the “A” root name server happened on February 6th, 2019 at 8:53 UTC (See Figure 3).                              Figure 3: Pin-pointing the date and time of the failure     For the other root name servers the last IPv6 contact was also at the same time, indicating a general IPv6 connectivity issue on this AT&amp;T connection.   Where is the fault?   Now that we know that IPv6 is broken and when it broke, let’s try to figure out where things are broken. Is the link between the AT&amp;T home gateway and the next hop upstream router broken or is the culprit further down in the AT&amp;T network? RIPE Atlas can also help answer this question.   To do so we create a One-off measurement from solely the affected Probe to one of the IPv6 addresses of Google Public DNS resolvers at 2001:4860:4860::8888. We could also use any other host that is known to respond to IPv6 pings. Using the RIPE Atlas API, this test can quickly be created via:       curl --dump-header - -H \"Content-Type: application/json\" -H \"Accept:     application/json\" -X POST -d '{       \"definitions\":[       {         \"target\":\"2001:4860:4860::8888\",         \"af\":6,         \"timeout\":4000,         \"description\":\"Traceroute measurement to 2001:4860:4860::8888\",         \"protocol\":\"ICMP\",         \"resolve_on_probe\":false,         \"packets\":3,         \"size\":48,         \"first_hop\":1,         \"max_hops\":32,         \"paris\":16,         \"destination_option_size\":0,         \"hop_by_hop_option_size\":0,         \"dont_fragment\":false,         \"skip_dns_check\":false,         \"type\":\"traceroute\"       }],       \"probes\":[       {         \"value\":\"12345\",         \"type\":\"probes\",         \"requested\":1       }],       \"is_oneoff\":true,       \"bill_to\":\"mymail@edge-cloud.net\"       }' https://atlas.ripe.net/api/v2/measurements//?key=YOUR_KEY_HERE   The result shows the Traceroute timing out after the 3rd IPv6 hop inside the AT&amp;T network (See Figure 4).                              Figure 4: Traceroute to Google’s Public DNS on broken AT&amp;T connection     To have another data point and to understand how a successful IPv6 Traceroute to Google Public DNS resolvers should look like, let’s run the same measurement from a RIPE Atlas probe that is also connected to AT&amp;T - indicated by the same ASN. The RIPE Atlas website allows you to find probes based on various characteristics, such as IPv6 working state, ASN but also location. With this it was easy to find a probe that is connected to AT&amp;T with a working IPv6 connection.   As expected, the traceroute to the Google Public DNS Resolver at 2001:4860:4860::8888 successfully completes after 9 hops (See Figure 5).                              Figure 5: Traceroute to Google’s Public DNS on working AT&amp;T connection     This indicates that the issue is not just on the direct link between my AT&amp;T home gateway and the next hop upstream, but deeper inside the AT&amp;T network. Therefore this problem most likely affects other users within my neighborhood. And of course I was able to find other reports of this issue on the AT&amp;T Forum, DSLReports and Reddit. Here is a small selection:     AT&amp;T Forum: ipv6 broken?   DSLReports: Replacement BGW210 and can’t route IPV6   Reddit: IPv6 Broken on AT&amp;T U-verse Gigapower (Fiber) (Northeast Austin)   Unfortunately the reported troubleshooting attempts from AT&amp;T staff on these posts were pretty useless.   Overall quality of AT&amp;T’s Internet offering   Before finishing up, let’s have a look at the overall quality of the AT&amp;T Internet connection at my home. For this we can compare the availability of the “A” root name server over IPv6 and IPv4 over the last 3 years.   First looking at IPv6, we can see frequent packet loss and even full outages (late 2017) for multiple days (See Figure 6). I would definitely not call this is a stellar performance for an ISP.                              Figure 6: RTT to A-Root Server over IPv6 over 3 years     At least for IPv4 things look a little better since my upgrade to GPON in mid 2017. Since then connectivity has been mosly stable with some packet loss, but no full outages (See Figure 7).                              Figure 7: RTT to A-Root Server over IPv4 over 3 years     Summary   RIPE Atlas provides an awesome tool to keep an eye on the performance and availability of your own Internet connection, but also helps troubleshooting. Keep in mind that for none of the steps shown above I had to be actually at home. You can use RIPE Atlas from anywhere.   I highly recommend you to consider hosting a RIPE Atlas probe yourself. Not only will you benefit from the data that it collects, but it will also benefit the overall Internet community.   At this point I have no hope whatsoever that AT&amp;T will fix this issue anytime soon. Getting in contact with a human at AT&amp;T is already a daunting task and finding someone with knowledge of IPv6 who can actually fix this issue seems impossible. As AT&amp;T also blocks 6in4 tunnels, I’ll be forced to disable IPv6 on my home internet connection for the time being.  ","categories": ["EdgeCloud"],
        "tags": ["IPv6","Network"],
        "url": "https://www.edge-cloud.net/2019/02/13/how-att-broke-my-ipv6/",
        "teaser":null},{
        "title": "Limitations of DNS-based geographic routing",
        "excerpt":"Various Internet services and offerings - such as AWS CloudFront  - rely on geographic routing, also known as geolocation or geo-locational routing. And with Amazon Route 53 you can build your own geographic routing enabled services. The basic concept behind this capability is to return a DNS answer for an endpoint that is physically closest to the requestor. In the case of AWS CloudFront this would be the closest edge location, while for a service that you build yourself it might be the decision between an Amazon Elastic Load Balancer endpoint in the us-west-2 (Oregon) or in the us-east-2 (Ohio) region.   This post will look at how geographic routing works, how you can validate the interaction of it with your service and what the expected limitations of the service are.   Geographic Routing  To understand how geographic routing works we first need to have a look at DNS resolution via DNS Resolver. Figure 1 shows a simplified representation of this mechanism, where a client connects to a local DNS Resolver, which then initiates and sequences the queries that ultimately lead to a full resolution (translation) of the resource sought. While depicted in an overly simplified way, this includes querying the Authoritative Nameserver for the interested record by the DNS Resolver.                              Figure 1: DNS Resolver and authoritative name server     For geographic routing the “magic” happens during this interaction of the DNS Resolver and the Authoritative Nameserver: The Authoritative Nameserver will respond with a record set that depends on the querying DNS Resolver, ideally its location. In order to accomplish this, the Authoritative Nameserver must be able to calculate the desired mapping based on the location information of the DNS Resolver and the desired mapping of a certain location to an origins record set.   With this the Authoritative Nameserver would e.g. respond to the querry for www.example.com with us-west-2.www.example.com (and its subsequent resolution to an actual IPv4 or IPv6 address), while the DNS Resolver’s IP address is believed to be closer to the AWS Oregon region and with us-east-2.www.example.com, in case the Resolver is closer to the AWS Ohio region.   To do so, the Authoritative Nameserver will use the IP address information for the DNS Resolver to calculate the geographic location. A common service provider for IP Address to location mapping data is Maxmind, which is frequently used for this purpose. An example is the Howto Guide for implementing geographical routing with BIND, which incorporates this GeoIP database.   And while AWS does not publicly divulge information about the source of the used IP address mapping data, Amazon Route 53 uses the same principle.   Limitations of the DNS Resolver location information   At this point you probably already guessed that the Authoritative Nameserver making a routing decision based on the DNS Resolvers location might not be the best approach. That’s because Client and Authoritative Nameserver might not actually be geographically close to each other. What if a client in San Francisco, CA is using a DNS Resolver in New York, NY? While we would want this client to be served by e.g. a CDN Edge location in the San Francisco, CA area, due to the location information attached to the DNS Resolver, that client is actually served by a CDN Edge location in New York, NY.   Looking at Figure 1 again, we can see that the cause for this undesired behavior is that the Authoritative Nameserver only “sees” the DNS Resolver via its IP source address. The Authoritative Nameserver is not aware of the client and therefore has no knowledge of its location. Wouldn’t it be great if the Authoritative Nameserver actually received information about the Client as part of the query request? This would allow the Authoritative Nameserver to respond based on the geolocation location information of the Client instead of the DNS Resolver. That’s where ENDS0-Client-Subnet (ECS) comes into the picture.   EDNS0-Client-Subnet extension   EDNS0 is an extension mechanism for the DNS, which expands the size of several parameters of the DNS protocol for increasing functionality of the protocol. One such increased functionality is EDNS0-Client-Subnet (ECS). The EDNS0-Client-Subnet (ECS) extension allows a recursive DNS Resolver to specify the network subnet for the host on which behalf it is making a DNS query. Therefore the Authoritative Nameserver will gain insight into the location of the client based on GeoIP information.   Unfortunately not all DNS Resolvers support EDNS0-Client-Subnet. You can find support in frequently used public resolvers, such as Google’s Public DNS Resolver, OpenDNS, and Quad9. Other public resolvers such as CloudFlare do not support it.   If you want to check if your Resolver supports EDNS0-Client-Subnet (ECS), you can use a special Google debugging hostname as outlined in the AWS Support article “How do I troubleshoot issues with latency-based resource records and Route 53?”. With this hostname the command dig +nocl TXT o-o.myaddr.l.google.com @&lt;DNS Resolver&gt; will not only show you the IPv4 or IPv6 address of the DNS Resolver that is seen by Google’s Authoritative Nameserver. If supported it will also show the client subnet as provided via EDNS0-Client-Subnet (ECS).   Below is an example run from an Amazon EC2 instance within the US-West-2 (Oregon) region against the Google Public DNS resolver:   ubuntu@ubuntu:~$ dig +nocl +short TXT o-o.myaddr.l.google.com @8.8.8.8 \"2607:f8b0:400e:c09::10f\" \"edns0-client-subnet 54.186.33.212/32\"   We can clearly see that Google’s Resolver supports EDNS0-Client-Subnet (ECS).   Similarly on the CDN side support to actually leverage the EDNS0-Client-Subnet provided information is not universal. Amazon CloudFront added support in 2014. Also Amazon Route 53 supports EDNS0-Client-Subnet as part of routing policies.   RIPE Atlas   In this post we will use RIPE Atlas to check the performance of a geographic routing enabled DNS entry.   RIPE Atlas is a global network of hardware devices, called probes and anchors, that actively measure Internet connectivity. Anyone can access this data via Internet traffic maps, streaming data visualisations, and an API. RIPE Atlas users can also perform customised measurements to gain valuable data about their own networks.   I highly recommend you to consider hosting a RIPE Atlas probe yourself. Not only will you benefit from the data that it collects on your Internet connection, but it will also allow you to run customized measurements against various Internet targets. And in the end every additional RIPE Atlas probe will benefit the overall Internet community.   Here we will be using RIPE Atlas customized measurements to investigate the performance of DNS-based geographic routing.   Test Setup   Amazon Route 53 Setup   For this article the test setup will consist of fictional origins where we want to steer traffic to via geographic routing. The test will focus on the US with one origin in the US East coast and one origin in the US West coast.   A third origin will be placed into Kansas. With IP location based data a mostly unknown lake in Kansas takes over a special meaning: The Cheney Reservoir is close to the geographical center of the continental US. As such the IP location company Maxmind places all IP addresses for which it has no more information than that it is located somewhere in the US, at this location. It is estimated that over 600 Million Internet IP addresses point to Cheney Reservoir. In the past Maxmind placed these IP addresses into the backyard of a Kansas family, whose life was turned upside down as a result.   We will model this setup with AWS Route 53 Geoproximity routing, where the AWS regions US-East-2 (Ohio) - as the US East coast location - and US-West-2 (Oregon) - as the US West coast location - receive neutral bias of 0. The origin at Cheney Reservoir receives a large negative bias, to create a small circle of a few miles around this area.   The expected geoproximity routing should look like depicted in Figure 2.                              Figure 2: Desired Geoproximity Routing     The idea behind this test setup is to create a TXT DNS record which will return one of possible three results depending on the Client DNS resolver and EDNS0-Client-Subnet provided location:     US-West-2 for clients or resolvers located closest to the AWS region US-West-2 (Oregon) based on IP Geolocation data.   US-East-2 for clients or resolvers located closest to the AWS region US-East-2 (Ohio) based on IP Geolocation data.   Unknown for clients or resolvers where IP Geolocation data only indicates them to be somewhere in the US and therefore places them into Kansas.   In the test setup here the “Unknown” location for IP Geolocation data (yellow dot in Figure 2) is within the area that would be steered towards the US-East coast origin. Therefore even clients located in Portland, OR might get directed to the Ohio origin if their resolver’s or EDNS0-provided client subnet cannot be correctly located. Breaking these clients out separately in the test setup will allow us to visualize this issue later.   RIPE Atlas setup   Next we create a RIPE Atlas customized measurement that will leverage up to 500 randomly selected probes within the US against the DNS hostname that was setup for geographic routing with Route 53 above. We expect each probe to report back a result of either “US-West-2”, “US-East-2”, or “Unknown”.   Similar the same set of up to 500 US-based RIPE Atlas probes will be used in a second customized DNS measurement. This measurement will resolve the above introduced TXT-based record for thr special hostname of TXT o-o.myaddr.l.google.com. With this RIPE Atlas measurement we can find out for each probe whether EDNS0-Client-Subnet (ECS) is supported and what the provided subnet is. Also we can find out the IP address of the DNS Resolver for each probe as seen by the Authoritative Nameserver. This information is especially of interest for the case where the DNS Resolver does not support EDNS0-Client-Subnet (ECS).   Test Results  Overview  With some Python code the gathered results can easily be turned into a GeoJSON file (See Figure 3).                              Figure 3: Mapping of RIPE Atlas probes to Origins     The location of each RIPE Atlas probe as reported by the probe’s host is leveraged in this visualization. The pin representing the location is colored in depending on the result for the Route 53 geographic routing test:      Red pin: Represents a response by Route 53 of “US-West-2” and would therefore route traffic from this probe to the AWS US-West-2 (Oregon) region. In this setup this represents 29% of the probes.   Blue pin: Represents a response by Route 53 of “US-East-2” and would therefore route traffic from this probe to the AWS US-East-2 (Ohio) region. Here this represents 59% of the probes.   Green pin: Represents a response by Route 53 of “Unknown”. While traffic from this probe would also be routed to the AWS US-East-2 (Ohio) region, this is effectively due to Route 53 not being able to locate the probe beyond “somewhere in the US”. For this setup 12% of the probes could therefore not be located.   We can see that most of the US-East coast based probes would correctly route to the US-East coast origin (depicted in blue) and most of the US-West coast probes would also correctly route to the US-West coast origin (depicted in red). But we do see a few blue pins on the US-West coast, meaning that the corresponding probe would be routed to the US-East coast instead of the closer US-West coast. Similarly we also see a few red pins on the US-East coast. This clearly shows some non-optimal routing behavior. Later on we will look in more detail into the reasons for this behavior.   Probe Details  The way that I’ve setup the GeoJSON is to also display relevant information for each of the RIPE Atlas Probes (Figure 4).                              Figure 4: Detail view of RIPE Atlas probe results     After clicking on one of the pins you’re able to see:      Atlas-ID: The RIPE Atlas ID of the probe linked to RIPE ATLAS API’s “Probe Detail” view.   Resolver: IPv4 or IPv6 address of the DNS Resolver for the probe as seen by an Authoritative Nameserver.   Client Subnet: EDNS0-Client-Subnet (ECS) provided client subnet if supported by the probe’s resolver.   Origin: Result of the DNS lookup against the Amazon Route 53 test entry. This will either be “US-West-2”, “US-East-2”, or “Unknown” and corresponds to the color of the pin.   Explore the GeoJSON   You can explore this GeoJSON yourself below, as it is published as a Gist to Github.     Give it a try and see what you’ll discover!   The good, bad and ugly   Let’s drill down on four RIPE Atlas probes to explore how geographic routing works and what the limitations are (See Figure 5).                              Figure 5: Zoom into subset of RIPE Atlas probe results     These probes are all located in the San Diego, CA area and should therefore be routed to the US-West-2 location, represented by a red pin on the map. This location and the resulting probes were selected as they represent typical cases observed with geographic routing.   We will be using Maxmind’s GeoIP2 City Database Demo to lookup GeoIP data associated with the IPv4 and IPv6 addresses that are discovered for these probes as part of this test.           Location 1: This probe is correctly routed to the US-West-2 origin. The DNS Resolver IPv4 address for this probe is located in The Dalles, Oregon and also resides on Google’s Autonomous System Number (ASN). It therefore appears to be one of Google’s Public DNS Resolver nodes. Further we can see that for this RIPE Atlas probe the client subnet is provided. Using Maxmind the corresponding IPv4 address can be mapped to the location Coronado, California and the ISP to Cox. This location is therefore an excellent example for how the EDNS0-Client-Subnet (ECS) improves the end-user experience, while using a DNS resolver that is physically located far away from the actual customer location.            Location 2: This probe is also correctly routed to the US-West-2 origin. Based on Maxmind data the DNS Resolver IPv4 address is located in Palo Alto, California and with the ISP WoodyNet. In this case the DNS Resolver does not support EDNS0-Client-Subnet (ECS) and therefore Amazon Route 53 is forced to make a routing decision solely based on the DNS Resolver location information. Despite the geographic distance of a few hundred miles between client and DNS resolver, the DNS geographic routing is nevertheless correct here.            Location 3: This probe is incorrectly routed to the US-East-2 origin and therefore provides the end-user a reduced experience. The DNS Resolver’s IPv4 address is located in Saint Paul, Minnesota belonging to the ASN for Bethel University. The question why this probe would use a DNS Resolver from this organization and in this location is probably answered by looking at the RIPE Atlas probe’s name of Bethel Seminary. This name correlation lets us assume that there is a relation between the two. As with location 2, this DNS Resolver also does not support EDNS0-Client-Subnet (ECS). But in contrary to location 2, here the geographic distance between client and DNS Resolver is a few thousand miles. In this case Amazon Route 53 is also forced to make a routing decision solely based on the DNS Resolver location in Saint Paul, Minnesota. Therefore this probe is routed across the country to the Ohio origin instead of the more efficient Oregon origin. With DNS geographic routing based CDNs - like AWS CloudFront - the end-user experience is even more degradeded. Instead of leveraging a CDN Edge location in Los Angeles - less than 150 miles away - a CDN Edge location in Minneapolis, MN - more than 1,500 miles away - is being used. This added distance will lead to higher latency and lower throughput.            Location 4: This last location - at the University of California, San Diego (UCSD) - cannot be identified by AWS Route 53 and is therefore routed incorrectly to the US-East-2 (Ohio) location instead of the closer US-West-2 (Oregon) location. Looking at the GeoIP information for the DNS Resolver’s IPv6 address we can see that Maxmind is indeed not able to locate this IP address beyond being somewhere in the US. Furthermore we can see that this DNS Resolver also does not support EDNS0-Client-Subnet (ECS). Looking at the RIPE Atlas probe’s actual IPv6 address, Maxmind is not able to locate this either. Therefore EDNS0-Client-Subnet (ECS) would not have provided any benefits here. Only the probe’s IPv4 address can be located correctly by Maxmind to be in San Diego, CA.This probe is an excellent example for Route 53 placing US-based clients or DNS Resolvers at the above mentioned Cheney reservoir location. And due to Cheney reservoir falling within the area that is closer to the US-East-2 (Ohio) origin, traffic is routed there.       Summary   This article provided a closer look at how DNS geographic routing - such as provided by AWS Route 53, but also leveraged by numerous CDNs - works. Using AWS Route 53 and RIPE Atlas we were able to visualize the real life results of a geographic routing DNS setup and dive deeper into some of the wanted and unwanted effects. A question that will need to remain to be explored in an upcoming post, is how these results compare to an Anycast based routing approach for the same set of Origins, leveraging e.g. the AWS Global Accelerator.  ","categories": ["EdgeCloud"],
        "tags": ["DNS","AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/03/01/limitations-of-geo-dns/",
        "teaser":null},{
        "title": "Limitations of Anycast for distributed content delivery",
        "excerpt":"In the previous article on Limitations of DNS-based geographic routing we have explored how DNS-based geographic routing works and especially what limitations you have to consider and accept, while using such a solution. In this article we will have a look at using Anycast instead. We will use the same set of origin locations within AWS and RIPE Atlas test probes as with the above referenced DNS-based geographic routing setup, to discover limitations of Anycast.   Anycast   The network and routing methodology of Anycast provides multiple routing to multiple endpoints for a single destination. Routers will select the desired path on the basis of number of hops, distance, lowest cost, latency measurements or based on the least congested route. This is in contrary to the typical Unicast network and routing methodology, where each destination address uniquely identifies a single receiver endpoint (See Figure 1).                              Figure 1: Unicast vs. Anycast     Anycast networks are widely used for content delivery network (CDN) products to bring their content closer to the end user, but also large public DNS resolvers make use of this concept. In the case of e.g. the Google Public DNS offering, the well-known IPv4 address of 8.8.8.8 is provided via Anycast from hundreds of Google locations, thus providing low latency DNS lookups from around the world.   AWS Global Accelerator   AWS Global Accelerator is a networking service offering from Amazon Web Services that aims at improving the availability and performance of the applications that you offer to your global users. It does so by using Anycast with a set of static IPv4 addresses that act as a fixed entry point into the AWS global network (See Figure 2).                              Figure 2: Concept of AWS Global Accelerator     When you set up AWS Global Accelerator, you associate anycasted static IP addresses to regional endpoints - such as Elastic IPv4 addresses, Network Load Balancers, and Application Load Balancers - in one or more AWS Regions. The static anycasted IPv4 addresses accept incoming traffic onto the AWS global network from the edge location that is closest to your users. From there, traffic for your application is routed to the desired AWS endpoint based on several factors, including the user’s location, the health of the endpoint, and the endpoint weights that you configure.   Note: AWS Global Accelerator unfortunately does not support IPv6 as of today.   RIPE Atlas   In this post we will use RIPE Atlas to check the performance of a geographic routing enabled DNS entry.   RIPE Atlas is a global network of hardware devices, called probes and anchors, that actively measure Internet connectivity. Anyone can access this data via Internet traffic maps, streaming data visualisations, and an API. RIPE Atlas users can also perform customised measurements to gain valuable data about their own networks.   I highly recommend you to consider hosting a RIPE Atlas probe yourself. Not only will you benefit from the data that it collects on your Internet connection, but it will also allow you to run customized measurements against various Internet targets. And in the end every additional RIPE Atlas probe will benefit the overall Internet community.   Here we will be using RIPE Atlas customized measurements to investigate the performance of DNS-based geographic routing.   Test Setup   For this article the test setup will consist of fictional origins where we want to steer traffic to via geographic routing. The test will focus on the US with one origin in the US East coast and one origin in the US West coast.   While investigating DNS-based geographic routing, we were able to leverage the RIPE Atlas DNS test. Unfortunately here this test won’t provide us the information that we are looking for as Anycast doesn’t depend on DNS. Instead we have to leverage one of the other RIPE Atlas test mechanism. The SSL test will provide us a mapping of RIPE Atlas probe location to origin endpoint if we assign a different TLS/SSL certificate to each of the two origin locations.   AWS Global Accelerator Setup   For this test we will create a single accelerator within AWS Global Accelerator along with a single Listener. This listener is configured on Port 443 for the protocol TCP, as we want to make use of HTTPS for this test run (See Figure 3).                              Figure 3: Test Setup with AWS Global Accelerator and two origins     Before we can add Endpoint Groups and Endpoints, we have to actually create these endpoints. And as mentioned before we want to create these endpoints with a separate TLS/SSL certificate, so that we can differentiate to which origin a given RIPE Atlas probe was steered towards.   The easiest approach is to create a Network Load Balancer (NLB) with TLS termination and a certificate issued by AWS Certificate Manager. Again, keep in mind that the NLB in US-East-2 and the NLB in US-West-2 need to be issued individual certificates for our test.   In real life you would issue either the same or different certificates to the origin endpoint, but leverage the same common name, based on the DNS hostname that you configured for your Anycast addresses. As this is not a production setup, but a test, the actual certificate common name doesn’t matter here.   RIPE Atlas setup   The RIPE Atlas test is configured as a SSL measurement against one of the anycasted IPv4 addresses of your AWS Global Accelerator setup. As each of the anycasted IPv4 addresses is announced via BGP from the same set of AWS Edge nodes, it is irrelevant which one you pick.   Test Results   Overview   With some Python code the gathered results can easily be turned into a GeoJSON file (See Figure 4).                              Figure 4: Mapping of RIPE Atlas probes to Origins     The location of each RIPE Atlas probe as reported by the probe’s host is leveraged in this visualization. The pin representing the location is colored in depending on the result for the Route 53 geographic routing test:      Red pin: Represents a response with the SSL certificate that is assigned to the “US-West-2” origin. Traffic from this probe is therefore served by the AWS US-West-2 (Oregon) region. In this setup this represents 31% of the probes, which is 2 percentage points higher than with DNS-based geographic routing for the same set of probes.   Blue pin: Represents a response with the SSL certificate that is assigned to the “US-East-2” origin. Traffic from this probe is therefore served by the AWS US-East-2 (Ohio) region. Here this represents 61% of the probes, which is 2 percentage points lower than with DNS-based geographic routing for the same set of probes.   We can see that most of the US-East coast based probes are correctly routed to the US-East coast origin (depicted in blue) and most of the US-West coast probes are also correctly routed to the US-West coast origin (depicted in red). But we do see a few blue pins on the US-West coast, meaning that the corresponding probe is routed to the US-East coast instead of the closer US-West coast. Similarly we also see a few red pins on the US-East coast. This clearly shows some non-optimal routing behavior. Later on we will look in more detail into the reasons for this behavior.   Probe Details   The way that I’ve setup the GeoJSON is to also display relevant information for each of the RIPE Atlas Probes (Figure 5).                              Figure 5: Detail view of RIPE Atlas probe results     After clicking on one of the pins you’re able to see:      Atlas-ID: The RIPE Atlas ID of the probe linked to RIPE ATLAS API’s “Probe Detail” view.   Traceroute: Traceroute results from the RIPE Atlas probe to the AWS Edge location serving the anycasted address.   ASN: The Autonomous System Number (ASN), allowing you to identify the Internet Service Provider of the probes host.   Origin: Result of the certificate lookup against the entry. This will either be “US-West-2” or “US-East-2” and corresponds to the color of the pin.   This setup will later help us drill down deeper into some of the RIPE Atlas locations that showed sub-optmal routing behavior.   Explore the GeoJSON   You can explore this GeoJSON yourself below, as it is published as a Gist to Github.     Give it a try and see what you’ll discover!   The good, bad and ugly   Let’s drill down on two RIPE Atlas probes to explore the limitations of Anycast routing.   The first case is for a probe at the PennState university in University Park, PA. While this location is only around 500 km away from the AWS US-East-2 (Ohio) region and around 4,500 km from the AWS US-West-2 (Oregon) region, the probe is steered to the US-West-2 (Oregon) region by Anycast. Looking at the Traceroute results for this probe, we can see that it uses the Internet2 Network, an IP network that delivers network services for research and education. Further we can see that the Internet2 Network peers with AWS at Seattle Internet Exchange (SIX) in Seattle, WA (See Figure 6).                              Figure 6: Example of RIPE Atlas probe on US East Coast preferring the origin in US-West-2     Looking at ASN 3999, the ASN for the Pennsylvania State University, we can see IPv4 peering with two different Internet2 ASNs. There is ASN 11537, which participates in a single public Internet Exchange. Also there is ASN 11164, which participates in 10 public Internet Exchanges.   It appears that the Pennsylvania State University - potentially due to financial reasons - made the decision to route all IPv4 traffic to AWS via ASN 11537 and therefore the Seattle Internet Exchange, leading to increased latency for accessing AWS’ US East coast regions as well as CDN PoPs.   Next we have a RIPE Atlas probe whose host is a customer of Charter Communications. Here we can see that Charter routes the traffic from this Gillroy, CA based customer via Denver, CA and St. Louis, MO to the AWS Edge at an unknown location (See Figure 7).                              Figure 7: Example of RIPE Atlas probe on US West Coast preferring the origin in US-East-2     Once again this routing decision could be based on financial reasons. Nevertheless Charter’s customer in this case will experience higher latency and therefore a reduced experience as a result of this decision.   Summary   This article provided a closer look at the limitations of Anycast routing. Using AWS Global Accelerator and RIPE Atlas we were able to visualize the real life results of Anycast-based routing and dive deeper into some of the unwanted effects. Also we were able to contrast these results with a similar DNS-based geographic routing setup. Similar to the test setup of DNS-based geographic routing, even with an Anycast base approach we see inefficient routings, where customers are directed to leverage an origin across the continental US even though another origin is closer by. But what performs better, DNS-based geographic routing or traffic steering via Anycast? Overall it appears that both perform in a similar way with each having some corner cases that provide a reduced end-user experience.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/04/08/limitations-of-anycast/",
        "teaser":null},{
        "title": "AWS Site-to-Site VPN with IPSec VPN (Strongwan) and BGP (FRRouting)",
        "excerpt":"This blog post walks through the setup of an EC2-based VPN endpoint - using Ubuntu Linux 18.04 with Strongswan and FRRouting - for a Site-to-Site VPN connection to AWS with BGP routing. It will allow you to experiment with BGP in your AWS account, test out new AWS features such as AWS Transit Gateway or use it for many other things. Especially if you are interested in learning more about the interaction of BGP over a Site-to-Site VPN with AWS Transit Gateway, this is an easy way to do so.   The desired final setup will look like depicted in Figure 1. The AWS Transit Gateway connects on one side to a VPC with the CIDR 172.31.0.0/16 and on the other side to an AWS Site-to-Site VPN. This AWS Site-to-Site VPN connects to an EC2-based router, which uses Strongswan for IPSec and FRRouting for BGP. To make things interesting the EC2-based router has a second network interface on a private subnet of 10.16.16.0/24, which can be announced via BGP.                              Figure 1: Setup Overview of EC2-based VPN endpoint for Site-to-Site VPN with AWS     While Transit Gateway and EC2 instance can reside in the same AWS account and even AWS region, the EC2 instance should reside in a different VPC than connected to the Transit Gateway.   AWS Transit Gateway   AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. For on-premises connectivity the AWS Transit Gateway allows you to leverage AWS Site-to-Site VPNs (IPSec) or AWS Direct Connect via AWS Direct Connect Gateways (See Figure 2).                              Figure 2: AWS Transit Gateway provides dynamic routing between VPCs, Site-to-Site VPNs, and AWS Direct Connect Gateways     A transit gateway acts as a regional virtual router for traffic flowing between your virtual private clouds (VPC) and VPN or DX connections. A transit gateway scales elastically based on the volume of network traffic. Routing through a transit gateway operates at layer 3, where the packets are sent to a specific next-hop attachment, based on their destination IP addresses.   The AWS Transit Gateway’s hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow.   Transit Gateway and Site-to-Site VPN setup   Follow the AWS documentation for setting up the AWS Transit Gateway and attaching it to an AWS Site-to-Site VPN.   It is recommended to configure “VPN ECMP support” with “enable” to enable Equal Cost Multipath (ECMP) routing support between VPN connections. This will allow you to use both tunnels of the AWS Sit-to-Site VPN connection at the same time.   Linux-based Router   This setup uses Ubuntu 16.04-LTS, Xenial Xerus as the Linux distribution for the EC2-based VPN gateway and router. Strongswan provides the IPSec termination for the AWS Site-to-Site VPN connection. And FRRouting provides the dynamic routing capabilities for BGP.   Interface Setup   As mentioned earlier the Ubuntu Linux EC2 instance uses a secondary network interface on a private subnet. This subnet is announced via BGP towards the AWS Transit Gateway. AWS Premium Support provides a detailed instruction on how to make a secondary network interface work in Ubuntu EC2 instances. The presented concepts are used here.   First create a configuration file for the secondary interface at /etc/network/interfaces.d/99-eth1.cfg. his example uses a secondary interface of ‘eth1’. Be sure to change ‘eth1’ to match your secondary interface name.  # # /etc/network/interfaces.d/99-eth1.cfg #  auto eth1 iface eth1 inet dhcp  # control-alias eth0 iface eth1 inet6 dhcp    Next create the restrict-default-gw file to prevent the default gateway from being overwritten on the main table via the /etc/dhcp/dhclient-enter-hooks.d/restrict-default-gw file.  # # /etc/dhcp/dhclient-enter-hooks.d/restrict-default-gw #  case ${interface} in   eth0)     ;;   *)     unset new_routers     ;; esac    After restarting your network the secondary interface should acquire an IPv4 and IPv6 address.   root@host:~# systemctl restart networking root@host:~# ifconfig eth1 eth1      Link encap:Ethernet  HWaddr 02:7b:35:90:92:ae           inet addr:10.16.16.254  Bcast:10.16.16.255  Mask:255.255.255.0           inet6 addr: fe80::7b:35ff:fe90:92ae/64 Scope:Link           inet6 addr: 2600:1f16:f2d:1211:2c35:28f7:7d5d:f776/128 Scope:Global           UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1           RX packets:9081 errors:0 dropped:0 overruns:0 frame:0           TX packets:2143 errors:0 dropped:0 overruns:0 carrier:0           collisions:0 txqueuelen:1000           RX bytes:854331 (854.3 KB)  TX bytes:213798 (213.7 KB)   Source or Destination checking   In AWS the Source or Destination checking attribute on an interface determines whether an instance can handle network traffic that isn’t specifically destined for the instance.   Before installing Strongswan on your EC2 instance disable Source/Destination Checks for this instance. Keep in mind that this needs to be done for both interfaces (ENIs) separately.   Strongswan setup   Next use apt-get update &amp;&amp; apt-get install -y strongswan to install Strongswan on the Ubuntu Linux 16.04 instance.   Update the configuration file /etc/ipsec.conf with generic settings for an AWS Site-to-Site VPN, as well as the specific settings for the two tunnels that each AWS Site-to-Site VPN provides.  Make sure to replace the relevant IPv4 addresses from this example with your IPv4 addresses.    #  # /etc/ipsec.conf  #  conn %default          # Authentication Method : Pre-Shared Key          leftauth=psk          rightauth=psk          # Encryption Algorithm : aes-128-cbc          # Authentication Algorithm : sha1          # Perfect Forward Secrecy : Diffie-Hellman Group 2          ike=aes128-sha1-modp1024!          # Lifetime : 28800 seconds          ikelifetime=28800s          # Phase 1 Negotiation Mode : main          aggressive=no          # Protocol : esp          # Encryption Algorithm : aes-128-cbc          # Authentication Algorithm : hmac-sha1-96          # Perfect Forward Secrecy : Diffie-Hellman Group 2          esp=aes128-sha1-modp1024!          # Lifetime : 3600 seconds          lifetime=3600s          # Mode : tunnel          type=tunnel          # DPD Interval : 10          dpddelay=10s          # DPD Retries : 3          dpdtimeout=30s          # Tuning Parameters for AWS Virtual Private Gateway:          keyexchange=ikev1          rekey=yes          reauth=no          dpdaction=restart          closeaction=restart          leftsubnet=0.0.0.0/0,::/0          rightsubnet=0.0.0.0/0,::/0          leftupdown=/etc/ipsec-vti.sh          installpolicy=yes          compress=no          mobike=no conn AWS-VPC-GW1          # Customer Gateway: :          left=10.16.1.254          leftid=18.123.45.67          # Virtual Private Gateway :          right=35.98.76.54          rightid=35.98.76.54          auto=start          mark=100          #reqid=1 conn AWS-VPC-GW2          # Customer Gateway: :          left=10.16.1.254          leftid=18.123.45.67          # Virtual Private Gateway :          right=54.98.76.54          rightid=54.98.76.54          auto=start          mark=200   Next update the configuration file /etc/ipsec.secrets with the Pre-Shared Keys of your AWS Site-to-Site VPN. Here also ensure that you update the IPv4 addresses from this example with the IPv4 addresses of your setup.   # # /etc/ipsec.secrets #  # This file holds shared secrets or RSA private keys for authentication.  # RSA private key for this host, authenticating it to any other host # which knows the public part. 18.123.45.67 35.98.76.54 : PSK \"7?-Mun7^XC3JUswf$f$mm=8s@v2U=-aG\" 18.123.45.67 54.98.76.54 : PSK \"!yTF%t$nQTwp++6rB9#qft?Vfa%KAhnU\"   As we want to run a dynamic routing protocol with BGP over this AWS Site-to-Site VPN, we need to use a route-based VPN setup instead of a policy-based one.   The bash script in file /etc/ipsec-vti.sh sets up the virtual tunnel interfaces for a route-based IPSec VPN. Also here make sure to replace the IPv4 addresses of this example with the IPv4 addresses of your setup.   #!/bin/bash  # # /etc/ipsec-vti.sh #  IP=$(which ip) IPTABLES=$(which iptables)  PLUTO_MARK_OUT_ARR=(${PLUTO_MARK_OUT//// }) PLUTO_MARK_IN_ARR=(${PLUTO_MARK_IN//// }) case \"$PLUTO_CONNECTION\" in AWS-VPC-GW1) VTI_INTERFACE=vti1 VTI_LOCALADDR=169.254.12.38/30 VTI_REMOTEADDR=169.254.12.37/30 ;; AWS-VPC-GW2) VTI_INTERFACE=vti2 VTI_LOCALADDR=169.254.14.230/30 VTI_REMOTEADDR=169.254.14.229/30 ;; esac  case \"${PLUTO_VERB}\" in up-client) #$IP tunnel add ${VTI_INTERFACE} mode vti local ${PLUTO_ME} remote ${PLUTO_PEER} okey ${PLUTO_MARK_OUT_ARR[0]} ikey ${PLUTO_MARK_IN_ARR[0]} $IP link add ${VTI_INTERFACE} type vti local ${PLUTO_ME} remote ${PLUTO_PEER} okey ${PLUTO_MARK_OUT_ARR[0]} ikey ${PLUTO_MARK_IN_ARR[0]} sysctl -w net.ipv4.conf.${VTI_INTERFACE}.disable_policy=1 sysctl -w net.ipv4.conf.${VTI_INTERFACE}.rp_filter=2 || sysctl -w net.ipv4.conf.${VTI_INTERFACE}.rp_filter=0 $IP addr add ${VTI_LOCALADDR} remote ${VTI_REMOTEADDR} dev ${VTI_INTERFACE} $IP link set ${VTI_INTERFACE} up mtu 1436 $IPTABLES -t mangle -I FORWARD -o ${VTI_INTERFACE} -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu $IPTABLES -t mangle -I INPUT -p esp -s ${PLUTO_PEER} -d ${PLUTO_ME} -j MARK --set-xmark ${PLUTO_MARK_IN} $IP route flush table 220 #/etc/init.d/bgpd reload || /etc/init.d/quagga force-reload bgpd ;; down-client) #$IP tunnel del ${VTI_INTERFACE} $IP link del ${VTI_INTERFACE} $IPTABLES -t mangle -D FORWARD -o ${VTI_INTERFACE} -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu $IPTABLES -t mangle -D INPUT -p esp -s ${PLUTO_PEER} -d ${PLUTO_ME} -j MARK --set-xmark ${PLUTO_MARK_IN} ;; esac  # Enable IPv4 forwarding sysctl -w net.ipv4.ip_forward=1 sysctl -w net.ipv4.conf.eth0.disable_xfrm=1 sysctl -w net.ipv4.conf.eth0.disable_policy=1    As this is a bash script, don’t forget to make the file executable:  chmod +x /etc/ipsec-vti.sh   Afterwards restart the Strongwan daemon to load the configuration changes and establish the tunnels:  systemctl restart strongswan   You can validate that the two tunnel interfaces vti1 and vti2 are up and running with the commands ip -s tunnel show or ifconfig vti1. You should see the IP address of the tunnels displayed within the 169.254.0.0/16 range.   FRRouting Setup   Follow the FFRouting documentation for installing FFRouting on Ubuntu 18.04 or have a look at my installation script.   When configuring the /etc/frr/daemons file, ensure to enable the bgpd daemon.   You can now connect via the vtysh Modal CLI to the router process.   ubuntu@host:~$ sudo vtysh  Hello, this is FRRouting (version 4.0+cl3u10). Copyright 1996-2005 Kunihiro Ishiguro, et al.    First we configure the BGP process with the provided AS number and the two VPN-based peers. We also chose to announce our local private subnet 10.16.16.0/24.   host# conf t host(config)# router bgp 65016 host(config-router)# neighbor 169.254.12.229 remote-as 64512 host(config-router)# neighbor 169.254.14.37 remote-as 64512 host(config-router)# address-family ipv4 unicast host(config-router-af)# network 10.16.16.0/24 host(config-router-af)# end host# wr   Next we can check if all interfaces are up and running. Besides eth0 and eth1 with IPv4 and IPv6 addresses, we should also see vti1 and vti2 for the IPSec tunnel interfacces.  host# sh int brief Interface       Status  VRF             Addresses ---------       ------  ---             --------- eth0            up      default         10.16.1.254/24                                         + 2600:1f16:e4d:1a01:fbe6:fa0b:929b:4d72/128 eth1            up      default         10.16.16.254/24                                         + 2600:1f16:e4d:1a11:2c38:23f7:7a5d:f773/128 ip_vti0         down    default lo              up      default vti1            up      default         169.254.12.38/32 vti2            up      default         169.254.14.230/32    Validation   EC2-Based Router   Now it’s time to validate that the AWS Transit Gateway is providing us with routes over BGP. Here we can see that the route for the VPC attached to the Transit Gateway is propagated via BGP to us.  host# sh ip route bgp Codes: K - kernel route, C - connected, S - static, R - RIP,        O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,        T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,        F - PBR,        &gt; - selected route, * - FIB route  B&gt;* 172.31.0.0/16 [20/100] via 169.254.14.229, vti2, 00:23:10   *                        via 169.254.12.37, vti1, 00:23:10   Notice that in case you configured the AWS Transit Gateway with Equal-cost multi-path routing (ECMP), you will see the remote VPCs CIDR of 172.31.0.0/16 announced over both AWS Site-to-Site VPN tunnels.   AWS Console   Within the AWS Console you should also be able to see the VPN connections for both tunnels in the state “UP” and should see 1 BGP route - which is the 10.16.16.0/24 network to be received over each tunnel (See Figure 3).                              Figure 3: AWS Site-to-Site VPN showing as UP with routes being received.     At this point you are all set and your VPN connection along with BGP routing is ready to be used.   Summary   This blog post showed you the setup of an EC2-based VPN endpoint while using Ubuntu Linux 16.04 with Strongswan for a Site-to-Site VPN connection and and FRRouting for BGP. This setup allows you to experiment with BGP in your AWS account. Especially if you are interested in learning more about the interaction of BGP over a Site-to-Site VPN with AWS Transit Gateway, the resulting setup is a great start to do so.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/07/18/aws-site-2-site-vpn-with-strongswan-frrouting/",
        "teaser":null},{
        "title": "BGP route summarization with AWS Transit Gateway",
        "excerpt":"This blog post will look at using BGP Route Aggregation from the AWS Transit Gateway via IPSec VPN to a customer gateway. While the example configuration and output provided for the customer gateway are using a Cisco CSR1000V with IOS-XE, you can replicate the same with e.g. Ubuntu Linux, StrongSwan and FFRouting. Have a look at the previous post “AWS Site-to-Site VPN with IPSec VPN (Strongwan) and BGP (FRRouting)”, which describes how to setup such a testbed.   BGP Route Aggregation   BGP Route Aggregation (RA) - also sometimes referred to as BGP Route Summarization - is a method to minimize the size of the routing table. RA reduces the size of the global routing table, decreases routers’ workload and saves network bandwidth. As an example: Instead of announcing the four routes of 192.168.0.0/24, 192.168.1.0/24, 192.168.2.0/24, and 192.168.3.0/24, one can announce the single route of 192.168.0.0/22 instead.   AWS Transit Gateway   AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. For on-premises connectivity the AWS Transit Gateway allows you to leverage AWS Site-to-Site VPNs (IPSec) or AWS Direct Connect via AWS Direct Connect Gateways (See Figure 2).                              Figure 1: AWS Transit Gateway provides dynamic routing between VPCs, Site-to-Site VPNs, and AWS Direct Connect Gateways     A transit gateway acts as a regional virtual router for traffic flowing between your virtual private clouds (VPC) and VPN or DX connections. A transit gateway scales elastically based on the volume of network traffic. Routing through a transit gateway operates at layer 3, where the packets are sent to a specific next-hop attachment, based on their destination IP addresses.   The AWS Transit Gateway’s hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow.   BGP Route Aggregation   Problem Description   As previously mentioned, AWS Transit Gateway allows you to connect multiple AWS VPCs to on-premises networks via an IPSec VPN or Direct Connect. In the case of an AWS IPSec VPN connection, AWS Transit Gateway will announce over BGP a separate route for each of these connected VPCs.   Figure 2 shows an example, where the four subnets 172.16.1.0/24, 172.16.2.0/24, 172.16.3.0/24, and 172.16.4.0/24 correspond to a separate VPC each.                              Figure 2: Transit Gateway Route Table with individual VPC routes.     On the customer gateway the routes for these four VPCs are then received separately as individual routes.   CSR100V-01#sh ip route bgp Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP        D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area        N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2        E1 - OSPF external type 1, E2 - OSPF external type 2        i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2        ia - IS-IS inter area, * - candidate default, U - per-user static route        o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP        a - application route        + - replicated route, % - next hop override, p - overrides from PfR  Gateway of last resort is 10.0.1.1 to network 0.0.0.0        10.0.0.0/8 is variably subnetted, 6 subnets, 2 masks B        10.1.16.0/24 [20/100] via 169.254.15.221, 1d01h                       [20/100] via 169.254.13.253, 1d01h B        10.16.16.0/24 [20/100] via 169.254.15.221, 1d01h                        [20/100] via 169.254.13.253, 1d01h       172.16.0.0/24 is subnetted, 4 subnets B        172.16.1.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.2.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.3.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.4.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B     172.31.0.0/16 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h CSR100V-01#   As in this example the four subnets for the VPCs, as well as the subnet for the default VPC of 172.31.0.0/16 are part of the aggregate route of 172.16.0.0/12 it might make more sense to announce this single aggregate over the IPSec VPN instead of the individual more specific routes (See Figure 3).                              Figure 3: Desired BGP announcements between VPCs, TGW and customer gateway over VPN.     Especially as you add further VPCs with a netblock from the 172.16.0.0/12 block, this summarization will keep the routing table on the customer gateway smaller.   Static TGW Route   A common mechanism to crate a BGP route aggregation with physical routers is to create a static discard route on the source router. This means that for an aggregate address - in our case this is 172.16.0.0/12 - a static route pointing to a null interface is created. As a result this aggregate route will be propagated via BGP. Once packets reach this router, the more specific routes will determine how to further route the packet.   While The AWS Transit Gateway supports blackhole routes, these routes are not propagated via BGP over the IPSec tunnels. Instead you have to create a regular static route and point it at a random VPC (See Figure 4).                              Figure 4: Transit Gateway Route Table with summary route.     As a result the customer gateway will now receive the aggregate route of 172.16.0.0/12 besides the 5 more specific routes for 172.16.1.0/24, 172.16.2.0/24, 172.16.3.0/24, 172.16.4.0/24, and 172.31.0.0/16.   CSR100V-01#sh ip route bgp Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP        D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area        N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2        E1 - OSPF external type 1, E2 - OSPF external type 2        i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2        ia - IS-IS inter area, * - candidate default, U - per-user static route        o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP        a - application route        + - replicated route, % - next hop override, p - overrides from PfR  Gateway of last resort is 10.0.1.1 to network 0.0.0.0        10.0.0.0/8 is variably subnetted, 6 subnets, 2 masks B        10.1.16.0/24 [20/100] via 169.254.15.221, 1d01h                       [20/100] via 169.254.13.253, 1d01h B        10.16.16.0/24 [20/100] via 169.254.15.221, 1d01h                        [20/100] via 169.254.13.253, 1d01h B     172.16.0.0/12 [20/100] via 169.254.15.221, 00:00:43                     [20/100] via 169.254.13.253, 00:00:43       172.16.0.0/24 is subnetted, 4 subnets B        172.16.1.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.2.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.3.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B        172.16.4.0 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h B     172.31.0.0/16 [20/100] via 169.254.15.221, 1d01h                     [20/100] via 169.254.13.253, 1d01h CSR100V-01#   Customer gateway route filtering   While this is already a good step in the right direction, still receiving the individual more specific routes on the customer defeats the purpose of route aggregation. While Cisco IOS e.g. has the command aggregate-address address / length summary-only for solely announcing the summary route, the AWS Transit Gateway is missing a similar capability.   Here instead we have to filter undesired prefixes as part of the BGP session in the customer gateway. This way we can accept only the aggregate prefixes, preventing and more specific route to be installed into the customer gateway route table.   To do so, we first create an ip prefix list with the desired prefixes that we want to receive. Next we create a route map matching this prefix list. Last but not least the route map is applied on incoming BGP connections for each configured BGP peer, aka neighbors.   router bgp 65000  bgp log-neighbor-changes  neighbor 169.254.13.253 remote-as 64512  neighbor 169.254.13.253 timers 10 30 30  neighbor 169.254.15.221 remote-as 64512  neighbor 169.254.15.221 timers 10 30 30  !  address-family ipv4   network 10.0.16.0 mask 255.255.255.0   neighbor 169.254.13.253 activate   neighbor 169.254.13.253 soft-reconfiguration inbound   neighbor 169.254.13.253 route-map REJECT in   neighbor 169.254.15.221 activate   neighbor 169.254.15.221 soft-reconfiguration inbound   neighbor 169.254.15.221 route-map REJECT in   maximum-paths eibgp 2  exit-address-family !  ip prefix-list incoming seq 5 permit 10.1.16.0/24 ip prefix-list incoming seq 10 permit 10.16.16.0/24 ip prefix-list incoming seq 15 permit 172.16.0.0/12 ! route-map REJECT permit 10  match ip address prefix-list incoming !   As a result our customer gateway will now only receive the aggregate prefix, but none of the more specific routes.   CSR100V-01#sh ip route bgp Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP        D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area        N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2        E1 - OSPF external type 1, E2 - OSPF external type 2        i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2        ia - IS-IS inter area, * - candidate default, U - per-user static route        o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP        a - application route        + - replicated route, % - next hop override, p - overrides from PfR  Gateway of last resort is 10.0.1.1 to network 0.0.0.0        10.0.0.0/8 is variably subnetted, 6 subnets, 2 masks B        10.1.16.0/24 [20/100] via 169.254.15.221, 1d01h                       [20/100] via 169.254.13.253, 1d01h B        10.16.16.0/24 [20/100] via 169.254.15.221, 00:00:40                        [20/100] via 169.254.13.253, 00:00:40                     B     172.16.0.0/12 [20/100] via 169.254.15.221, 00:27:23                     [20/100] via 169.254.13.253, 00:27:23 CSR100V-01#   Summary   This blog post walked you through configuring BGP route summarization with AWS Transit Gateway for attachments over IPSec VPN. Keep in mind, that with the AWS Transit Gateway and a Direct Connect Gateway attachment, this approach is not applicable as you are effectively specifying the IP prefixes to be announced over Direct Connect manually via allowed prefixes.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/08/07/bgp-route-summary-with-tgw/",
        "teaser":null},{
        "title": "AWS Transit Gateway with Direct Connect Gateway and Site-to-Site (IPSec) VPN Backup",
        "excerpt":"A very common network architecture pattern on AWS is to deploy an AWS Site-to-Site (IPSec) VPN connection as the backup for an AWS Direct Connect (DX) connection between on-premises networks and AWS VPCs. With the introduction of AWS Transit Gateway and AWS Direct Connect Gateway, this architecture pattern is no longer a trivial task. This blog post highlights the associated challenges and offers a solution using BGP route summarization and BGP prefix filtering to manage traffic over the Direct Connect and Site-to-Site (IPSec) VPN path as expected.   Desired Architecture   To highlight the challenges with this architecture pattern, we assume the AWS network architecture as outlined in Figure 1.                              Figure 1: Desired Setup with Direct Connect Gateway as the primary active path and Site-to-Site (IPSec) VPN as the backup path.     This architecture includes the following assumptions and design decisions:     VPC Prefixes: Within AWS we assume that each of the four VPCs is configured with a single /24 prefix.   DX Gateway announced prefixes: As the number of prefixes  per AWS Transit Gateway from AWS to on-premises on a transit virtual interface (via Direct Connect Connect Gateway) is limited to 20, we will announce a single /16 summary route for the VPC prefixes.   Site-to-Site (IPSec) VPN over Internet: The backup path via the Site-to-Site (IPSec) VPN tunnels will leverage the Internet and not another Direct Connect connection as transport mechanism.   Equal Cost Multi-Pathing (ECMP): A single AWS Site-to-Site (IPSec) VPN tunnel only provides a maximum bandwidth of 1.25 Gbps. For the case that the primary active DX connection has a bandwidth higher than 1 Gbps this could lead to contention during a failover scenario. Therefore we want to leverage multiple VPN tunnels with ECMP to provide sufficient throughputs over the backup path.   Direct Connect as primary active path: as Direct Connect offers a more consistent network experience than Internet-based connections, this network path is desired to be the primary active path, while the Site-to-Site (IPSec) VPN path should solely serve as the standby backup path.   BGP as failover mechanism: We want to leverage BGP as the failover mechanism and not implement any manual out-of-band monitoring and failover mechanism. As such we will be using a BGP based Site-to-Site (IPSec) VPN.   No IPv6 failover support: Unfortunately the AWS Site-to-Site (IPSec) VPN does not support carrying IPv6 traffic. Therefore this failover design is not suited for use cases, where you want to carry private IPv6 traffic between on-premises and AWS.   Note: The design in Figure 1 depicts solely a single DX connection as well as a single Site-to-Site (IPSec) VPN tunnel. A more realistic setup would include two DX connections, as well as a pair of VPN tunnels. As the presented challenges and proposed solution are the same for both designs, the simplified version is used here to better explain the concepts.   Actual asymmetric routing   Unfortunately when implementing the above design, you’ll quickly notice that the result is not what you were expecting. Instead you will observe asymmetric routing (See Figure 2) with traffic from AWS to on-premises traversing - as desired - the Direct Connect link, while traffic from on-premises to AWS traversing the Site-to-Site (IPSec) VPN tunnel instead.                              Figure 2: Actual asymmetric routing due to more specific prefixes being propagated over the Site-to-Site (IPSec) VPN.     For traffic from on-premises to AWS, a few challenges cause this traffic to traverse the Site-to-Site (VPN) VPN connection instead of the AWS Direct Connect link. The following sections will dive into these challenges in more detail:   AS path length   When looking from the customer router at the CIDR(s) announced via BGP from the AWS Direct Connect Gateway, we would expect to see them with an ASN path of 65001 and 64512, therefore with a path length of two. Instead we will only see the ASN 65001 of the AWS Direct Connect Gateway in the path. This is the result of users manually setting the CIDRs to be announced by the AWS Direct Connect Gateway towards on-premises. Imagine the AWS Direct Connect Gateway effectively filtering out any CIDRs learned from the AWS Transit Gateway and instead originates a route with the manually configured CIDR. The result is a reduced path length to one over Direct Connect, which is the same AS path length as over the Site-to-Site (IPSec) VPN link.   Multi Exit Discriminator (MED)   We might end up in a situation where the same prefix is being announced over the Direct Connect link as well as the VPN link with the same AS path length. In case Equal Cost Multipathing (ECMP) is configured on the customer router, we would expect both of these path to be considered at the same time. To combat this behavior AWS sets a Multi Exit Discriminator (MED) value of 100 on BGP sessions over VPN links. As this value is higher than the default value of 0 - which the DX path uses - the DX path should be preferred. This works well in the case DX and VPN are used with a Virtual Private Gateway (VGW) as the same AS is announced over both connections. But with the TGW the Direct Connect path uses a different ASN compared to the VPN path. Therefore the MED value is only taken into consideration if the setting “bgp always-compare-med” is used within the customer’s router. With this setting MED is always compared if multiple routes to a destination have the same local preference and AS path length.   TGW preference of DX over VPN   The AS path received by the AWS Transit Gateway is not reduced to the path length of one by the BGP configuration. This results in a longer path length over the Direct Connect Gateway link as over the Site-to-Site (IPSec) VPN link. Nevertheless, traffic will solely traverses over the Direct Connect link to on-premises. The AWS Transit Gateway in this case prefers the AWS Direct Connect gateway over the VPN connection, as outlined in the AWS Transit Gateway documentation. You can imagine the AWS Transit Gateway setting a higher “local preference” (LOCAL_PREF) on the AWS Direct Connect gateway BGP sessions.   More Specific Routes   The AWS Transit Gateway will announce all active static routes and propagated routes over the Site-to-Site (IPSec) VPN link, which in this example results in four /24 prefixes being announced towards the customer routers. As mentioned before, due to the limitation on the number of prefixes per AWS Transit Gateway from AWS to on-premise on a transit virtual interface (via Direct Connect Connect Gateway), we are announcing a /16 summary route instead of the four /24 routes over the AWS Direct Connect Gateway. As a result the customer router will see more specific routes over the Site-to-Site (IPSec) VPN link and therefore prefer this link for traffic towards the VPCs. Obviously this is not what we have desired.   Corrected traffic flow   To correct the asymmetric routing situation we need to implement a few changes as highlighted in Figure 3.                              Figure 3: Corrected traffic flow after using route summarization and prefix filtering.     The following sections look at these required changes in detail:   Route summarization and filtering   I already covered how to implement BGP route summarization with the AWS Transit Gateway in a previous post. In this example we assume that VPCs 1 - 4 are using the CIDRs 10.1.1.0/24, 10.1.2.0/24, 10.1.3.0/24, and 10.1.4.0/24 respectively. Because we are propagating the summary prefix of 10.1.0.0/16 over the Direct Connect link, we want to send the same summary prefix over the Site-to-Site (IPSec) VPN.   As outlined in the previous post we can achieve this by creating a static route for the prefix 10.1.0.0/16 and point it to e.g. VPC 1.   Now the customer gateway will receive the summary route of 10.1.0.0/16 along with the more specific routes of 10.1.1.0/24, 10.1.2.0/24, 10.1.3.0/24, and 10.1.4.0/24. Using prefix filtering on the customer gateway, we can filter out the more specific routes and solely install the summary route into the BGP route table.   The below example shows how to implement this under Cisco IOS with route-maps.   router bgp 65100  bgp log-neighbor-changes  neighbor 169.254.254.1 remote-as 64512  neighbor 169.254.254.1 description Direct Connect  neighbor 169.254.254.1 timers 10 30 30  neighbor 169.254.254.1 password 0 mypassword  neighbor 169.254.15.221 remote-as 65001  neighbor 169.254.15.221 description VPN  neighbor 169.254.15.221 timers 10 30 30  !  address-family ipv4   network 10.2.0.0 mask 255.255.0.0   neighbor 169.254.254.1 activate   neighbor 169.254.254.1 soft-reconfiguration inbound   neighbor 169.254.15.221 activate   neighbor 169.254.15.221 soft-reconfiguration inbound   neighbor 169.254.15.221 route-map SUM_FILTER in   maximum-paths 4  exit-address-family !  ip prefix-list incoming seq 5 permit 10.1.0.0/16 ! route-map SUM_FILTER permit 10  match ip address prefix-list incoming !   In this case we only allow the summary prefix of 10.1.0.0/16 over the VPN link.   Results   At this point we are done and have succeeded: The customer gateway will see the same summary route announced with the same AS path length over Direct Connect and VPN link, except that the route over VPN will have an MED of 100. Therefore the DX route - having an MED of 0 - will be chosen instead.   With this we can look at the routes that are received from the BGP peer over DX (here: 169.254.254.1) and the BGP peer over VPN (here: 169.254.15.221)  #sh ip bgp neighbors 169.254.254.1 received-routes | beg Network       Network          Next Hop            Metric LocPrf Weight Path   *    10.1.0.0/16     169.254.254.1                          0 65001 i  #sh ip bgp neighbors 169.254.15.221 received-routes | beg Network       Network          Next Hop            Metric LocPrf Weight Path   *    10.1.0.0/16     169.254.15.221         100             0 64512 i   *    10.1.1.0/24     169.254.15.221         100             0 64512 i   *    10.1.2.0/24     169.254.15.221         100             0 64512 i   *    10.1.3.0/24     169.254.15.221         100             0 64512 i   *    10.1.4.0/24     169.254.15.221         100             0 64512 i  While we see the four more specific /24 routes over VPN - which will be filtered out - we also see the summary route matching the route over DX. While both routes have the same AS path length, we can see the difference in Metric (showing MED).   With this the route installed into the RIB by BGP will solely be the one traversing DX.   #sh ip bgp | beg Network       Network          Next Hop            Metric LocPrf Weight Path   *    10.1.0.0/16     169.254.15.221         100             0 64512 i   *&gt;                   169.254.254.1            0             0 65001 i   Word of Warning   The fundamental underlying principle of this approach is to have the same IP CIDRs with the same AS path length announced over both Direct Connect and VPN. Having a more specific CIDR announced over one of the two paths, would shift traffic towards this path. With that you might be tempted to announce more specific routes from the Transit Gateway over the Direct Connect Gateway into on-premises, than what is sent over VPN. While this approach is technically possible, it will very quickly bring you within the service limit of 20 prefixes that can be announced from a Transit Gateway to a Direct Connect Gateway.   Summary   This article walked you through the challenges associated with configuring a Site-to-Site (IPSec) VPN tunnel as a backup path with a primary active traffic between an AWS Transit Gateway and on-premises networks via a Direct Connect Gateway. To overcome these challenges a solution is proposed that leverages BGP route summarization and BGP prefix filtering.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/08/16/aws-dxgw-with-ipsec-vpn-backup/",
        "teaser":null},{
        "title": "AWS Direct Connect Gateway Deep Dive",
        "excerpt":"In November 2017 AWS released AWS Direct Connect Gateway, which is probably one of the biggest innovations within the AWS Direct Connect product in recent years. At the same time Direct Connect Gateway is often overlooked or not well understood. Therefore this blog post will dive deeper at the ins and outs of this extremely helpful AWS network capability.   Use Cases   AWS Direct Connect Gateway allows you establish connectivity that spans Virtual Private Clouds (VPCs) spread across multiple AWS Regions. Instead of establishing multiple BGP sessions for each VPC, you only need to establish a single BGP session with the Direct Connect Gateway per DX location. As the AWS Direct Connect Gateway is a global object, VPCs and DX locations in any location (except China) can be bridged.   The rules of IP routing still apply and you will only be able to reach VPCs from on-premises locations if the IP CIDRs don’t overlap.   This functionality of AWS Direct Connect Gateway allows you to accomplish the following very common use cases:   Direct Connect to remote regions   Before Direct Connect Gateway you were only able to reach the AWS Region associated with a DX location via a Private Virtual Interface (VIF). With this the AWS DX location at e.g. CoreSite NY1, New York, NY could only establish a Private VIF for usage in the US-East (N. Virginia) region, but not for the US-West (Oregon) region (See Figure 1).                              Figure 1: Direct Connect with associated AWS region.     If you required to access multiple AWS regions via Direct Connect over Private VIFs you had to work with AWS Direct Connect Partners to establish a presence in a DX location associated with the AWS Region of interest. Depending on your location and the location of the DX location this could become very expensive.   With AWS Direct Connect Gateway you can now access Virtual Private Clouds (VPC) via Virtual Private Gateways (VGW) in any AWS region (except China) from any Direct Connect location (except China), simplifying this use case dramatically (See Figure 2).                              Figure 2: Direct Connect with Direct Connect Gateway.     This dramatically improves flexibility and reduces cost, when connecting from on-premises to AWS regions. Looking at the AWS Direct Connect data transfer cost for AWS Regions in North America, you can see that the data transfer OUT price to any of the Direct Connect locations in North America is the same at currently $0.02. Therefore in this case picking a Direct Connect location within North America that is the closest to your on-premises location does not increase the AWS cost.   Connectivity to multiple Virtual Private Clouds (VPC)   If you want to connect from on-premises to multiple Virtual Private Clouds (VPCs) within AWS, without Direct Connect Gateway you had to create a separate Private Virtual Interface on your DX connection for each of these VPCs. In addition you had to create a separate BGP peering over the different VLANs for each of these Private Virtual Interfaces between your physical router and the Virtual Private Gateway (VGW) in AWS.   As depicted in Figure 2, with Direct Connect Gateway there is only one Private Virtual Interface necessary to connect to multiple VPCs. As a result you now only need to establish a single BGP session between your on-premises router and the Direct Connect gateway. When attaching additional VPCs their CIDRs are propagated towards on-premises - with the Direct Connect Gateway’s ASN as the origin - towards on-premises.   This also simplifies the physical setup of your Direct Connect dramatically.   AWS Transit Gateway with Direct Connect   If you want to connect an AWS Transit Gateway to on-premises via AWS Direct Connect, you have to leverage AWS Direct Connect Gateway (See Figure 3). It is not possible to connect directly to a Direct Connect connection from a Transit Gateway.                              Figure 3: Direct Connect Gateway with Transit Gateway.     As reference, AWS Transit Gateway only supports the following attachment types:     Virtual Private Cloud (VPC)   Direct Connect Gateway   Site-to-Site (IPSec) VPN   Attachments and associations   AWS Direct Connect Gateway is a global AWS object which supports “Virtual Interface Attachments” on the on-premises facing side and “Gateway Associations” on the AWS facing side. These attachments and associations cannot be freely mixed and matched, but instead only allow two fixed combinations. Below is an overview of these two combinations and resulting attachments and associations.           Virtual Private Gateway with Private Virtual Interface: In this case the AWS-facing termination point, or gateway association, is a Virtual Private Gateway (VGW). A VGW can connect to exactly one VPC. The on-premises facing corresponding attachment for this scenario is a private virtual interface. An AWS Direct Connect Dedicated Connection or Hosted Connection can support up to 50 such private virtual interfaces. A Hosted Virtual Interface on the other hand support only supports a single virtual Interface, which can be Private Virtual Interface.            Transit Gateway with Transit Virtual interface: A Transit Gateway is an AWS networking component, which allows you to connect multiple VPCs, Direct Connect Gateways, and Site-to-Site (IPSec) VPNs together via attachments. Currently these attachments have to reside in the same AWS region, although cross-region support has been announced. In this case the on-premises facing attachment of the Direct Connect Gateway is a Transit Virtual Interface (VIF). Only AWS Direct Connect Dedicated Connections or Hosted Connections with a capacity of greater than or equal to 1G support Transit VIFs. In the case of a Dedicated Connection you can use one Transit VIF in addition to 50 private or public VIFs. In the case of a Hosted Connection - which only provides a single Virtual Interface - that VIF can either be a private, public or transit Virtual Interface. AWS Direct Connect Hosted Virtual Interfaces do not support Transit VIFs at all.       Automation   AWS Direct Connect gateway supports various forms of automation via it’s API.      API and SDKs: The AWS Direct Connect gateway’s API is part of the AWS Direct Connect API. Therefore you can manage AWS Direct Connect gateway via AWS SDKs and command line tools.   CloudFormation: As of today, CloudFormation does not support AWS Direct Connect and therefore also does not support AWS Direct Connect gateway.   TerraForm: HashiCorp’s TerraForm does support creation and management of AWS Direct Connect gateways.   Multi-Account support   Similar to AWS Direct Connect itself, DX Gateway also support multi-account setups. This is especially important for larger customer that want to split ownership of the components across teams or units. Also customers of AWS GovCloud (US) benefit from this capability, as various components can be managed from a standard commercial account instead of the GovCloud account.   Lookin at a standard deployment as depicted in Figure 4, it is possible - although not necessary - to split ownership of the three component types across different accounts.                              Figure 4: Possible account ownership in multi-account setup.     To share ownership between DX Gateway and the DX Connection, the owner of the DX connection creates a new Virtual Interface and assigns it to the account that owns the DX Gateway. The account owning the DX Gateway can then accept this Virtual Interface and attach it to a DX Gateway.   Similarly if the account owning the AWS Virtual Private Gateway (VGW) or AWS Transit Gateway (TGW), as well as the account owning the Direct Connect gateway belong to the same AWS payer account ID, sharing is here possible as well. In that case the account owner of the AWS Virtual Private Gateway (VGW) or AWS Transit Gateway (TGW) initiates the association proposal, which has to be approved by the account owning the Direct Connect Gateway.   Restrictions and Limits   The following limits and restrictions apply to Direct Connect Gateway.   Object Limits   The number of AWS Direct Connect gateways and associated objects is limited within a single AWS account to the following values:     AWS Direct Connect gateways per account: 200   Virtual Interfaces: 30 (Either Private Virtual Interfaces or Transit Virtual Interfaces)   Transit Gateways: 3 (Cannot be combined with Virtual Private Gateways)   Virtual Private Gateways: 30 (Cannot be combined with Transit Gateways)   Data flow   It is important to point out that only data flow between AWS-facing Gateway associations and on-premises facing VIF attachments is possible. This is depicted in Figure 5 as green pathes. Data flow between associated Gateways or data flow between multiple VIFs, connected to the same Direct Connect Gateway, is not possible. This is depicted in Figure 5 as red pathes.                              Figure 5: Permitted data flow (green) and not permitted data flow (red) with Direct Connect Gateway.     It is important to point out that the red depicted data flow not only includes BGP routing traffic, but also routed traffic. Looking at the VIFs facing on-premises, you will not receive BGP route announcements from the Direct Connect Gateway that were originated by one of the other VIFs. But even attempting to place a static route towards the Direct Connect for traffic from one VIF to another will fail.   BGP prefixes   You are also limited by the number of BGP prefixes that you can announce from on-premises networks towards AWS, as well as from AWS towards on-premises (Figure 6).                              Figure 6: BGP prefix limits with Direct Connect Gateway.     Looking at the example in Figure, you also want to keep aggregates in mind. While you can e.g. announce 100 prefixes from each of the two corporate data center depicted, in the case of these prefixes being non-overlapping, not all 200 prefixes will make it into an associated VPC route table. That’s due to the VPC route table only being able to hold 100 BGP advertised (propagated) routes.   If you advertise more than 100 routes over the BGP session, the BGP session will go into an idle state.   Summary   This article provided deeper insights into AWS Direct Connect Gateway, covering use cases, the Direct Connect Gateway components, multi-account setup support, as well as limits and restrictions.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/09/06/dx-gateway-deep-dive/",
        "teaser":null},{
        "title": "Troubleshooting BGP neighbor problem with a Direct Connect Hosted VIF",
        "excerpt":"Here is a quick look at an issue with a BGP session between a customer router (customer-premises equipment; CPE) and an AWS Direct Connect peer.   Problem   While it was possible to ping the AWS Direct Connect peer interface from the customer peer interface the BGP remained in the Idle state. Local and remote ASNs matched up and IP addresses also matched up.   Turning on “debug ip bgp” gave the following insight, which solely showed that the BGP peer connection was timing out.   10.1.103.34 active went from Idle to Active 10.1.103.34 open active, local address 10.1.103.33 10.1.103.34 open failed: Connection timed out; remote host not responding 10.1.103.34 Active open failed - tcb is not available, open active delayed 12288ms (35000ms max, 60% jitter) ses global 10.1.103.34 act Reset (Active open failed). 10.1.103.34 active went from Active to Idle    Setup   The customer router was connected to a Hosted Virtual Interface from the provider Megaport.   The view of this Hosted Virtual Interface within the AWS Console is shown in Figure 1. Notice that the BGP Authentication key shows as empty.                              Figure 1: Hosted VIF showing no BGP Auth Key configured.     With this the corresponding BGP setup on the Cisco-based customer router should be quite trivial.   router bgp 64970 neighbor 10.1.103.34 remote-as AWS_ASN   Troubleshooting   As mentioned before it was possible to ping the AWS Direct Connect peer interface successfully:  Router1#ping 10.1.103.34 Type escape sequence to abort. Sending 5, 100-byte ICMP Echos to 10.1.103.34, timeout is 2 seconds: !!!!! Success rate is 100 percent (5/5), round-trip min/avg/max = 12/12/13 ms   Next, came checking via Telnet whether the BGP daemon was accessible on port TCP/179 on the AWS Direct Connect peer side. A successful connection would eventually be closed by the remote side and therefore look like this:  Router1#telnet 10.1.103.34 179 Trying 10.1.103.34, 179 ... Open  [Connection to 10.1.103.34 closed by foreign host]   Just as a reference: In case the remote host was not accessible due to lack of Layer 3 connectivity, the result would like like this:  Router1#telnet 10.1.103.34 179 Trying 10.1.103.34, 179 ... % Connection timed out; remote host not responding   And if connectivity to port TCP/179 was blocked by e.g. an access control list (ACL), the result would look like this:  Router1#telnet 10.1.103.34 179 Trying 10.1.103.34, 179 ... % Connection refused by remote host   After validating that the BGP peer could be reached successfully, it was time to look further.   Turns out that looking at the Megaport portal gave a slightly different view with the BGP Auth Key showing up. To be fair, Megaport clearly documents this behavior of the BGP Auth key solely showing up in the Megaport portal, but not the AWS Console.   Using some more advanced Cisco IOS troubleshooting commands then confirmed that the AWS Direct Connect peer router was indeed setting an BGP Auth MD5, which the local router was not accepting.   Router1#debug ip tcp transactions address 10.1.103.34 MD5 received, but NOT expected from 10.1.103.34:24834 to 10.1.103.33:179    Fix   After adding the MD5 Auth key to the customer’s BGP config, the BGP peer session came up right away.   router bgp 64970 neighbor 10.1.103.34 remote-as AWS_ASN neighbor 10.1.103.34 password My5UpeR5eCRetPA55W0rD   I would have expected the above “debug ip bgp” command would have showed us some information regarding the missing BGP Auth key. But as there was no BGP Auth setup on the local node, there was no information about the Auth mismatch in the debug output.   Improvements   Keep in mind, that in this case two AWS accounts are involved in this setup. Megaport owns account “A”, which includes the DX connection. Megaport then creates a Private VIF on this connection and shares it out with the customer into account “B”. In this case account “A” can see the BGP MD5 auth key - which is needed to configure the physical router - while account “B” cannot.   It is understandable that AWS does not necessarily want to show the actual MD5 auth value of a shared private VIF within the receiving. In e.g. Enterprise customer scenarios it is common, that account “A” would be owned by the network team - which configures the physical router, while account “B” is owned by an infrastructure team.   Yet it would make sense that account “B” could at least see that an MD5 hash is set instead of making the user believe that it is empty.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/09/09/troubleshooting-bgp-session-hosted-vif/",
        "teaser":null},{
        "title": "Block access from certain countries with Route 53 Geolocation",
        "excerpt":"This article walks you through using Amazon Route 53 Geolocation Routing, in order to block access to your services from certain countries. In addition RIPE Atlas is used in a subsequent step to validate the setup.   Motivation   There are multiple reasons why you might want to block access to your website or API from certain countries. If you are a US based company, you are required to comply with US regulations regarding sanctions against countries such as Cuba, Iran, North Korea, Sudan, or Syria. This could result in the requirement to block access to your website or API from these countries.   Another motivation could be to prevent illicit traffic from countries that you do not conduct business with. Especially China and Russia are known to be a prime source of illicit traffic. Therefore blocking access from these traffic might be wanted in your situation.   Configuration setup   Here, we are using Amazon Route 53 Geolocation Routing to direct traffic from China for one or multiple domain names to either an invalid target IP or a static error page.   Using an invalid IP address, such as a Loopback address like 127.0.0.1 would cause traffic destined for web site or API not to even reach your or your provider’s network. Instead it will already be discarded on the client device.   Route 53 Geolocation routing   In this setup we solely create a DNS test entry to validate functionality of Amazon Route 53 Geolocation Routing for our desired purpose. We will do so by creating a DNS entry of the type “TXT”, that will respond with “China”, when queried from within China, and with “Default” when queried from all other countries.   Geolocation works by mapping IP addresses to locations. However, some IP addresses aren’t mapped to geographic locations, so even if you create geolocation records that cover all seven continents, Amazon Route 53 will receive some DNS queries from locations that it can’t identify. These locations are mapped to the default record. This default record handles both queries from IP addresses that aren’t mapped to any location and queries that come from locations that you haven’t created geolocation records for.   First we create a Route 53 record for the hostname “geoblock” of the type “TXT” (See Figure 1). As depicted the routing policy is specified as “Geolocation”, with the location of “China”. The value for this record is “China”.                              Figure 1: Create TXT record with a ‘Geolocation’ routing policy for the origin country ‘China’.     Next, we create another Route 53 for the same hostname of “geoblock” and the type “TXT” (See Figure 2). As depicted the routing policy is also specified as “Geolocation”. But this time the location is configured as “Default” with the record value being “Default”.                              Figure 2: Create TXT record with a ‘Geolocation’ routing policy for all other countries.     That’s it! In our simple case only two entries are needed. The resulting two Route 53 Geolocation records ar show in Figure 3.                              Figure 3: Resulting TXT record sets for a ‘Geolocation’ routing policy.     Once we have used the above “TXT” records to validate the setup, we can setup a corresponding production record of e.g. our website or API endpoint. This setup will look almost the same, but use a Route 53 record type of “A” instead along with the correct IP addresses.   Error page using CloudFront and S3   As mentioned above you can easily blackhole traffic from undesired locations, by responding with the Loopback IPv4 address of 127.0.0.1. As a result traffic from these clients destined for your endpoint would never leave their system. But as mentioned before, geolocation routing works by mapping IP addresses to locations. And this approach comes with inherent inaccuracies.   Therefore a better approach would be to direct blocked users to a static website, which outlines the reason of the block and also provides an appeal-process. In its simplest form this process could ask users to contact your technical support for help.   You can easily use CloudFront and S3 to serve this static error page.   Testing the Setup   Let’s get back to testing our Amazon Route 53 Geolocation Routing setup and validate that users in China are indeed served with a separate answer than users outside China. To do so, we will be using RIPE Atlas probes.   RIPE Atlas   RIPE Atlas is a global network of hardware devices, called probes and anchors, that actively measure Internet connectivity. Anyone can access this data via Internet traffic maps, streaming data visualisations, and an API. RIPE Atlas users can also perform customised measurements to gain valuable data about their own networks.   I highly recommend you to consider hosting a RIPE Atlas probe yourself. Not only will you benefit from the data that it collects on your Internet connection, but it will also allow you to run customized measurements against various Internet targets. And in the end every additional RIPE Atlas probe will benefit the overall Internet community.   For our purposes we will create a one-off RIPE Atlas measurement of type DNS with the above configured hostname as the target (See Figure 4). Make sure to configure usage of the probe’s resolver and force DNS resolution on the probe. Also we want to select all available RIPE Atlas probes within China, as well as a set of probes outside China.                              Figure 4: RIPE Atlas measurement setup to test Geoblocking in China.     A few minutes after running the one-off RIPE Atlas measurement you should be able to see and download your results. In order to analyze the results and figure out whether our configuration is working, we need to write a small script.   Results   After downloading your RIPE Atlas measurement results in the nd-json (fragmented) format, the below script will allow you to analyze the results.   This script iterates through the results and for each probe determines:     The location country of the probe as specified by the probe’s host.   The result of the DNS lookup, which is either “China” or “Default”.   Whether probes identified to be   in China receive the result “China”   outside China receive the result “Default”   Please note that this script uses two custom RIPE Atlas libraries, which you first need to install with pip install ripe.atlas.sagan --user and pip install ripe.atlas.cousteau --user.   #!/usr/bin/env python3  from ripe.atlas.sagan import Result from ripe.atlas.cousteau import Probe  my_results_file = \"./RIPE-Atlas-measurement-fraq.json\" with open(my_results_file) as results:     for result in results.readlines():         parsed_result = Result.get(result)         probe = Probe(id=parsed_result.probe_id)         probe_country = probe.country_code         probe_id = parsed_result.probe_id         try:             probe_result = parsed_result.responses[0].abuf.answers[0].address         except:             probe_result = \"None\"         status = \"Not OK &lt;====\"         if (probe_country == \"CN\" and probe_result == \"China\"):             status = \"OK\"         if (probe_country != \"CN\" and probe_result == \"Default\"):             status = \"OK\"         if (probe_result == \"None\"):             status = \"Unknown\"          print(probe_country + \": \" + str(probe_id) + \": \" + probe_result + \": \" + status)   Running the above script will yield the following results:   user@host:~$ ./geo-loc-atlas.py CN: 1000050: China: OK EC: 10032: Default: OK CN: 1008: China: OK AM: 11623: Default: OK RS: 12835: Default: OK CN: 14584: None: Unknown BA: 14628: Default: OK GB: 14775: Default: OK CN: 16562: China: OK GB: 18321: Default: OK HU: 18355: Default: OK CZ: 18611: Default: OK TR: 20019: Default: OK US: 20418: Default: OK SK: 20970: Default: OK CN: 21744: Default: Not OK &lt;==== CN: 21832: China: OK RS: 22363: Default: OK FI: 23163: Default: OK FR: 23939: Default: OK ...   Findings   Using the above script along with the RIPE Atlas measurement results, you’ll notice that a few probes - identified by their hosts to be located in China - are not receiving a DNS resolution of “China” for the geoblock DNS entry. Instead they are receiving the “Default” entry, as Route 53 does not identify them to be in China. Looking closer at these probes we can see that the probes are indeed located in Hong Kong. Keep in mind that Route 53 treats Hong Kong as a separate country and also RIPE Atlas allows specifying Hong Kong as a probe’s country. In this case the probe’s host felt that the probe - even though located in Hong Kong - should be labeled as being in China.   Summary   This blog post walked you through using Amazon Route 53 Geolocation Routing, in order to block access to services from certain countries. Furthermore it showed how RIPE Atlas can be used to validate the geoblocking setup.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/12/09/block-countries-with-route53/",
        "teaser":null},{
        "title": "Enabling connectivity between on-premises locations connected to AWS through Direct Connect",
        "excerpt":"This post covers the caveats of data flow between on-premises locations that are each connected to AWS via AWS Direct Connect. In case you have multiple on-premise locations connected to AWS via Direct Connect, enabling the data flow between these locations is not always trivial. Therefore this blog post highlights some of the pitfalls and outlines possible solutions.   Introduction   This blog post assumes the fundamental design as depicted in Figure 1. One or more on-premises locations such as offices connect via carrier ethernet or another local connectivity option to two Direct Connect locations within close proximity.                              Figure 1: Design to enable connectivity between on-premises locations and AWS within a geo.     Traffic from on-premises destined to AWS is routed via one of the two Direct locations, while the other location serves as backup path. Alternatively both Direct Connect locations can be used in an Active/Active setup.   Caveats   While implementing this approach of enabling connect between on-premises locations there are a few caveats that need to be considered.   Intra Region traffic   Keep in mind that the AWS Direct Connect Gateway does not allow you to route traffic from one Virtual Interface to another Virtual Interface. Therefore the traffic flow as despicted in Figure 2 is not currently possible.                              Figure 2: Unsupported VIF to VIF routing with Direct Connect Gateway.     If you have a need to route traffic between on-premise locations in a certain region through the same AWS region, you need to leverage a separate Direct Connect Gateway, Transit VIF, and Direct Connect connection for each of your offices. The resulting design is depicted in Figure 3.                              Figure 3: Workaround to leverage Transit Gateway for intra-office routing.     This approach could make sense in case you have the requirement of inspecting and filtering traffic between on-premises locations via an AWS-based device. In case you have no such requirement, it makes more sense to route traffic between locations directly via e.g. a local Carrier Ethernet connectivity (Figure 4), completely leaving it out of AWS.                              Figure 4: Intra-office connectivity within the same region.     Inter Region traffic   Thanks to the recently released capability of Inter-Region Peering for the Transit Gateway you can extend the above described model and connect your on-premises locations across the globe to AWS using AWS Direct Connect and Transit Gateway (Figure 5).                              Figure 5: Intra-office connectivity outside a region over the AWS backbone.     This approach is also useful in case you want to connect your on-premises locations to more than three AWS regions. Due to the limitation of only being able to connect up to three Transit Gateways per Direct Connect Gateway regionalizing your Direct Connect Gateways this way allows you to scale very elegantly.   Summary   In this article we discuss the caveats of data path between your on-premises locations while using AWS Direct Connect.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2019/12/27/aws-dx-connect-on-premises/",
        "teaser":null},{
        "title": "Drawing monitor for network diagrams",
        "excerpt":"As a Senior Solutions Architect Specialist for Networking at AWS, I spent a lot of time in online meetings with customers, helping them design networks or understand AWS networking better. Part of these meetings usually involves whiteboarding network diagrams. While visualizing ideas or concepts is something very common and straight forward on a physical whiteboard, it is not trivial in an online meeting.   As I often get asked about my setup during or after calls, I want to shed some light on what I’m using and what’s working for me.   Home office setup   As I primarily work from home, my home office setup (See Figure 1) provides everything I need to get through the day.                              Figure 1: Home office setup with Huion Kamvas Pro 12 Drawing Monitor.     This article will focus on the Drawing monitor, visible in the lower left.   Hardware   The key component of my home office setup is a Huion Kamvas Pro 12 Drawing Monitor, connected via HDMI and USB to the docking station of my notebook. On my PC it shows up as a secondary screen in addition to my primary Dell U3419 Ultrasharp monitor. Pen input on the Huion monitor is translated into standard mouse input.   It’s important to point out that in contrary to touch enabled notebooks, this monitor does not provide touch input. It only reacts to the provided pen. Therefore you can touch it with the balm of your hand while drawing (See Figure 2). Although that does leave smudges and you’ll have to clean the screen with a good microfiber clearning cloth from time to time.                              Figure 2: Drawing on Huion Kamvas Pro 12 Drawing Monitor with balm on screen.     Software   As mentioned before the pen input on the drawing monitor is translated to standard mouse input. You can therefore use any software that allows drawing with a mouse, ranging from common software such as Acrobat Reader to Microsoft PowerPoint.   For my particular use case the software OpenBoard has proven to be most effective. OpenBoard is an open source teaching software for interactive whiteboards designed primarily for use in schools and universities. It is maintained by the Education Department (DIP) of the canton of Geneva, in Switzerland.   OpenBoard not only allows free-hand drawing, but you can also quickly and easily import existing customer design documents or pull up pre-made diagrams to draw over (See Figure 3).                              Figure 3: Huion Kamvas Pro 12 Drawing Monitor with pre-made diagrams and free-hand drawings.     Effectively having two mice as input along with two monitors can cause unwanted situations in the form of not finding the mouse pointer. Under Windows 10 you can enable the setting “Show location of pointer when I press the CTRL key” under the mouse properties to mitigate this challenge (See Figure 4).                              Figure 4: Windows 10 setting for ‘Show location of pointer when I press the CTRL key’.     Summary   This article showed you my home office setup for visualizing ideas and concepts during customer calls with a drawing monitor and interactive whiteboard software. If you wonder who these birds on the pictures are. That’s Jenny the Kea and Pete the Pukeko.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2020/04/19/drawing-monitor-for-network-diagrams/",
        "teaser":null},{
        "title": "Multicast with AWS Transit Gateway",
        "excerpt":"At the beginning of December 2019, AWS released Multicast support within the AWS Transit Gateway. This marked the beginning of being able to use applications that require support for IP Multicast within AWS.   The following article will give you a brief overview of how to use IP Multicast with the AWS Transit Gateway and especially how to validate that your setup is working correctly.   Multicast   Let’s have a look at the basics of IP Multicasting and IP Multicast on AWS.   What is IP Multicasting   With IP Multicasting a source host can send a single packet to hundreds or thousands of hosts at the same time. All this works over a route network - e.g. the Internet. The replication of the source packet along with tracking of destination membership happens inside of the network itself, instead of the application (See Figure 1).                              Figure 1: IP Multicasting with source host, rendezvous point and multicast receiver.     For this to work the source host addresses the packet with a Multicast Address from the range of 224.0.0.0 through 239.255.255.255. Each Multicast Address specifies a Multicast group to which other hosts can subscribe to. Such a group can have between one and an unlimited number of members as neither hosts nor routers maintain a list of all members. Instead the source host send the packet to an initial router, called the rendezvous point (RP), which serves as the root of a tree-like multicast distribution. The most common transport layer protocol to use multicast addressing is User Datagram Protocol (UDP).   Multicast and AWS Transit Gateway   While an AWS VPC by itself does not support Multicast, AWS Transit Gateway can provide this new capability. The Transit Gateway will act as the rendezvous point, receiving the packets from the Multicast source, replicate it and send it to the Multicast Receiver. With the Multicast-enabled Transit Gateway, source and receiver can be in the same VPC or in different VPCs.   Constraints   Today Multicast on AWS Transit Gateway comes with a few restrictions that need to be considered:     Creation and usage of Multicast-enabled TGW is currently only supported in the AWS Region us-east-1 (N. Virginia).   You must create a new TGW to enable Multicast. It is not possible to enable Multicast on an existing Transit Gateway. In case you are already using a Transit Gateway, you can create another instance that will just serve the purpose of distributing Multicast traffic (See Figure 2). As neither VPC nor TGW route tables are used to handle multicast traffic, this deployment model will not interfere with your existing traditional TGW or VPC route tables.   Self-Management of Multicast group membership by hosts through the Internet Group Management Protocol (IGMP) is not yet supported. Instead Multicast group membership is solely managed using Amazon VPC Console or the AWS CLI.   Only AWS Nitro instances can be a Multicast source. If you use a non-Nitro instance as a receiver, you must disable the Source/Dest check.   While the AWS Transit Gateway has a default limit of only supporting 1x TGW source per group, this limit can be increased.                              Figure 2: Multicast-enabled TGW besides traditional TGW.     Setup   Before kicking the tires on Multicast, it needs to be setup. At a minimum this requires the network setup within the Transit Gateway, as well as at least one Multicast Source and one Multicast Receiver instance.   Source and Receiver Instances   First setup the EC2 instances that we will use in our testing as the Multicast Source and Multicast Receiver. To better understand how Multicast works, you should create at least one Multicast Source and two Multicast Receiver. Also keep the constraint around usage of AWS Nitro instances in mind and select a Nitro-based EC2 instance for the three EC2 instances. This blog post will assume use of Ubuntu Linux instances for the EC2 instances and you should install iPerf on these instances. On Ubuntu Linux you will do this via sudo apt get install iperf.   Last, but not least ensure that port UDP 5001 is opened within the security group associated with the Multicast Receiver instances. The source IP will be the IPv4 address of the ENI associated with the Multicast Source EC2 instance.   Transit Gateway (TGW)   You can follow the AWS instructions for setting up a Multicast-enabled TGW, along with a multicast domain. Next associate any of the VPCs and subnets that will include Multicast Sources and Receivers to this TGW. Then associate the individual Multicast Sources and Receiver via their Elastic Network Interface (ENI) with a Multicast Group (See Figure 3).                              Figure 3: Multicast-Group with single source and multiple receiver.     The above example shows the Multicast Group 224.1.1.1 with one Multicast Source and two Multicast Receivers.   Testing   Now that the setup is complete, it’s time to test our setup. We expect that a packet send by the Multicast Source to the Multicast group 224.1.1.1 should be simultaneously received by both Multicast Receivers.   Multicast Receiver   First get the two Multicast Receiver ready by starting iPerf in “Server” mode against the Multicast group IP of “224.1.1.1” and using the protocol UDP. iPerf will use the default port of 5001.   % iperf -s -u -B 224.1.1.1 -i 1 ------------------------------------------------------------ Server listening on UDP port 5001 Binding to local address 224.1.1.1 Joining multicast group  224.1.1.1 Receiving 1470 byte datagrams UDP buffer size:  110 KByte (default) ------------------------------------------------------------   iPerf will recognize the provided address as an IP Multicast address and join this group. After that iPerf will wait for incoming traffic over UDP port 5001 and display it accordingly.   Multicast Source   Now that the Multicast Receiver are ready to accept traffic, it’s time to get the Multicast Source ready. On the Multicast Source EC2 instance, we will start iPerf in client mode against the above Multicast group.   % iperf -c 224.1.1.1 -u -T 32 -t 3 -i 1 ------------------------------------------------------------ Client connecting to 224.1.1.1, UDP port 5001 Sending 1470 byte datagrams Setting multicast TTL to 32 UDP buffer size:  110 KByte (default) ------------------------------------------------------------ [  3] local 192.168.220.20 port 59347 connected with 224.1.1.1 port 5001 [ ID] Interval       Transfer     Bandwidth [  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec [  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec [  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec [  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec [  3] Sent 269 datagrams    This way iPerf will generate traffic and send it to the Multicast group 224.1.1.1 over UDP to port 5001, which is the default setting.   Result   While the Multicast Source is generating traffic, we can head over to the two Multicast Receiver instances. There we should notice that both of the instances are receiving the test traffic at the same time.   ... [  3] local 224.1.1.1 port 5001 connected with 192.168.220.20 port 59347 [ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams [  3]  0.0- 1.0 sec   128 KBytes  1.05 Mbits/sec   0.035 ms    0/   89 (0%) [  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec   0.015 ms    0/   89 (0%) [  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec   0.025 ms    0/   89 (0%) [  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec   0.068 ms    0/  269 (0%)    As expected the underlying network replicated the test traffic from the Multicast Source and delivered it to both Multicast Receiver at the same time.   Troubleshooting   Here are a few things to look at in case something is not working     Security Groups and Firewalls: Make sure that the Security Group associated with the Multicast Receiver EC2 instance allows the Multicast test traffic in. We will be using the transport protocol UDP and by default iPerf uses the port 5001. Also, the source IP of the received traffic will be the IPv4 address of the ENI that is the Multicast Source. In case you are using multiple Multicast Sources, this will translate to multiple IPv4 addresses.   TGW Multicast Domain groups: Make sure that the Multicast group IP address that you are using with iPerf matches the setup in TGW. Also make sure that you specified the correct EC2 instances - via their ENI - as the source and receiver.   If things still don’t work, fire up tcpdump on the Source and Receiver side and see what packets you’re seeing.   Summary   This article walked you through the setup and configuration of a Multicast-enabled Transit Gateway and showed you how to quickly test functionality using iPerf.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network"],
        "url": "https://www.edge-cloud.net/2020/05/01/tgw-multicast-intro/",
        "teaser":null},{
        "title": "AWS Site-to-Site VPN (IPSec) with IPv6",
        "excerpt":"Recently AWS released support for IPv6 traffic over the AWS Site-to-Site VPN (IPSec). The setup of the AWS Site-to-Site VPN has always been quite straight forward and thanks to the downloadable configuration files at times even trivial. With the introduction of IPv6 support this is unfortunately no longer the case. Therefore this blog post will guide you around some of the pitfalls of setting up an AWS Site-to-Site VPN with IPv6 support, hoping that they will eventually be removed and this post become unnecessary.   Constraints   A few constraints apply when using AWS Site-to-Site VPN (IPSec) with IPv6:     The outside tunnel IP addresses - which are the public non-RFC1918 addresses - still only support IPv4. You can only use IPv6 on the inside of the tunnel, in order to carry IPv6 traffic between your on-premises network and AWS.   You have to use an AWS Transit Gateway (TGW) as the AWS termination of your VPN. Site-to-Site VPNs to a Virtual Private Gateways (VGW) do not support IPv6.   You cannot retrofit existing Site-to-Site connections with IPv6, but need to create a new connection.   A Site-to-Site VPN connection can only support IPv4, or IPv6. This means that if you need to carry both IPv4 and IPv6 traffic between AWS and on-premises you need to create two separate connections, one for IPv4 and one for IPv6 (See Figure 1).                              Figure 1: AWS Site-to-Site VPN setup with IPv4 and IPv6 support.     As each AWS Site-to-Site VPN connection consist of two tunnels, in the case of supporting IPv4/IPv6 Dualstack traffic you will therefore end up with a total of four tunnels, two for IPv4 traffic and two for IPv6 traffic. Also note that this means you’ll be paying separately for the tunnel carrying the IPv4 traffic as well as for the tunnel carrying the IPv6 traffic.   Configuration   AWS Setup   The creation of a Site-to-Site VPN connection is straight forward and only differs from its IPv4 counterpart by setting the “Tunnel Inside IP version” to IPv6 instead of IPv4 (See Figure 2).                              Figure 2: Creating a new AWS Site-to-Site VPN with IPv6 support.     The IPv6 enabled Site-to-Site VPN connection also supports defining the IPv6 addresses used within the tunnel yourself. If you want to make use of this capability you have to select a /126 IPv6 subent out of the fd00::/8 Unique local address address range. This is useful to prevent IP address collisions across multiple tunnels. Although with IPv6 addresses, such a collision is much less likely than with IPv4.   Customer-side Configuration   Basic configuration information   When configuring the costumer-side of the solution, the challenges will start. Having a look at the Tunnel details for a newly created AWS Site-to-Site VPN with IPv6 support will yield some surprising results (See Figure 3).                              Figure 3: Tunnel details for AWS Site-to-Site VPN with IPv6 support.     You’ll see that for both tunnels “Inside IPv4 CIDRs” from the 169.254.0.0/16 Link-local address range will be shown. As mentioned previously an AWS Site-to-Site connection - and thereby its tunnels - only supports either IPv4 or IPv6. As this is an IPv6 enabled tunnel, the displayed Inner IPv4 CIDRs are irrelevant and cannot be used for anything. Therefore it’s best to completely ignore this particular column here.   Next you will notice that for the “Inside IPv6 CIDRs” the addresses are provided with a /128 netmask - which is the IPv6 equivalent of /32 and therefore a “host-only” netmask. This netmask therefore only contains a single IP address. Yet the documentation clearly calls out that these “Inside IPv6 CIDRs” should be a /126 IPv6 CIDR block, which includes 4 IPv6 addresses. A /126 IPv6 CIDR is the equivalent of a /30 IPv4 CIDR. Also the provided IPv6 address from the fd00::/8 Unique local address address range ends in an odd number and is therefore not a network address, which always end in even numbers. Turns out that the displayed IPv6 address is actually the AWS side of the inner connection.   So with the value of “fdbe:1a26:45b0:4631:ca60:3307:371b:6315/128” displayed in the provided example, the relevant IPv6 information would be:     Subnet (aka CIDR): fdbe:1a26:45b0:4631:ca60:3307:371b:6314/126   AWS-side interface address: fdbe:1a26:45b0:4631:ca60:3307:371b:6315/126   Customer-side address: fdbe:1a26:45b0:4631:ca60:3307:371b:6316/126   You should notice the pattern for constructing the unnecessary IPv6 information:     Subnet (aka CIDR): Provided IPv6 address - 1   AWS-side interface address: Provided IPv6 address   Customer-side address: Provided IPv6 address + 1   Just keep in mind that IPv6 addresses use hex digits, which start with 0,1,2,.. then continue with ..,8,9,A,B,.. and end with ..,E,F.   You can double check your conversion math knowing that the used netmask is a /128, allowing IPv6 subnets with 4 addresses. Therefore 4 different combinations of trailing IPv6 digits exist:     Subnet (aka CIDR): Must always end in 0,4,8,C   AWS-side interface address: Must always end in 1,5,9,D   Customer-side address: Must always end in 2,6,A,E   Configuration download   The next challenge you will notice is that when downloading the downloadable configuration files from the AWS Console, it does not include any IPv6 address information. Instead for the inner address it includes IPv4 address information that are irrelevant as already pointed out.   Nevertheless you can leverage the downloadable configuration file for the Internet Key Exchange (IKE) and IPSec Configuration of your tunnels. These two sections within the file can be used without any changes, unless you diverted with your IKE or IPSec settings from the default values. Because these downloadable configuration files are only baseline examples that assume default values.   Tunnel Interface Configuration   With that the Tunnel Interface Configuration has to be adapted from IPv4 to IPv6. Using a Cisco IOS configuration as an example, the downloadable configuration file will provide the following Interface Configuration.   interface Tunnel1   ip address 169.254.123.90 255.255.255.252   ip virtual-reassembly   tunnel source 198.51.100.123   tunnel destination 54.68.62.136   tunnel mode ipsec ipv4   tunnel protection ipsec profile ipsec-vpn-0b1561f60da62e5eb-0   ip tcp adjust-mss 1379    Within this configuration we have to replace the inner IPv4 address with an IPv6 address and also specify that IPv6 should be used within the inside of the tunnel. The outer IP address remains to be an IPv4 address. The resulting corrected configuration using the IPv6 address as outlined in the previous section becomes as follows.   interface Tunnel1  no ip address  ip virtual-reassembly  ipv6 address FDBE:1A26:45B0:4631:CA60:3307:371B:6316/126  tunnel source 198.51.100.123  tunnel destination 54.68.62.136  tunnel mode ipsec ipv4 v6-overlay  tunnel protection ipsec profile ipsec-vpn-0b1561f60da62e5eb-0  ip tcp adjust-mss 1379    Border Gateway Protocol (BGP) Configuration   Next the Border Gateway Protocol (BGP) Configuration also needs to be adapted, replacing the IPv4 configuration with a corresponding IPv6 configuration.   The downloadable configuration will provide the following BGP configuration.   router bgp 65000   neighbor 169.254.123.89 remote-as 64512   neighbor 169.254.123.89 activate   neighbor 169.254.123.89 timers 10 30 30   address-family ipv4 unicast     neighbor 169.254.123.89 remote-as 64512     neighbor 169.254.123.89 timers 10 30 30     neighbor 169.254.123.89 activate     neighbor 169.254.123.89 soft-reconfiguration inbound   As we want to run BGP over the inner IPv6 connection, we again have to replace all IPv4 configuration items with the corresponding IPv6 addresses. The result will look as follows.   router bgp 65000   neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 remote-as 64512   neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 activate   neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 timers 10 30 30   address-family ipv6 unicast     neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 remote-as 64512     neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 timers 10 30 30     neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 activate     neighbor FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 soft-reconfiguration inbound   Here you effectively have to replace the unusable IPv4 address for the neighbor with the correct IPv6 address of that neighbor and change the address-family section from IPv4 to IPv6.   Validation   As a final step, we can validate that IPv6 routes are being learned from the TGW via the Site-to-Site VPN.   CSR1000V-01#sh ipv6 route bgp IPv6 Routing Table - default - 13 entries Codes: C - Connected, L - Local, S - Static, U - Per-user Static route        B - BGP, R - RIP, H - NHRP, I1 - ISIS L1        I2 - ISIS L2, IA - ISIS interarea, IS - ISIS summary, D - EIGRP        EX - EIGRP external, ND - ND Default, NDp - ND Prefix, DCE - Destination        NDr - Redirect, RL - RPL, O - OSPF Intra, OI - OSPF Inter        OE1 - OSPF ext 1, OE2 - OSPF ext 2, ON1 - OSPF NSSA ext 1        ON2 - OSPF NSSA ext 2, la - LISP alt, lr - LISP site-registrations        ld - LISP dyn-eid, lA - LISP away, le - LISP extranet-policy        lp - LISP publications, a - Application, m - OMP B   2600:1234::/64 [20/100]      via FDBE:1A26:45B0:4631:CA60:3307:371B:6315      via FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 B   2600:1A14:5DE:DB00::/56 [20/100]      via FDBE:1A26:45B0:4631:CA60:3307:371B:6315      via FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1 B   2600:1A16:807:3A00::/56 [20/100]      via FDBE:1A26:45B0:4631:CA60:3307:371B:6315      via FDB0:C9AB:9EC7:3934:EE0:9873:54BC:C8F1    In this case we can see that a total of three prefixes is learned, whereas the first prefix is originated on the TGW via a summary route, while the other two prefixes correspond to VPCs.   Summary   This blog post guided you around some of the pitfalls when setting up an IPv6 capable AWS Site-to-Site VPN connection. It covered some of the rather cosmetic imperfections within the AWS Console around unusable IPv4 addresses, as well as generating a working Customer Gateway (CGW) configuration based on the downloadable configuration files.  ","categories": ["EdgeCloud"],
        "tags": ["AWS","Network","IPv6"],
        "url": "https://www.edge-cloud.net/2020/09/11/aws-ipsec-vpn-ipv6/",
        "teaser":null}]
